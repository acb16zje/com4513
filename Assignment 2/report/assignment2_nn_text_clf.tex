\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)

    \usepackage{iftex}
    \ifPDFTeX
    	\usepackage[T1]{fontenc}
    	\usepackage{mathpazo}
    \else
    	\usepackage{fontspec}
    \fi

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}
    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    \makeatletter % fix for grffile with XeLaTeX
    \def\Gread@@xetex#1{%
      \IfFileExists{"\Gin@base".bb}%
      {\Gread@eps{\Gin@base.bb}}%
      {\Gread@@xetex@aux#1}%
    }
    \makeatother

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}



    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{assignment2\_nn\_text\_clf}





% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb.
    \makeatletter
        \newbox\Wrappedcontinuationbox
        \newbox\Wrappedvisiblespacebox
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}}
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}}
        \newcommand*\Wrappedcontinuationindent {3ex }
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox}
        % Take advantage of the already applied Pygments mark-up to insert
        % potential linebreaks for TeX processing.
        %        {, <, #, %, $, ' and ": go to next line.
        %        _, }, ^, &, >, - and ~: stay at end of broken line.
        % Use of \textquotesingle for straight quote.
        \newcommand*\Wrappedbreaksatspecials {%
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}%
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}%
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}%
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}%
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}%
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}%
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}%
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}%
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}%
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}%
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}%
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}%
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}%
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}%
        }
        % Some characters . , ; ? ! / are not pygmentized.
        % This macro makes them "active" and they will insert potential linebreaks
        \newcommand*\Wrappedbreaksatpunct {%
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}%
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}%
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}%
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}%
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}%
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}%
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}%
            \catcode`\.\active
            \catcode`\,\active
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active
            \lccode`\~`\~
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%

        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}

    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        \ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}
    }



    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults

    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}



\begin{document}

    \maketitle




    \hypertarget{com4513-6513-assignment-2-text-classification-with-a-feedforward-network}{%
\subsection{{[}COM4513-6513{]} Assignment 2: Text Classification with a
Feedforward
Network}\label{com4513-6513-assignment-2-text-classification-with-a-feedforward-network}}

\hypertarget{instructor-nikos-aletras}{%
\subsubsection{Instructor: Nikos
Aletras}\label{instructor-nikos-aletras}}

The goal of this assignment is to develop a Feedforward network for text
classification.

For that purpose, you will implement:

\begin{itemize}
\item
  Text processing methods for transforming raw text data into input
  vectors for your network (\textbf{1 mark})
\item
  A Feedforward network consisting of:

  \begin{itemize}
  \tightlist
  \item
    \textbf{One-hot} input layer mapping words into an \textbf{Embedding
    weight matrix} (\textbf{1 mark})
  \item
    \textbf{One hidden layer} computing the mean embedding vector of all
    words in input followed by a \textbf{ReLU activation function}
    (\textbf{1 mark})
  \item
    \textbf{Output layer} with a \textbf{softmax} activation. (\textbf{1
    mark})
  \end{itemize}
\item
  The Stochastic Gradient Descent (SGD) algorithm with
  \textbf{back-propagation} to learn the weights of your Neural network.
  Your algorithm should:

  \begin{itemize}
  \tightlist
  \item
    Use (and minimise) the \textbf{Categorical Cross-entropy loss}
    function (\textbf{1 mark})
  \item
    Perform a \textbf{Forward pass} to compute intermediate outputs
    (\textbf{4 marks})
  \item
    Perform a \textbf{Backward pass} to compute gradients and update all
    sets of weights (\textbf{4 marks})
  \item
    Implement and use \textbf{Dropout} after each hidden layer for
    regularisation (\textbf{2 marks})
  \end{itemize}
\item
  Discuss how did you choose hyperparameters? You can tune the learning
  rate (hint: choose small values), embedding size \{e.g.~50, 300,
  500\}, the dropout rate \{e.g.~0.2, 0.5\} and the learning rate.
  Please use tables or graphs to show training and validation
  performance for each hyperparam combination (\textbf{2 marks}).
\item
  After training the model, plot the learning process (i.e.~training and
  validation loss in each epoch) using a line plot and report accuracy.
\item
  Re-train your network by using pre-trained embeddings
  (\href{https://nlp.stanford.edu/projects/glove/}{GloVe}) trained on
  large corpora. Instead of randomly initialising the embedding weights
  matrix, you should initialise it with the pre-trained weights. During
  training, you should not update them (i.e.~weight freezing) and
  backprop should stop before computing gradients for updating embedding
  weights. Report results by performing hyperparameter tuning and
  plotting the learning process. Do you get better performance?
  (\textbf{3 marks}).
\item
  \textbf{BONUS:} Extend you Feedforward network by adding more hidden
  layers (e.g.~one more). How does it affect the performance? Note: You
  need to repeat hyperparameter tuning, but the number of combinations
  grows exponentially. Therefore, you need to choose a subset of all
  possible combinations (\textbf{+2 extra marks})
\end{itemize}

\hypertarget{data}{%
\subsubsection{Data}\label{data}}

The data you will use for Task 2 is a subset of the
\href{http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html}{AG
News Corpus} and you can find it in the \texttt{./data\_topic} folder in
CSV format:

\begin{itemize}
\tightlist
\item
  \texttt{data\_topic/train.csv}: contains 2,400 news articles, 800 for
  each class to be used for training.
\item
  \texttt{data\_topic/dev.csv}: contains 150 news articles, 50 for each
  class to be used for hyperparameter selection and monitoring the
  training process.
\item
  \texttt{data\_topic/test.csv}: contains 900 news articles, 300 for
  each class to be used for testing.
\end{itemize}

\hypertarget{pre-trained-embeddings}{%
\subsubsection{Pre-trained Embeddings}\label{pre-trained-embeddings}}

You can download pre-trained GloVe embeddings trained on Common Crawl
(840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download) from
\href{http://nlp.stanford.edu/data/glove.840B.300d.zip}{here}. No need
to unzip, the file is large.

\hypertarget{save-memory}{%
\subsubsection{Save Memory}\label{save-memory}}

To save RAM, when you finish each experiment you can delete the weights
of your network using \texttt{del\ W} followed by Python's garbage
collector \texttt{gc.collect()}

\hypertarget{submission-instructions}{%
\subsubsection{Submission Instructions}\label{submission-instructions}}

You should submit a Jupyter Notebook file (assignment2.ipynb) and an
exported PDF version (you can do it from Jupyter:
\texttt{File-\textgreater{}Download\ as-\textgreater{}PDF\ via\ Latex}).

You are advised to follow the code structure given in this notebook by
completing all given funtions. You can also write any auxilliary/helper
functions (and arguments for the functions) that you might need but note
that you can provide a full solution without any such functions.
Similarly, you can just use only the packages imported below but you are
free to use any functionality from the
\href{https://docs.python.org/2/library/index.html}{Python Standard
Library}, NumPy, SciPy and Pandas. You are not allowed to use any
third-party library such as Scikit-learn (apart from metric functions
already provided), NLTK, Spacy, Keras etc.. You are allowed to re-use
your code from Assignment 1.

Please make sure to comment your code. You should also mention if you've
used Windows to write and test your code. There is no single correct
answer on what your accuracy should be, but correct implementations
usually achieve F1 of \textasciitilde75-80\% and \textasciitilde85\%
without and with using pre-trained embeddings respectively.

This assignment will be marked out of 20. It is worth 20\% of your final
grade in the module. If you implement the bonus question you can get up
to 2 extra points but your final grade will be capped at 20.

The deadline for this assignment is \textbf{23:59 on Mon, 18 May 2020}
and it needs to be submitted via Blackboard (MOLE). Standard
departmental penalties for lateness will be applied. We use a range of
strategies to detect
\href{https://www.sheffield.ac.uk/ssid/unfair-means/index}{unfair
means}, including Turnitin which helps detect plagiarism, so make sure
you do not plagiarise.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{from} \PY{n+nn}{collections} \PY{k+kn}{import} \PY{n}{Counter}
\PY{k+kn}{import} \PY{n+nn}{re}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k+kn}{import} \PY{n}{accuracy\PYZus{}score}\PY{p}{,} \PY{n}{precision\PYZus{}score}\PY{p}{,} \PY{n}{recall\PYZus{}score}\PY{p}{,} \PY{n}{f1\PYZus{}score}
\PY{k+kn}{from} \PY{n+nn}{time} \PY{k+kn}{import} \PY{n}{localtime}\PY{p}{,} \PY{n}{strftime}
\PY{k+kn}{from} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{stats} \PY{k+kn}{import} \PY{n}{spearmanr}\PY{p}{,}\PY{n}{pearsonr}
\PY{k+kn}{import} \PY{n+nn}{zipfile}
\PY{k+kn}{import} \PY{n+nn}{gc}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{transform-raw-texts-into-training-and-development-data}{%
\subsection{Transform Raw texts into training and development
data}\label{transform-raw-texts-into-training-and-development-data}}

First, you need to load the training, development and test sets from
their corresponding CSV files (tip: you can use Pandas dataframes).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{topic\PYZus{}dev} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{data\PYZus{}topic/dev.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{names}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{label}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{text}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\PY{n}{topic\PYZus{}test} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{data\PYZus{}topic/test.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{names}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{label}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{text}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\PY{n}{topic\PYZus{}train} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{data\PYZus{}topic/train.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{names}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{label}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{text}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}

\PY{n}{topic\PYZus{}dev\PYZus{}texts} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{topic\PYZus{}dev}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{text}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\PY{n}{topic\PYZus{}train\PYZus{}texts} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{topic\PYZus{}train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{text}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\PY{n}{topic\PYZus{}test\PYZus{}texts} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n}{topic\PYZus{}test}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{text}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{create-input-representations}{%
\section{Create input
representations}\label{create-input-representations}}

To train your Feedforward network, you first need to obtain input
representations given a vocabulary. One-hot encoding requires large
memory capacity. Therefore, we will instead represent documents as lists
of vocabulary indices (each word corresponds to a vocabulary index).

\hypertarget{text-pre-processing-pipeline}{%
\subsection{Text Pre-Processing
Pipeline}\label{text-pre-processing-pipeline}}

To obtain a vocabulary of words. You should: - tokenise all texts into a
list of unigrams (tip: you can re-use the functions from Assignment 1) -
remove stop words (using the one provided or one of your preference) -
remove unigrams appearing in less than K documents - use the remaining
to create a vocabulary of the top-N most frequent unigrams in the entire
corpus.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{default\PYZus{}stop\PYZus{}words} \PY{o}{=} \PY{p}{\PYZob{}}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{a}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ad}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{after}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{again}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{all}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{also}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{am}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{an}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{and}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{any}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{are}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{as}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{at}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{be}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{because}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{been}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{being}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{between}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{both}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{but}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{by}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{can}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{could}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{does}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{each}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ed}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{eg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{either}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{etc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{even}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ever}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{every}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{for}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{from}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{had}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{has}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{have}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{he}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{her}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hers}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{herself}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{him}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{himself}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{his}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{i}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ie}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{if}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{in}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{inc}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{into}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{is}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{it}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{its}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{itself}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{li}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ll}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ltd}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{may}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{maybe}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{me}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{might}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mine}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{minute}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{minutes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{must}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{my}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{myself}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neither}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{nor}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{now}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{of}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{on}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{only}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{or}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{other}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{our}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ours}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ourselves}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{own}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{same}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{seem}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{seemed}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{shall}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{she}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{some}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{somehow}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{something}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sometimes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{somewhat}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{somewhere}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{spoiler}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{spoilers}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{such}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{suppose}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{that}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{the}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{their}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{theirs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{them}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{themselves}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{there}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{these}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{they}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{this}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{those}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{thus}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{to}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{today}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tomorrow}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{us}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ve}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{vs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{was}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{we}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{were}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{what}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{whatever}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{when}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{whenever}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{where}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{whereby}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{which}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{who}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{whom}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{whose}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{will}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{with}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{yesterday}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{you}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{your}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{yours}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{yourself}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{yourselves}\PY{l+s+s1}{\PYZsq{}}
\PY{p}{\PYZcb{}}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{unigram-extraction-from-a-document}{%
\subsubsection{Unigram extraction from a
document}\label{unigram-extraction-from-a-document}}

You first need to implement the \texttt{extract\_ngrams} function. It
takes as input: - \texttt{x\_raw}: a string corresponding to the raw
text of a document - \texttt{ngram\_range}: a tuple of two integers
denoting the type of ngrams you want to extract, e.g.~(1,2) denotes
extracting unigrams and bigrams. - \texttt{token\_pattern}: a string to
be used within a regular expression to extract all tokens. Note that
data is already tokenised so you could opt for a simple white space
tokenisation. - \texttt{stop\_words}: a list of stop words -
\texttt{vocab}: a given vocabulary. It should be used to extract
specific features.

and returns:

\begin{itemize}
\tightlist
\item
  a list of all extracted features.
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{extract\PYZus{}ngrams}\PY{p}{(}\PY{n}{x\PYZus{}raw}\PY{p}{,}
                   \PY{n}{ngram\PYZus{}range}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,}
                   \PY{n}{token\PYZus{}pattern}\PY{o}{=}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{b[A\PYZhy{}Za\PYZhy{}z]}\PY{l+s+s1}{\PYZob{}}\PY{l+s+s1}{2,\PYZcb{}}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                   \PY{n}{stop\PYZus{}words}\PY{o}{=}\PY{n}{default\PYZus{}stop\PYZus{}words}\PY{p}{,}
                   \PY{n}{vocab}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}\PY{p}{:}

    \PY{n}{tokens} \PY{o}{=} \PY{p}{[}
        \PY{n}{word}\PY{o}{.}\PY{n}{lower}\PY{p}{(}\PY{p}{)} \PY{k}{for} \PY{n}{word} \PY{o+ow}{in} \PY{n}{re}\PY{o}{.}\PY{n}{findall}\PY{p}{(}\PY{n}{token\PYZus{}pattern}\PY{p}{,} \PY{n}{x\PYZus{}raw}\PY{p}{)}
        \PY{k}{if} \PY{n}{word}\PY{o}{.}\PY{n}{lower}\PY{p}{(}\PY{p}{)} \PY{o+ow}{not} \PY{o+ow}{in} \PY{n}{stop\PYZus{}words}
    \PY{p}{]}

    \PY{n}{ngrams} \PY{o}{=} \PY{p}{[}\PY{p}{]}

    \PY{k}{for} \PY{n}{n} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{ngram\PYZus{}range}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{ngram\PYZus{}range}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
        \PY{k}{if} \PY{n}{n} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} Create unigram by concatenating list}
            \PY{n}{ngrams} \PY{o}{+}\PY{o}{=} \PY{n}{tokens}
        \PY{k}{else}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} Create bigram / trigram by unzipping list}
            \PY{n}{ngrams} \PY{o}{+}\PY{o}{=} \PY{n+nb}{zip}\PY{p}{(}\PY{o}{*}\PY{p}{(}\PY{n}{tokens}\PY{p}{[}\PY{n}{i}\PY{p}{:}\PY{p}{]} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{n}\PY{p}{)}\PY{p}{)}\PY{p}{)}

    \PY{k}{return} \PY{p}{[}\PY{n}{ngram} \PY{k}{for} \PY{n}{ngram} \PY{o+ow}{in} \PY{n}{ngrams} \PY{k}{if} \PY{n}{ngram} \PY{o+ow}{in} \PY{n}{vocab}\PY{p}{]} \PY{k}{if} \PY{n}{vocab} \PY{k}{else} \PY{n}{ngrams}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{create-a-vocabulary-of-n-grams}{%
\subsubsection{Create a vocabulary of
n-grams}\label{create-a-vocabulary-of-n-grams}}

Then the \texttt{get\_vocab} function will be used to (1) create a
vocabulary of ngrams; (2) count the document frequencies of ngrams; (3)
their raw frequency. It takes as input: - \texttt{X\_raw}: a list of
strings each corresponding to the raw text of a document -
\texttt{ngram\_range}: a tuple of two integers denoting the type of
ngrams you want to extract, e.g.~(1,2) denotes extracting unigrams and
bigrams. - \texttt{token\_pattern}: a string to be used within a regular
expression to extract all tokens. Note that data is already tokenised so
you could opt for a simple white space tokenisation. -
\texttt{stop\_words}: a list of stop words - \texttt{min\_df}: keep
ngrams with a minimum document frequency. - \texttt{keep\_topN}: keep
top-N more frequent ngrams.

and returns:

\begin{itemize}
\tightlist
\item
  \texttt{vocab}: a set of the n-grams that will be used as features.
\item
  \texttt{df}: a Counter (or dict) that contains ngrams as keys and
  their corresponding document frequency as values.
\item
  \texttt{ngram\_counts}: counts of each ngram in vocab
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{get\PYZus{}vocab}\PY{p}{(}\PY{n}{X\PYZus{}raw}\PY{p}{,}
              \PY{n}{ngram\PYZus{}range}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,}
              \PY{n}{token\PYZus{}pattern}\PY{o}{=}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{b[A\PYZhy{}Za\PYZhy{}z]}\PY{l+s+s1}{\PYZob{}}\PY{l+s+s1}{2,\PYZcb{}}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
              \PY{n}{min\PYZus{}df}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,}
              \PY{n}{keep\PYZus{}topN}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,}
              \PY{n}{stop\PYZus{}words}\PY{o}{=}\PY{n}{default\PYZus{}stop\PYZus{}words}\PY{p}{)}\PY{p}{:}

    \PY{n}{df} \PY{o}{=} \PY{n}{Counter}\PY{p}{(}\PY{p}{)}
    \PY{n}{ngram\PYZus{}counts} \PY{o}{=} \PY{n}{Counter}\PY{p}{(}\PY{p}{)}

    \PY{k}{for} \PY{n}{text} \PY{o+ow}{in} \PY{n}{X\PYZus{}raw}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} A list of ngrams for the given document `text`}
        \PY{n}{ngram\PYZus{}list} \PY{o}{=} \PY{n}{extract\PYZus{}ngrams}\PY{p}{(}\PY{n}{text}\PY{p}{,} \PY{n}{ngram\PYZus{}range}\PY{p}{,} \PY{n}{token\PYZus{}pattern}\PY{p}{,} \PY{n}{stop\PYZus{}words}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} Count document frequency}
        \PY{n}{df}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{n+nb}{set}\PY{p}{(}\PY{n}{ngram\PYZus{}list}\PY{p}{)}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} Count ngram frequency}
        \PY{n}{ngram\PYZus{}counts}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{n}{ngram} \PY{k}{for} \PY{n}{ngram} \PY{o+ow}{in} \PY{n}{ngram\PYZus{}list} \PY{k}{if} \PY{n}{df}\PY{p}{[}\PY{n}{ngram}\PY{p}{]} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{n}{min\PYZus{}df}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Extract ngram into vocab set}
    \PY{n}{vocab} \PY{o}{=} \PY{p}{\PYZob{}}\PY{n}{ngram} \PY{k}{for} \PY{n}{ngram}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n}{ngram\PYZus{}counts}\PY{o}{.}\PY{n}{most\PYZus{}common}\PY{p}{(}\PY{n}{keep\PYZus{}topN}\PY{p}{)}\PY{p}{\PYZcb{}}

    \PY{k}{return} \PY{n}{vocab}\PY{p}{,} \PY{n}{df}\PY{p}{,} \PY{n}{ngram\PYZus{}counts}
\end{Verbatim}
\end{tcolorbox}

    Now you should use \texttt{get\_vocab} to create your vocabulary and get
document and raw frequencies of unigrams:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{dev\PYZus{}vocab}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{get\PYZus{}vocab}\PY{p}{(}\PY{n}{topic\PYZus{}dev\PYZus{}texts}\PY{p}{,} \PY{n}{ngram\PYZus{}range}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{keep\PYZus{}topN}\PY{o}{=}\PY{l+m+mi}{5000}\PY{p}{)}
\PY{n}{train\PYZus{}vocab}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{get\PYZus{}vocab}\PY{p}{(}\PY{n}{topic\PYZus{}train\PYZus{}texts}\PY{p}{,} \PY{n}{ngram\PYZus{}range}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{keep\PYZus{}topN}\PY{o}{=}\PY{l+m+mi}{5000}\PY{p}{)}
\PY{n}{test\PYZus{}vocab}\PY{p}{,} \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{get\PYZus{}vocab}\PY{p}{(}\PY{n}{topic\PYZus{}test\PYZus{}texts}\PY{p}{,} \PY{n}{ngram\PYZus{}range}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{keep\PYZus{}topN}\PY{o}{=}\PY{l+m+mi}{5000}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Set order is random for every new run. Use `sorted` for reproducibility}
\PY{n}{vocab} \PY{o}{=} \PY{n+nb}{sorted}\PY{p}{(}\PY{n}{dev\PYZus{}vocab}\PY{o}{.}\PY{n}{union}\PY{p}{(}\PY{n}{train\PYZus{}vocab}\PY{p}{)}\PY{o}{.}\PY{n}{union}\PY{p}{(}\PY{n}{test\PYZus{}vocab}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    Then, you need to create vocabulary id -\textgreater{} word and word
-\textgreater{} id dictionaries for reference:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{vocab\PYZus{}id\PYZus{}to\PYZus{}word} \PY{o}{=} \PY{n+nb}{dict}\PY{p}{(}\PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{vocab}\PY{p}{)}\PY{p}{)}
\PY{n}{word\PYZus{}to\PYZus{}vocab\PYZus{}id} \PY{o}{=} \PY{p}{\PYZob{}}\PY{n}{v}\PY{p}{:} \PY{n}{k} \PY{k}{for} \PY{n}{k}\PY{p}{,} \PY{n}{v} \PY{o+ow}{in} \PY{n}{vocab\PYZus{}id\PYZus{}to\PYZus{}word}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{\PYZcb{}}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{convert-the-list-of-unigrams-into-a-list-of-vocabulary-indices}{%
\subsubsection{Convert the list of unigrams into a list of vocabulary
indices}\label{convert-the-list-of-unigrams-into-a-list-of-vocabulary-indices}}

Storing actual one-hot vectors into memory for all words in the entire
data set is prohibitive. Instead, we will store word indices in the
vocabulary and look-up the weight matrix. This is equivalent of doing a
dot product between an one-hot vector and the weight matrix.

First, represent documents in train, dev and test sets as lists of words
in the vocabulary:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{topic\PYZus{}dev\PYZus{}uni} \PY{o}{=} \PY{p}{[}
    \PY{n}{extract\PYZus{}ngrams}\PY{p}{(}\PY{n}{doc}\PY{p}{,} \PY{n}{ngram\PYZus{}range}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{vocab}\PY{o}{=}\PY{n}{vocab}\PY{p}{)}
    \PY{k}{for} \PY{n}{doc} \PY{o+ow}{in} \PY{n}{topic\PYZus{}dev\PYZus{}texts}
\PY{p}{]}
\PY{n}{topic\PYZus{}train\PYZus{}uni} \PY{o}{=} \PY{p}{[}
    \PY{n}{extract\PYZus{}ngrams}\PY{p}{(}\PY{n}{doc}\PY{p}{,} \PY{n}{ngram\PYZus{}range}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{vocab}\PY{o}{=}\PY{n}{vocab}\PY{p}{)}
    \PY{k}{for} \PY{n}{doc} \PY{o+ow}{in} \PY{n}{topic\PYZus{}train\PYZus{}texts}
\PY{p}{]}
\PY{n}{topic\PYZus{}test\PYZus{}uni} \PY{o}{=} \PY{p}{[}
    \PY{n}{extract\PYZus{}ngrams}\PY{p}{(}\PY{n}{doc}\PY{p}{,} \PY{n}{ngram\PYZus{}range}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{vocab}\PY{o}{=}\PY{n}{vocab}\PY{p}{)}
    \PY{k}{for} \PY{n}{doc} \PY{o+ow}{in} \PY{n}{topic\PYZus{}test\PYZus{}texts}
\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    Then convert them into lists of indices in the vocabulary:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{topic\PYZus{}dev\PYZus{}ids} \PY{o}{=} \PY{p}{[}\PY{p}{[}\PY{n}{word\PYZus{}to\PYZus{}vocab\PYZus{}id}\PY{p}{[}\PY{n}{uni}\PY{p}{]} \PY{k}{for} \PY{n}{uni} \PY{o+ow}{in} \PY{n}{unigrams}\PY{p}{]}
                 \PY{k}{for} \PY{n}{unigrams} \PY{o+ow}{in} \PY{n}{topic\PYZus{}dev\PYZus{}uni}\PY{p}{]}

\PY{n}{topic\PYZus{}train\PYZus{}ids} \PY{o}{=} \PY{p}{[}\PY{p}{[}\PY{n}{word\PYZus{}to\PYZus{}vocab\PYZus{}id}\PY{p}{[}\PY{n}{uni}\PY{p}{]} \PY{k}{for} \PY{n}{uni} \PY{o+ow}{in} \PY{n}{unigrams}\PY{p}{]}
                   \PY{k}{for} \PY{n}{unigrams} \PY{o+ow}{in} \PY{n}{topic\PYZus{}train\PYZus{}uni}\PY{p}{]}

\PY{n}{topic\PYZus{}test\PYZus{}ids} \PY{o}{=} \PY{p}{[}\PY{p}{[}\PY{n}{word\PYZus{}to\PYZus{}vocab\PYZus{}id}\PY{p}{[}\PY{n}{uni}\PY{p}{]} \PY{k}{for} \PY{n}{uni} \PY{o+ow}{in} \PY{n}{unigrams}\PY{p}{]}
                  \PY{k}{for} \PY{n}{unigrams} \PY{o+ow}{in} \PY{n}{topic\PYZus{}test\PYZus{}uni}\PY{p}{]}
\end{Verbatim}
\end{tcolorbox}

    Put the labels \texttt{Y} for train, dev and test sets into arrays:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} The labels start from 1, subtract 1 to make them start from 0}
\PY{n}{topic\PYZus{}dev\PYZus{}labels} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{topic\PYZus{}dev}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{label}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}
\PY{n}{topic\PYZus{}train\PYZus{}labels} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{topic\PYZus{}train}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{label}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}
\PY{n}{topic\PYZus{}test\PYZus{}labels} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{topic\PYZus{}test}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{label}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{network-architecture}{%
\section{Network Architecture}\label{network-architecture}}

Your network should pass each word index into its corresponding
embedding by looking-up on the embedding matrix and then compute the
first hidden layer \(\mathbf{h}_1\):

\[\mathbf{h}_1 = \frac{1}{|x|}\sum_i W^e_i, i \in x\]

where \(|x|\) is the number of words in the document and \(W^e\) is an
embedding matrix \(|V|\times d\), \(|V|\) is the size of the vocabulary
and \(d\) the embedding size.

Then \(\mathbf{h}_1\) should be passed through a ReLU activation
function:

\[\mathbf{a}_1 = relu(\mathbf{h}_1)\]

Finally the hidden layer is passed to the output layer:

\[\mathbf{y} = \text{softmax}(\mathbf{a}_1W^T) \] where \(W\) is a
matrix \(d \times |{\cal Y}|\), \(|{\cal Y}|\) is the number of classes.

During training, \(\mathbf{a}_1\) should be multiplied with a dropout
mask vector (elementwise) for regularisation before it is passed to the
output layer.

You can extend to a deeper architecture by passing a hidden layer to
another one:

\[\mathbf{h_i} = \mathbf{a}_{i-1}W_i^T \]

\[\mathbf{a_i} = relu(\mathbf{h_i}) \]

\hypertarget{network-training}{%
\section{Network Training}\label{network-training}}

First we need to define the parameters of our network by initiliasing
the weight matrices. For that purpose, you should implement the
\texttt{network\_weights} function that takes as input:

\begin{itemize}
\tightlist
\item
  \texttt{vocab\_size}: the size of the vocabulary
\item
  \texttt{embedding\_dim}: the size of the word embeddings
\item
  \texttt{hidden\_dim}: a list of the sizes of any subsequent hidden
  layers (for the Bonus). Empty if there are no hidden layers between
  the average embedding and the output layer
\item
  \texttt{num\_classes}: the number of the classes for the output layer
\end{itemize}

and returns:

\begin{itemize}
\tightlist
\item
  \texttt{W}: a dictionary mapping from layer index (e.g.~0 for the
  embedding matrix) to the corresponding weight matrix initialised with
  small random numbers (hint: use numpy.random.uniform with from -0.1 to
  0.1)
\end{itemize}

See the examples below for expected outputs. Make sure that the
dimensionality of each weight matrix is compatible with the previous and
next weight matrix, otherwise you won't be able to perform forward and
backward passes. Consider also using np.float32 precision to save
memory.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{network\PYZus{}weights}\PY{p}{(}\PY{n}{vocab\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{1000}\PY{p}{,} \PY{n}{embedding\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{300}\PY{p}{,} \PY{n}{hidden\PYZus{}dim}\PY{o}{=}\PY{p}{[}\PY{p}{]}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} fixing random seed for reproducibility}
    \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{123}\PY{p}{)}

    \PY{n}{dimensions} \PY{o}{=} \PY{p}{[}\PY{n}{vocab\PYZus{}size}\PY{p}{,} \PY{n}{embedding\PYZus{}dim}\PY{p}{]} \PY{o}{+} \PY{n}{hidden\PYZus{}dim} \PY{o}{+} \PY{p}{[}\PY{n}{num\PYZus{}classes}\PY{p}{]}

    \PY{c+c1}{\PYZsh{} Use \PYZdq{}He Weight Initialisation\PYZdq{}}
    \PY{n}{W} \PY{o}{=} \PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{o}{*}\PY{n}{size}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{float32}\PY{p}{)} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{l+m+mi}{2} \PY{o}{/} \PY{p}{(}\PY{n}{size}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{k}{for} \PY{n}{size} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{o}{*}\PY{p}{(}\PY{n}{dimensions}\PY{p}{[}\PY{n}{i}\PY{p}{:}\PY{p}{]} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{p}{]}

    \PY{k}{return} \PY{n}{W}
\end{Verbatim}
\end{tcolorbox}

    Then you need to develop a \texttt{softmax} function (same as in
Assignment 1) to be used in the output layer. It takes as input:

\begin{itemize}
\tightlist
\item
  \texttt{z}: array of real numbers
\end{itemize}

and returns:

\begin{itemize}
\tightlist
\item
  \texttt{smax}: the softmax of \texttt{z}
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{softmax}\PY{p}{(}\PY{n}{z}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Compute probability for each class}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{n}{e\PYZus{}z} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{n}{z}\PY{p}{)}
    \PY{k}{return} \PY{n}{e\PYZus{}z} \PY{o}{/} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{e\PYZus{}z}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1} \PY{k}{if} \PY{n}{e\PYZus{}z}\PY{o}{.}\PY{n}{ndim} \PY{o}{\PYZgt{}} \PY{l+m+mi}{1} \PY{k}{else} \PY{k+kc}{None}\PY{p}{,} \PY{n}{keepdims}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    Now you need to implement the categorical cross entropy loss by slightly
modifying the function from Assignment 1 to depend only on the true
label \texttt{y} and the class probabilities vector \texttt{y\_preds}:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{categorical\PYZus{}loss}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{y\PYZus{}preds}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{o}{\PYZhy{}}\PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{y\PYZus{}preds}\PY{p}{[}\PY{n}{y}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    Then, implement the \texttt{relu} function to introduce non-linearity
after each hidden layer of your network (during the forward pass):

\[relu(z_i)= max(z_i,0)\]

and the \texttt{relu\_derivative} function to compute its derivative
(used in the backward pass):

\begin{equation}
  \text{relu\_derivative}(z_i)=\begin{cases}
    0, & \text{if $z_i<=0$}.\\
    1, & \text{otherwise}.
  \end{cases}
\end{equation}

Note that both functions take as input a vector \(z\)

Hint use .copy() to avoid in place changes in array z

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{14}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{relu}\PY{p}{(}\PY{n}{z}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{clip}\PY{p}{(}\PY{n}{z}\PY{p}{,} \PY{n}{a\PYZus{}min}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{a\PYZus{}max}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}


\PY{k}{def} \PY{n+nf}{relu\PYZus{}derivative}\PY{p}{(}\PY{n}{z}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{clip}\PY{p}{(}\PY{n}{z}\PY{p}{,} \PY{n}{a\PYZus{}min}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{a\PYZus{}max}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    During training you should also apply a dropout mask element-wise after
the activation function (i.e.~vector of ones with a random percentage
set to zero). The \texttt{dropout\_mask} function takes as input:

\begin{itemize}
\tightlist
\item
  \texttt{size}: the size of the vector that we want to apply dropout
\item
  \texttt{dropout\_rate}: the percentage of elements that will be
  randomly set to zeros
\end{itemize}

and returns:

\begin{itemize}
\tightlist
\item
  \texttt{dropout\_vec}: a vector with binary values (0 or 1)
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{15}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{dropout\PYZus{}mask}\PY{p}{(}\PY{n}{size}\PY{p}{,} \PY{n}{dropout\PYZus{}rate}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{choice}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{size}\PY{p}{,} \PY{n}{p}\PY{o}{=}\PY{p}{[}\PY{n}{dropout\PYZus{}rate}\PY{p}{,} \PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{dropout\PYZus{}rate}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    Now you need to implement the \texttt{forward\_pass} function that
passes the input x through the network up to the output layer for
computing the probability for each class using the weight matrices in
\texttt{W}. The ReLU activation function should be applied on each
hidden layer.

\begin{itemize}
\tightlist
\item
  \texttt{x}: a list of vocabulary indices each corresponding to a word
  in the document (input)
\item
  \texttt{W}: a list of weight matrices connecting each part of the
  network, e.g.~for a network with a hidden and an output layer:
  W{[}0{]} is the weight matrix that connects the input to the first
  hidden layer, W{[}1{]} is the weight matrix that connects the hidden
  layer to the output layer.
\item
  \texttt{dropout\_rate}: the dropout rate that is used to generate a
  random dropout mask vector applied after each hidden layer for
  regularisation.
\end{itemize}

and returns:

\begin{itemize}
\tightlist
\item
  \texttt{out\_vals}: a dictionary of output values from each layer: h
  (the vector before the activation function), a (the resulting vector
  after passing h to the activation function), its dropout mask vector;
  and the prediction vector (probability for each class) from the output
  layer.
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{16}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{forward\PYZus{}pass}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{W}\PY{p}{,} \PY{n}{dropout\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{)}\PY{p}{:}

    \PY{n}{h\PYZus{}vecs} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{n}{a\PYZus{}vecs} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{n}{dropout\PYZus{}vecs} \PY{o}{=} \PY{p}{[}\PY{p}{]}

    \PY{c+c1}{\PYZsh{} Embedding layer}
    \PY{n}{h\PYZus{}1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{W}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{n}{x}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
    \PY{n}{h\PYZus{}vecs}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{h\PYZus{}1}\PY{p}{)}

    \PY{n}{a\PYZus{}1} \PY{o}{=} \PY{n}{relu}\PY{p}{(}\PY{n}{h\PYZus{}1}\PY{p}{)}
    \PY{n}{a\PYZus{}vecs}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{a\PYZus{}1}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Apply dropout mask to embedding layer}
    \PY{n}{dropout\PYZus{}vecs}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{dropout\PYZus{}mask}\PY{p}{(}\PY{n}{W}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{dropout\PYZus{}rate}\PY{p}{)}\PY{p}{)}
    \PY{n}{a\PYZus{}i} \PY{o}{=} \PY{n}{a\PYZus{}1} \PY{o}{*} \PY{n}{dropout\PYZus{}vecs}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}

    \PY{c+c1}{\PYZsh{} Loop through each hidden layer}
    \PY{k}{for} \PY{n}{weights} \PY{o+ow}{in} \PY{n}{W}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{:}
        \PY{n}{h\PYZus{}i} \PY{o}{=} \PY{n}{a\PYZus{}i}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{weights}\PY{p}{)}
        \PY{n}{h\PYZus{}vecs}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{h\PYZus{}i}\PY{p}{)}

        \PY{n}{a\PYZus{}i} \PY{o}{=} \PY{n}{relu}\PY{p}{(}\PY{n}{h\PYZus{}i}\PY{p}{)}
        \PY{n}{a\PYZus{}vecs}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{a\PYZus{}i}\PY{p}{)}

        \PY{n}{dropout\PYZus{}vecs}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{dropout\PYZus{}mask}\PY{p}{(}\PY{n}{weights}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{dropout\PYZus{}rate}\PY{p}{)}\PY{p}{)}
        \PY{n}{a\PYZus{}i} \PY{o}{*}\PY{o}{=} \PY{n}{dropout\PYZus{}vecs}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}

    \PY{c+c1}{\PYZsh{} Output layer: make prediction}
    \PY{n}{y} \PY{o}{=} \PY{n}{softmax}\PY{p}{(}\PY{n}{a\PYZus{}i}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{W}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}

    \PY{k}{return} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{h}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{h\PYZus{}vecs}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{a}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{a\PYZus{}vecs}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dropout\PYZus{}vec}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{dropout\PYZus{}vecs}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{y}\PY{p}{\PYZcb{}}
\end{Verbatim}
\end{tcolorbox}

    The \texttt{backward\_pass} function computes the gradients and update
the weights for each matrix in the network from the output to the input.
It takes as input

\begin{itemize}
\tightlist
\item
  \texttt{x}: a list of vocabulary indices each corresponding to a word
  in the document (input)
\item
  \texttt{y}: the true label
\item
  \texttt{W}: a list of weight matrices connecting each part of the
  network, e.g.~for a network with a hidden and an output layer:
  W{[}0{]} is the weight matrix that connects the input to the first
  hidden layer, W{[}1{]} is the weight matrix that connects the hidden
  layer to the output layer.
\item
  \texttt{out\_vals}: a dictionary of output values from a forward pass.
\item
  \texttt{learning\_rate}: the learning rate for updating the weights.
\item
  \texttt{freeze\_emb}: boolean value indicating whether the embedding
  weights will be updated.
\end{itemize}

and returns:

\begin{itemize}
\tightlist
\item
  \texttt{W}: the updated weights of the network.
\end{itemize}

Hint: the gradients on the output layer are similar to the multiclass
logistic regression.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{17}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{backward\PYZus{}pass}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{W}\PY{p}{,} \PY{n}{out\PYZus{}vals}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{0.001}\PY{p}{,} \PY{n}{freeze\PYZus{}emb}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} Compute the gradient on the output layer}
    \PY{n}{g} \PY{o}{=} \PY{n}{out\PYZus{}vals}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{out\PYZus{}vals}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{)} \PY{o}{==} \PY{n}{y}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Compute gradients on weights}
    \PY{n}{out\PYZus{}layer\PYZus{}input} \PY{o}{=} \PY{n}{out\PYZus{}vals}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{a}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{*} \PY{n}{out\PYZus{}vals}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dropout\PYZus{}vec}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
    \PY{n}{g\PYZus{}on\PYZus{}w} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{outer}\PY{p}{(}\PY{n}{g}\PY{p}{,} \PY{n}{out\PYZus{}layer\PYZus{}input}\PY{p}{)}\PY{o}{.}\PY{n}{T}

    \PY{c+c1}{\PYZsh{} Propagate the gradients w.r.t. the last hidden layer}
    \PY{n}{g} \PY{o}{=} \PY{n}{g}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{W}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{T}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Update output layer weight}
    \PY{n}{W}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n}{lr} \PY{o}{*} \PY{n}{g\PYZus{}on\PYZus{}w}

    \PY{c+c1}{\PYZsh{} Update each hidden layer}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{W}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} Convert the gradient on the layer\PYZsq{}s output}
        \PY{c+c1}{\PYZsh{} into a gradient before the activation function}
        \PY{n}{g} \PY{o}{*}\PY{o}{=} \PY{n}{relu\PYZus{}derivative}\PY{p}{(}\PY{n}{out\PYZus{}vals}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{h}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} Compute gradients on weights}
        \PY{n}{layer\PYZus{}input} \PY{o}{=} \PY{n}{out\PYZus{}vals}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{a}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{n}{i} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{]} \PY{o}{*} \PY{n}{out\PYZus{}vals}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dropout\PYZus{}vec}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{n}{i} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{]}
        \PY{n}{g\PYZus{}on\PYZus{}w} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{outer}\PY{p}{(}\PY{n}{g}\PY{p}{,} \PY{n}{layer\PYZus{}input}\PY{p}{)}\PY{o}{.}\PY{n}{T}

        \PY{c+c1}{\PYZsh{} Propagate the gradients w.r.t. the next hidden layer\PYZsq{}s activations}
        \PY{n}{g} \PY{o}{=} \PY{n}{g}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{W}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{.}\PY{n}{T}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} Update weights}
        \PY{n}{W}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n}{lr} \PY{o}{*} \PY{n}{g\PYZus{}on\PYZus{}w}

    \PY{c+c1}{\PYZsh{} Update embedding layer weight}
    \PY{k}{if} \PY{o+ow}{not} \PY{n}{freeze\PYZus{}emb}\PY{p}{:}
        \PY{n}{g} \PY{o}{*}\PY{o}{=} \PY{n}{relu\PYZus{}derivative}\PY{p}{(}\PY{n}{out\PYZus{}vals}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{h}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
        \PY{n}{W}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{n}{x}\PY{p}{]} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n}{lr} \PY{o}{*} \PY{n}{g}

    \PY{k}{return} \PY{n}{W}
\end{Verbatim}
\end{tcolorbox}

    Finally you need to modify SGD to support back-propagation by using the
\texttt{forward\_pass} and \texttt{backward\_pass} functions.

The \texttt{SGD} function takes as input:

\begin{itemize}
\tightlist
\item
  \texttt{X\_tr}: array of training data (vectors)
\item
  \texttt{Y\_tr}: labels of \texttt{X\_tr}
\item
  \texttt{W}: the weights of the network (dictionary)
\item
  \texttt{X\_dev}: array of development (i.e.~validation) data (vectors)
\item
  \texttt{Y\_dev}: labels of \texttt{X\_dev}
\item
  \texttt{lr}: learning rate
\item
  \texttt{dropout}: regularisation strength
\item
  \texttt{epochs}: number of full passes over the training data
\item
  \texttt{tolerance}: stop training if the difference between the
  current and previous validation loss is smaller than a threshold
\item
  \texttt{freeze\_emb}: boolean value indicating whether the embedding
  weights will be updated (to be used by the backward pass function).
\item
  \texttt{print\_progress}: flag for printing the training progress
  (train/validation loss)
\end{itemize}

and returns:

\begin{itemize}
\tightlist
\item
  \texttt{weights}: the weights learned
\item
  \texttt{training\_loss\_history}: an array with the average losses of
  the whole training set after each epoch
\item
  \texttt{validation\_loss\_history}: an array with the average losses
  of the whole development set after each epoch
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{18}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{SGD}\PY{p}{(}\PY{n}{X\PYZus{}tr}\PY{p}{,} \PY{n}{Y\PYZus{}tr}\PY{p}{,} \PY{n}{W}\PY{p}{,} \PY{n}{X\PYZus{}dev}\PY{p}{,} \PY{n}{Y\PYZus{}dev}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{0.001}\PY{p}{,} \PY{n}{dropout}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,}
        \PY{n}{tolerance}\PY{o}{=}\PY{l+m+mf}{0.001}\PY{p}{,} \PY{n}{freeze\PYZus{}emb}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{early\PYZus{}stopping}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{print\PYZus{}progress}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} fixing random seed for reproducibility}
    \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{123}\PY{p}{)}
    \PY{n}{training\PYZus{}loss\PYZus{}history} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{n}{validation\PYZus{}loss\PYZus{}history} \PY{o}{=} \PY{p}{[}\PY{p}{]}

    \PY{c+c1}{\PYZsh{} Create training tuples}
    \PY{n}{train\PYZus{}docs} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{zip}\PY{p}{(}\PY{n}{X\PYZus{}tr}\PY{p}{,} \PY{n}{Y\PYZus{}tr}\PY{p}{)}\PY{p}{)}

    \PY{k}{for} \PY{n}{epoch} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{epochs}\PY{p}{)}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} Randomise order in train\PYZus{}docs}
        \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{shuffle}\PY{p}{(}\PY{n}{train\PYZus{}docs}\PY{p}{)}

        \PY{k}{for} \PY{n}{x\PYZus{}i}\PY{p}{,} \PY{n}{y\PYZus{}i} \PY{o+ow}{in} \PY{n}{train\PYZus{}docs}\PY{p}{:}
            \PY{n}{W} \PY{o}{=} \PY{n}{backward\PYZus{}pass}\PY{p}{(}\PY{n}{x\PYZus{}i}\PY{p}{,} \PY{n}{y\PYZus{}i}\PY{p}{,} \PY{n}{W}\PY{p}{,} \PY{n}{forward\PYZus{}pass}\PY{p}{(}\PY{n}{x\PYZus{}i}\PY{p}{,} \PY{n}{W}\PY{p}{,} \PY{n}{dropout}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{p}{,} \PY{n}{freeze\PYZus{}emb}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} Monitor training loss}
        \PY{n}{cur\PYZus{}loss\PYZus{}tr} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{[}\PY{n}{categorical\PYZus{}loss}\PY{p}{(}\PY{n}{y\PYZus{}i}\PY{p}{,} \PY{n}{forward\PYZus{}pass}\PY{p}{(}\PY{n}{x\PYZus{}i}\PY{p}{,} \PY{n}{W}\PY{p}{,} \PY{n}{dropout}\PY{p}{)}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
                               \PY{k}{for} \PY{n}{x\PYZus{}i}\PY{p}{,} \PY{n}{y\PYZus{}i} \PY{o+ow}{in} \PY{n}{train\PYZus{}docs}\PY{p}{]}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} Monitor validation loss}
        \PY{n}{cur\PYZus{}loss\PYZus{}dev} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{[}\PY{n}{categorical\PYZus{}loss}\PY{p}{(}\PY{n}{y\PYZus{}i}\PY{p}{,} \PY{n}{forward\PYZus{}pass}\PY{p}{(}\PY{n}{x\PYZus{}i}\PY{p}{,} \PY{n}{W}\PY{p}{,} \PY{n}{dropout}\PY{p}{)}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
                                \PY{k}{for} \PY{n}{x\PYZus{}i}\PY{p}{,} \PY{n}{y\PYZus{}i} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{X\PYZus{}dev}\PY{p}{,} \PY{n}{Y\PYZus{}dev}\PY{p}{)}\PY{p}{]}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} Early stopping}
        \PY{k}{if} \PY{n}{early\PYZus{}stopping} \PY{o+ow}{and} \PY{n}{epoch} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0} \PY{o+ow}{and} \PY{n}{validation\PYZus{}loss\PYZus{}history}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{cur\PYZus{}loss\PYZus{}dev} \PY{o}{\PYZlt{}} \PY{n}{tolerance}\PY{p}{:}
            \PY{k}{break}
        \PY{k}{else}\PY{p}{:}
            \PY{n}{training\PYZus{}loss\PYZus{}history}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{cur\PYZus{}loss\PYZus{}tr}\PY{p}{)}
            \PY{n}{validation\PYZus{}loss\PYZus{}history}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{cur\PYZus{}loss\PYZus{}dev}\PY{p}{)}

        \PY{k}{if} \PY{n}{print\PYZus{}progress}\PY{p}{:}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Epoch: }\PY{l+s+si}{\PYZob{}}\PY{n}{epoch}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{ | Training loss: }\PY{l+s+si}{\PYZob{}}\PY{n}{cur\PYZus{}loss\PYZus{}tr}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{ | Validation loss: }\PY{l+s+si}{\PYZob{}}\PY{n}{cur\PYZus{}loss\PYZus{}dev}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

    \PY{k}{return} \PY{n}{W}\PY{p}{,} \PY{n}{training\PYZus{}loss\PYZus{}history}\PY{p}{,} \PY{n}{validation\PYZus{}loss\PYZus{}history}
\end{Verbatim}
\end{tcolorbox}

    Now you are ready to train and evaluate you neural net. First, you need
to define your network using the \texttt{network\_weights} function
followed by SGD with backprop:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{19}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{W} \PY{o}{=} \PY{n}{network\PYZus{}weights}\PY{p}{(}\PY{n}{vocab\PYZus{}size}\PY{o}{=}\PY{n+nb}{len}\PY{p}{(}\PY{n}{vocab}\PY{p}{)}\PY{p}{,}
                    \PY{n}{embedding\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{300}\PY{p}{,}
                    \PY{n}{num\PYZus{}classes}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{)}

\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{W}\PY{p}{)}\PY{p}{)}\PY{p}{:}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape W}\PY{l+s+si}{\PYZob{}}\PY{n}{i}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{ }\PY{l+s+si}{\PYZob{}}\PY{n}{W}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{.}\PY{n}{shape}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{W}\PY{p}{,} \PY{n}{tr\PYZus{}loss}\PY{p}{,} \PY{n}{dev\PYZus{}loss} \PY{o}{=} \PY{n}{SGD}\PY{p}{(}\PY{n}{X\PYZus{}tr}\PY{o}{=}\PY{n}{topic\PYZus{}train\PYZus{}ids}\PY{p}{,}
                           \PY{n}{Y\PYZus{}tr}\PY{o}{=}\PY{n}{topic\PYZus{}train\PYZus{}labels}\PY{p}{,}
                           \PY{n}{W}\PY{o}{=}\PY{n}{W}\PY{p}{,}
                           \PY{n}{X\PYZus{}dev}\PY{o}{=}\PY{n}{topic\PYZus{}dev\PYZus{}ids}\PY{p}{,}
                           \PY{n}{Y\PYZus{}dev}\PY{o}{=}\PY{n}{topic\PYZus{}dev\PYZus{}labels}\PY{p}{,}
                           \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{0.15}\PY{p}{,}
                           \PY{n}{dropout}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,}
                           \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Shape W0 (7610, 300)
Shape W1 (300, 3)
Epoch: 0 | Training loss: 1.0941477671067623 | Validation loss:
1.0965148843157566
Epoch: 1 | Training loss: 1.0683677614308555 | Validation loss:
1.0847936884412994
Epoch: 2 | Training loss: 0.6963450312406894 | Validation loss:
0.7978179175058632
Epoch: 3 | Training loss: 0.5057582228183621 | Validation loss:
0.5977412120295634
Epoch: 4 | Training loss: 0.350007241604076 | Validation loss:
0.4881203616660654
Epoch: 5 | Training loss: 0.2997647915233003 | Validation loss:
0.4866841181602369
    \end{Verbatim}

    Plot the learning process:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{20}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{tr\PYZus{}loss}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{dev\PYZus{}loss}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Monitoring (Average embedding)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_39_0.png}
    \end{center}
    { \hspace*{\fill} \\}

    Compute accuracy, precision, recall and F1-Score:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{21}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{preds\PYZus{}te} \PY{o}{=} \PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{forward\PYZus{}pass}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{W}\PY{p}{,} \PY{n}{dropout\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{0.0}\PY{p}{)}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
            \PY{k}{for} \PY{n}{x}\PY{p}{,} \PY{n}{y} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{topic\PYZus{}test\PYZus{}ids}\PY{p}{,} \PY{n}{topic\PYZus{}test\PYZus{}labels}\PY{p}{)}\PY{p}{]}
\PY{n}{args} \PY{o}{=} \PY{n}{topic\PYZus{}test\PYZus{}labels}\PY{p}{,} \PY{n}{preds\PYZus{}te}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{o}{*}\PY{n}{args}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Precision:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{precision\PYZus{}score}\PY{p}{(}\PY{o}{*}\PY{n}{args}\PY{p}{,} \PY{n}{average}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{macro}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Recall:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{recall\PYZus{}score}\PY{p}{(}\PY{o}{*}\PY{n}{args}\PY{p}{,} \PY{n}{average}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{macro}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{F1\PYZhy{}Score:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{f1\PYZus{}score}\PY{p}{(}\PY{o}{*}\PY{n}{args}\PY{p}{,} \PY{n}{average}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{macro}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy: 0.8577777777777778
Precision: 0.8575432340880728
Recall: 0.8577777777777778
F1-Score: 0.8572448556010199
    \end{Verbatim}

    \hypertarget{discuss-how-did-you-choose-model-hyperparameters}{%
\subsection{Discuss how did you choose model hyperparameters
?}\label{discuss-how-did-you-choose-model-hyperparameters}}

\hypertarget{preliminary-analysis}{%
\subsubsection{Preliminary Analysis}\label{preliminary-analysis}}

\hypertarget{fixed-set-ordering-of-vocabulary}{%
\paragraph{Fixed Set Ordering of
Vocabulary}\label{fixed-set-ordering-of-vocabulary}}

The vocabulary is stored as a set object to prevent duplicates. However,
Python Sets are unordered collection that do not record element position
or order of insertion
(\href{https://docs.python.org/3.8/library/stdtypes.html\#set-types-set-frozenset}{Python
Set Types}). This imples that the order of each vocabulary in the set is
not reproducible when the Python kernel is restarted.

After several experiments, it is concluded that different orders of the
vocabulary set produce different results. The highest and lowest
F1-Score obtained are \textasciitilde83\% and \textasciitilde81\%
respectively. To ensure that each set of hyperparameters combination
produce the same result (i.e.~not affected by ordering of vocabulary),
it is decided to sort the vocabulary alphabetically, as shown in the
code below.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Set order is random when kernel is restarted. Use \textasciigrave{}sorted\textasciigrave{} for reproducibility}
\NormalTok{vocab }\OperatorTok{=} \BuiltInTok{sorted}\NormalTok{(dev\_vocab.union(train\_vocab).union(test\_vocab))}
\end{Highlighting}
\end{Shaded}

\hypertarget{weight-initialisation-method-he-weight-initialisation}{%
\paragraph{Weight Initialisation Method: He Weight
Initialisation}\label{weight-initialisation-method-he-weight-initialisation}}

When using ReLU as the activation function and initialising the weights
using a uniform distribution method between the same lower and upper
boundary (e.g.~\texttt{np.random.uniform(-0.5,\ 0.5)}), then by uniform
distribution half of the layer will output zero when passing the ReLU
activation function
(\href{http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf}{Glorot
et al., 2011}).

\href{https://arxiv.org/abs/1502.01852}{He et al.~(2015)} suggested a
weight initialisation method that is suitable for the ReLU activation
function, as shown below
(\href{https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94}{Ref
1},
\href{https://towardsdatascience.com/weight-initialization-techniques-in-neural-networks-26c649eb3b78}{Ref
2}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{np.random.randn(layer\_dim[i], layer\_dim[i }\OperatorTok{+} \DecValTok{1}\NormalTok{]) }\OperatorTok{*}\NormalTok{ np.sqrt(}\DecValTok{2} \OperatorTok{/}\NormalTok{ (layer\_dim[i]))}
\end{Highlighting}
\end{Shaded}

Advantages of using He Weight Initialisation: 1. The method is
mathematically proven to be effective with ReLU activation function in a
published paper (\textasciitilde7810 Google Scholar citations)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  The weights of each layer is not fixed between the same lower and
  upper boundary. Now the weights of each layer are multipled by
  \texttt{np.sqrt(2\ /\ (layer\_dim{[}i{]}))}.
\item
  The process of hyperparameter optimisation is simpler as the initial
  boundary of weights (\texttt{init\_val} in \texttt{network\_weights})
  is excluded.

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Manually searching for the optimal initial weights is challenging.
    The optimal value could depends on the learning rate (\texttt{lr}),
    the embedding dimension (\texttt{embedding\_dim}). With these
    dependency relationship between different hyperparameters, having
    fewer hyperparameters to optimise would greatly reduce the
    difficulty to find the optimal hyperparameter combination.
  \end{enumerate}
\end{enumerate}

\hypertarget{relationship-between-hyperparameters-and-scores}{%
\paragraph{Relationship Between Hyperparameters and
Scores}\label{relationship-between-hyperparameters-and-scores}}

Before optimising the hyperparameters, it is required to know the
relationship between each hyperparameter and the resulting scores.

\hypertarget{embedding-dimension}{%
\subparagraph{Embedding dimension}\label{embedding-dimension}}

The large the embedding dimension, the longer the training time.

\begin{itemize}
\item
  If the embedding dimension is too large, then there might be an
  excessive amount of information that is not useful. This would have an
  adverse effect on performance if the model is overfitting.
\item
  If the embedding dimension is too small, then there might not be
  enough information to represent the word relation.
\end{itemize}

According to
\href{http://papers.nips.cc/paper/7368-on-the-dimensionality-of-word-embedd}{Yin
and Shen (2018)}, the performance of a word embedding is largely
affected by the chosen dimensionality. However, unlike learning rate,
there does not seems to be a general rule for choosing embedding
dimension.

\hypertarget{learning-rate}{%
\subparagraph{Learning Rate}\label{learning-rate}}

Choosing a good learning rate is challenging as every model differs from
each other. Generally, it is required to perform some preliminary
analysis on the model performance before the hyperparameters
optimisation.

Learning rate controls how much to change the model in response to the
estimated error each time the model weights are updated.

\begin{itemize}
\item
  If the learning rate is too large, the model may overshoot and lead to
  divergent behaviour (epochs required is low)
\item
  If the learning rate is too small, the model will require many updates
  to the weights before the loss is converged (epochs required is high)

  \begin{itemize}
  \tightlist
  \item
    However, if the tolerance is higher, the training is likely to stop
    in the first epoch due to early stopping
  \end{itemize}
\end{itemize}

\hypertarget{dropout-rate-regularisation}{%
\subparagraph{Dropout Rate
(Regularisation)}\label{dropout-rate-regularisation}}

Dropout rate is used to prevent the model from overfitting. If the
dropout rate is too high, most of the layer outputs will be ignored, and
thus the model would underfit.

\hypertarget{optimising-hyperparameters}{%
\subsubsection{Optimising
Hyperparameters}\label{optimising-hyperparameters}}

\hypertarget{limitations-of-trial-and-error-method}{%
\paragraph{Limitations of Trial and Error
Method}\label{limitations-of-trial-and-error-method}}

Although the trial and error method is a straightforward strategy and
requires no extra implementation, it is prone to the local optimum
problem. It is impractical to try every possible combination of learning
rate and regularisation strength to find the optimal result. Therefore,
the best achievable performance improvement through fine-tuning is
largely dependent on the initial set of values selected to explore the
hyperparameters.

Furthermore, another optimisation problem has arisen as the number of
hyperparameters to search is more than one. Some hyperparameters could
also have a dependency relationship with each other. This has lead to
another assumption that the optimisation order of hyperparameters may
have a certain impact on the final result.

\hypertarget{finding-the-baseline-performance}{%
\paragraph{Finding the Baseline
Performance}\label{finding-the-baseline-performance}}

According to the submission instructions, the model should achieve a
F1-score of at least 75\%. However, the default hyperparameters provided
is only getting a F1-score of 37.2\%. Therefore, the first step is to
find the hyperparameter combination that is able to achieve the baseline
performance.

\begin{longtable}[]{@{}ccccccccc@{}}
\toprule
Trial & embedding\_dim & lr & dropout & tolerance & Epochs & Tr. loss &
Val. loss & F1-score\tabularnewline
\midrule
\endhead
0 & 300 & 0.001 & 0.2 & 0.01 & 0 & 1.0984 & 1.0986 &
0.3721\tabularnewline
1 & 300 & 0.01 & 0.2 & 0.01 & 0 & 1.0982 & 1.0985 &
0.4255\tabularnewline
2 & 300 & 0.1 & 0.2 & 0.01 & 0 & 1.0959 & 1.0973 & 0.6668\tabularnewline
3 & 300 & 0.2 & 0.2 & 0.01 & 4 & 0.2310 & 0.4400 & 0.8164\tabularnewline
\bottomrule
\end{longtable}

As shown in the table above, the initial learning rate of 0.001 was too
low. After increasing the learning rate to 0.2, the F1-score has
improved to 79.5\%. As a result, F1-score of 81.6\% is set as the
baseline peformance. Any subsequent optimisations should therefore be
aiming to achieve performance higher than 81.6\%.

\hypertarget{optimising-learning-rate-and-tolerance}{%
\paragraph{Optimising Learning Rate and
Tolerance}\label{optimising-learning-rate-and-tolerance}}

In \textbf{Finding the Baseline Performance}, the learning rate is
increased in big step size, which resulted in small epochs. It would be
better for the model to converge with slower speed to prevent the
gradient from overshooting.

Fixed parameters: \texttt{embedding\_dim=300,\ dropout=0.2}

\begin{longtable}[]{@{}ccccccc@{}}
\toprule
Trial & lr & tolerance & Epochs & Tr. loss & Val. loss &
F1-score\tabularnewline
\midrule
\endhead
0 & 0.2 & 0.01 & 4 & 0.2310 & 0.4400 & 0.8164\tabularnewline
1 & 0.2 & 0.001 & 4 & 0.2310 & 0.4400 & 0.8164\tabularnewline
2 & 0.15 & 0.01 & 4 & 0.3500 & 0.4881 & 0.8164\tabularnewline
3 & 0.15 & 0.001 & 5 & 0.2997 & 0.4866 & 0.8572\tabularnewline
4 & 0.1 & 0.01 & 0 & 1.0959 & 1.0973 & 0.6668\tabularnewline
5 & 0.1 & 0.001 & 7 & 0.3303 & 0.4705 & 0.8486\tabularnewline
6 & 0.05 & 0.01 & 0 & 1.0973 & 1.0980 & 0.5775\tabularnewline
7 & 0.05 & 0.001 & 0 & 1.0973 & 1.0980 & 0.5775\tabularnewline
\bottomrule
\end{longtable}

With \texttt{lr=0.15,\ tolerance=0.001}, the model is now able to
achieve a F1-score of 85.7\%.

It is observed that for smaller learning rate (\textless{} 0.2), both
training and validation will decrease very slowly in the first 3 epochs.
Therefore, higher tolerance such as 0.01 will stop the training very
early, which is why a significant performance improvement is achieved
when the tolerance is lowered to 0.001.

\hypertarget{optimising-learning-rate}{%
\paragraph{Optimising Learning Rate}\label{optimising-learning-rate}}

With the tolerance fixed, the learning rate can now be fine-tuned in a
small step size to search for possible improvements.

Fixed parameters:
\texttt{embedding\_dim=300,\ dropout=0.2,\ tolerance=0.001}

\begin{longtable}[]{@{}cccccc@{}}
\toprule
Trial & lr & Epochs & Tr. loss & Val. loss & F1-score\tabularnewline
\midrule
\endhead
0 & 0.15 & 5 & 0.2997 & 0.4866 & 0.8572\tabularnewline
1 & 0.16 & 4 & 0.3134 & 0.4680 & 0.8181\tabularnewline
2 & 0.14 & 5 & 0.3293 & 0.5035 & 0.8550\tabularnewline
3 & 0.13 & 7 & 0.2500 & 0.4384 & 0.8470\tabularnewline
4 & 0.145 & 5 & 0.3137 & 0.4939 & 0.8561\tabularnewline
5 & 0.1475 & 5 & 0.3067 & 0.4901 & 0.8572\tabularnewline
6 & 0.151 & 4 & 0.3465 & 0.4859 & 0.8197\tabularnewline
7 & 0.1505 & 5 & 0.2983 & 0.4860 & 0.8560\tabularnewline
\bottomrule
\end{longtable}

No better \texttt{lr} has been found. 0.15 seems to be the maximum limit
of the learning rate to achieve the best F1-score with lowest training
and validation loss.

\hypertarget{optimising-dropout-rate}{%
\paragraph{Optimising Dropout Rate}\label{optimising-dropout-rate}}

Compared to other hyperparameters, the dropout rate is relatively easier
to optimise as it can only be a value between 0 and 1.

Fixed parameters:
\texttt{embedding\_dim=300,\ lr=0.15,\ tolerance=0.001}

\begin{longtable}[]{@{}cccccc@{}}
\toprule
Trial & dropout & Epochs & Tr. loss & Val. loss &
F1-score\tabularnewline
\midrule
\endhead
0 & 0.2 & 5 & 0.2997 & 0.4866 & 0.8572\tabularnewline
1 & 0.3 & 7 & 0.2490 & 0.3969 & 0.8053\tabularnewline
2 & 0.4 & 5 & 0.4433 & 0.4704 & 0.8545\tabularnewline
3 & 0.5 & 6 & 0.3479 & 0.4878 & 0.7705\tabularnewline
4 & 0.1 & 4 & 0.2929 & 0.4610 & 0.8055\tabularnewline
5 & 0.19 & 5 & 0.2609 & 0.4490 & 0.8539\tabularnewline
6 & 0.21 & 5 & 0.2963 & 0.4472 & 0.8537\tabularnewline
\bottomrule
\end{longtable}

No better dropout rate has been found. The initial dropout rate of 0.2
is considered to be the optimal value.

\hypertarget{optimising-embedding-dimension}{%
\paragraph{Optimising Embedding
Dimension}\label{optimising-embedding-dimension}}

Fixed parameters: \texttt{lr=0.15,\ tolerance=0.001,\ dropout=0.2}

\begin{longtable}[]{@{}cccccc@{}}
\toprule
Trial & embedding\_dim & Epochs & Tr. loss & Val. loss &
F1-score\tabularnewline
\midrule
\endhead
0 & 300 & 5 & 0.2997 & 0.4866 & 0.8572\tabularnewline
1 & 400 & 5 & 0.2723 & 0.3679 & 0.8513\tabularnewline
2 & 500 & 7 & 0.1673 & 0.3869 & 0.8095\tabularnewline
3 & 1000 & 8 & 0.1523 & 0.3579 & 0.8482\tabularnewline
4 & 200 & 2 & 0.6866 & 0.7152 & 0.6770\tabularnewline
5 & 100 & 4 & 0.2890 & 0.4059 & 0.8551\tabularnewline
6 & 50 & 4 & 0.2989 & 0.4216 & 0.7969\tabularnewline
\bottomrule
\end{longtable}

No better embedding dimension has been found. The initial dimension of
300 is considered to be the optimal value.

According to the table above, there is no clear indiciation whether
increasing or decreasing the embedding dimension would have positive or
negative effect on the performance.

When the embedding dimension is decreased to 200, the F1-score has to
67.7\%. However, the F1-score has increased back to 85.5\% after further
decreasing the dimension down to 100. Conversely, the performance did
not drop below 80\% when the dimension is increased up to 1000.

\hypertarget{summary}{%
\subsubsection{Summary}\label{summary}}

Factors that affect the model performance, excluding hyperparameters: 1.
The order of the vocabulary

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  The weight initialisation method
\end{enumerate}

The optimal value found for hyperparameters: 1. Embedding dimension: 300
2. Learning rate: 0.15 3. Dropout rate: 0.2 4. Tolerance: 0.001

    \hypertarget{use-pre-trained-embeddings}{%
\section{Use Pre-trained Embeddings}\label{use-pre-trained-embeddings}}

Now re-train the network using GloVe pre-trained embeddings. You need to
modify the \texttt{backward\_pass} function above to stop computing
gradients and updating weights of the embedding matrix.

Use the function below to obtain the embedding martix for your
vocabulary.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{22}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{get\PYZus{}glove\PYZus{}embeddings}\PY{p}{(}\PY{n}{f\PYZus{}zip}\PY{p}{,} \PY{n}{f\PYZus{}txt}\PY{p}{,} \PY{n}{word2id}\PY{p}{,} \PY{n}{emb\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{300}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Args:}
\PY{l+s+sd}{        f\PYZus{}zip: The compressed zip file}
\PY{l+s+sd}{        f\PYZus{}txt: The text file containing the embeddings}
\PY{l+s+sd}{        word2id: The word to vocabulary ID dictionary}
\PY{l+s+sd}{        emb\PYZus{}size: The size of the word embeddings}
\PY{l+s+sd}{        }
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{        Weights for the embedding layer}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{n}{w\PYZus{}emb} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{word2id}\PY{p}{)}\PY{p}{,} \PY{n}{emb\PYZus{}size}\PY{p}{)}\PY{p}{)}

    \PY{k}{with} \PY{n}{zipfile}\PY{o}{.}\PY{n}{ZipFile}\PY{p}{(}\PY{n}{f\PYZus{}zip}\PY{p}{)} \PY{k}{as} \PY{n}{z}\PY{p}{:}
        \PY{k}{with} \PY{n}{z}\PY{o}{.}\PY{n}{open}\PY{p}{(}\PY{n}{f\PYZus{}txt}\PY{p}{)} \PY{k}{as} \PY{n}{f}\PY{p}{:}
            \PY{k}{for} \PY{n}{line} \PY{o+ow}{in} \PY{n}{f}\PY{p}{:}
                \PY{n}{line} \PY{o}{=} \PY{n}{line}\PY{o}{.}\PY{n}{decode}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{utf\PYZhy{}8}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                \PY{n}{word} \PY{o}{=} \PY{n}{line}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}

                \PY{k}{if} \PY{n}{word} \PY{o+ow}{in} \PY{n}{vocab}\PY{p}{:}
                    \PY{n}{emb} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{line}\PY{o}{.}\PY{n}{strip}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{float32}\PY{p}{)}
                    \PY{n}{w\PYZus{}emb}\PY{p}{[}\PY{n}{word2id}\PY{p}{[}\PY{n}{word}\PY{p}{]}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{n}{emb}
    \PY{k}{return} \PY{n}{w\PYZus{}emb}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{23}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{w\PYZus{}glove} \PY{o}{=} \PY{n}{get\PYZus{}glove\PYZus{}embeddings}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{glove.840B.300d.zip}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{glove.840B.300d.txt}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
                               \PY{n}{word\PYZus{}to\PYZus{}vocab\PYZus{}id}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    First, initialise the weights of your network using the
\texttt{network\_weights} function. Second, replace the weigths of the
embedding matrix with \texttt{w\_glove}. Finally, train the network by
freezing the embedding weights:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{24}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{W} \PY{o}{=} \PY{n}{network\PYZus{}weights}\PY{p}{(}\PY{n}{vocab\PYZus{}size}\PY{o}{=}\PY{n+nb}{len}\PY{p}{(}\PY{n}{vocab}\PY{p}{)}\PY{p}{,}
                    \PY{n}{embedding\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{300}\PY{p}{,}
                    \PY{n}{num\PYZus{}classes}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{)}
\PY{n}{W}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{=} \PY{n}{w\PYZus{}glove}


\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{W}\PY{p}{)}\PY{p}{)}\PY{p}{:}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape W}\PY{l+s+si}{\PYZob{}}\PY{n}{i}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{ }\PY{l+s+si}{\PYZob{}}\PY{n}{W}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{.}\PY{n}{shape}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{W}\PY{p}{,} \PY{n}{tr\PYZus{}loss}\PY{p}{,} \PY{n}{dev\PYZus{}loss} \PY{o}{=} \PY{n}{SGD}\PY{p}{(}\PY{n}{X\PYZus{}tr}\PY{o}{=}\PY{n}{topic\PYZus{}train\PYZus{}ids}\PY{p}{,}
                           \PY{n}{Y\PYZus{}tr}\PY{o}{=}\PY{n}{topic\PYZus{}train\PYZus{}labels}\PY{p}{,}
                           \PY{n}{W}\PY{o}{=}\PY{n}{W}\PY{p}{,}
                           \PY{n}{X\PYZus{}dev}\PY{o}{=}\PY{n}{topic\PYZus{}dev\PYZus{}ids}\PY{p}{,}
                           \PY{n}{Y\PYZus{}dev}\PY{o}{=}\PY{n}{topic\PYZus{}dev\PYZus{}labels}\PY{p}{,}
                           \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,}
                           \PY{n}{dropout}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,}
                           \PY{n}{freeze\PYZus{}emb}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
                           \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Shape W0 (7610, 300)
Shape W1 (300, 3)
Epoch: 0 | Training loss: 0.39196310346400937 | Validation loss:
0.31723636739356187
Epoch: 1 | Training loss: 0.37113303463091457 | Validation loss:
0.2614393910627511
Epoch: 2 | Training loss: 0.32417943335986604 | Validation loss:
0.225559466206273
Epoch: 3 | Training loss: 0.3072251838800586 | Validation loss:
0.22377844162323404
Epoch: 4 | Training loss: 0.30196525400380764 | Validation loss:
0.21991298689775662
Epoch: 5 | Training loss: 0.2921520713493237 | Validation loss:
0.2116361607768931
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{25}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{tr\PYZus{}loss}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{dev\PYZus{}loss}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Monitoring (Pre\PYZhy{}trained embedding)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_48_0.png}
    \end{center}
    { \hspace*{\fill} \\}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{26}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{preds\PYZus{}te} \PY{o}{=} \PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{forward\PYZus{}pass}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{W}\PY{p}{,} \PY{n}{dropout\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{0.0}\PY{p}{)}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
            \PY{k}{for} \PY{n}{x}\PY{p}{,} \PY{n}{y} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{topic\PYZus{}test\PYZus{}ids}\PY{p}{,} \PY{n}{topic\PYZus{}test\PYZus{}labels}\PY{p}{)}\PY{p}{]}
\PY{n}{args} \PY{o}{=} \PY{n}{topic\PYZus{}test\PYZus{}labels}\PY{p}{,} \PY{n}{preds\PYZus{}te}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{o}{*}\PY{n}{args}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Precision:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{precision\PYZus{}score}\PY{p}{(}\PY{o}{*}\PY{n}{args}\PY{p}{,} \PY{n}{average}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{macro}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Recall:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{recall\PYZus{}score}\PY{p}{(}\PY{o}{*}\PY{n}{args}\PY{p}{,} \PY{n}{average}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{macro}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{F1\PYZhy{}Score:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{f1\PYZus{}score}\PY{p}{(}\PY{o}{*}\PY{n}{args}\PY{p}{,} \PY{n}{average}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{macro}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy: 0.8977777777777778
Precision: 0.897868613832692
Recall: 0.8977777777777778
F1-Score: 0.8967871274658675
    \end{Verbatim}

    \hypertarget{discuss-how-did-you-choose-model-hyperparameters}{%
\subsection{Discuss how did you choose model hyperparameters
?}\label{discuss-how-did-you-choose-model-hyperparameters}}

\hypertarget{initial-results}{%
\subsubsection{Initial Results}\label{initial-results}}

Using the optimal hyperparameters obtained in the previous model
(average embedding):

Parameters:
\texttt{embedding\_dim=300,\ lr=0.15,\ dropout=0.2,\ tolerance=0.001}

\begin{longtable}[]{@{}ccccccc@{}}
\toprule
Epochs & Training loss & Validation loss & Accuracy & Precision & Recall
& F1-score\tabularnewline
\midrule
\endhead
5 & 0.2798 & 0.2010 & 0.8966 & 0.8965 & 0.8966 & 0.8957\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{optimising-hyperparameters}{%
\subsubsection{Optimising
Hyperparameters}\label{optimising-hyperparameters}}

Since the dimension has to be 300 with the GloVe embeddings, there are
only two hyperparameters need to optimise: learning rate and dropout
rate.

\hypertarget{optimising-learning-rate}{%
\paragraph{Optimising Learning Rate}\label{optimising-learning-rate}}

Fixed parameters: \texttt{dropout=0.2,\ tolerance=0.001}

\begin{longtable}[]{@{}cccccc@{}}
\toprule
Trial & lr & Epochs & Tr. loss & Val. loss & F1-score\tabularnewline
\midrule
\endhead
0 & 0.15 & 5 & 0.2798 & 0.2010 & 0.8957\tabularnewline
1 & 0.2 & 5 & 0.2754 & 0.1960 & 0.8924\tabularnewline
2 & 0.3 & 5 & 0.2807 & 0.1962 & 0.8944\tabularnewline
3 & 0.4 & 3 & 0.2777 & 0.1968 & 0.8800\tabularnewline
4 & 0.1 & 5 & 0.2921 & 0.2116 & 0.8967\tabularnewline
5 & 0.05 & 3 & 0.3343 & 0.2523 & 0.8776\tabularnewline
6 & 0.01 & 11 & 0.3594 & 0.2760 & 0.8837\tabularnewline
\bottomrule
\end{longtable}

\texttt{lr=0.1} has the highest F1-score of 89.67\%, therefore it is
selected as the optimal learning rate. Although it has about 1\% higher
training and validation loss than \texttt{lr=0.15}, but the aim of
hyperparameters optimisation is to achieve the highest possible scores.

According to the results, changing the learning rate only produce a ±
2\% performance differences. It is also observed that the validation
loss is always lower than the training loss in this model.

\hypertarget{optimising-dropout-rate}{%
\paragraph{Optimising Dropout Rate}\label{optimising-dropout-rate}}

Fixed parameters: \texttt{lr=0.1,\ tolerance=0.001}

\begin{longtable}[]{@{}cccccc@{}}
\toprule
Trial & dropout & Epochs & Tr. loss & Val. loss &
F1-score\tabularnewline
\midrule
\endhead
0 & 0.2 & 5 & 0.2921 & 0.2116 & 0.8967\tabularnewline
1 & 0.1 & 2 & 0.3206 & 0.2033 & 0.8861\tabularnewline
2 & 0.3 & 3 & 0.3317 & 0.2379 & 0.8868\tabularnewline
3 & 0.5 & 3 & 0.3923 & 0.3119 & 0.8757\tabularnewline
4 & 0.7 & 7 & 0.4672 & 0.3993 & 0.8879\tabularnewline
5 & 0.9 & 11 & 0.7462 & 0.7007 & 0.8809\tabularnewline
\bottomrule
\end{longtable}

No better dropout rate has been found. \texttt{dropout=0.2} has the best
F1-score with lowest training and validation loss.

\hypertarget{summary}{%
\subsubsection{Summary}\label{summary}}

According to the results in the tables above, the performance of
\textbf{Pre-Trained Embeddings Model} is not affected significantly by
the value of learning rate and dropout rate. In comparison with
\textbf{Average Embeddings Model}, it is safe to conclude that the
performance this feedforward network is largely dependent on the initial
weights.

The optimal value found for hyperparameters: 1. Learning rate: 0.1 2.
Dropout rate: 0.2

Using pre-trained embeddings improved the total performance by
\textasciitilde4\% in comparison to randomly initialising the embedding
weights matrix.

    \hypertarget{extend-to-support-deeper-architectures-bonus}{%
\section{Extend to support deeper architectures
(Bonus)}\label{extend-to-support-deeper-architectures-bonus}}

Extend the network to support back-propagation for more hidden layers.
You need to modify the \texttt{backward\_pass} function above to compute
gradients and update the weights between intermediate hidden layers.
Finally, train and evaluate a network with a deeper architecture.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{27}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{W} \PY{o}{=} \PY{n}{network\PYZus{}weights}\PY{p}{(}\PY{n}{vocab\PYZus{}size}\PY{o}{=}\PY{n+nb}{len}\PY{p}{(}\PY{n}{vocab}\PY{p}{)}\PY{p}{,}
                    \PY{n}{embedding\PYZus{}dim}\PY{o}{=}\PY{l+m+mi}{300}\PY{p}{,}
                    \PY{n}{hidden\PYZus{}dim}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{1050}\PY{p}{]}\PY{p}{,}
                    \PY{n}{num\PYZus{}classes}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{)}
\PY{n}{W}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{=} \PY{n}{w\PYZus{}glove}

\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{W}\PY{p}{)}\PY{p}{)}\PY{p}{:}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Shape W}\PY{l+s+si}{\PYZob{}}\PY{n}{i}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{ }\PY{l+s+si}{\PYZob{}}\PY{n}{W}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{.}\PY{n}{shape}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{W}\PY{p}{,} \PY{n}{tr\PYZus{}loss}\PY{p}{,} \PY{n}{dev\PYZus{}loss} \PY{o}{=} \PY{n}{SGD}\PY{p}{(}\PY{n}{X\PYZus{}tr}\PY{o}{=}\PY{n}{topic\PYZus{}train\PYZus{}ids}\PY{p}{,}
                           \PY{n}{Y\PYZus{}tr}\PY{o}{=}\PY{n}{topic\PYZus{}train\PYZus{}labels}\PY{p}{,}
                           \PY{n}{W}\PY{o}{=}\PY{n}{W}\PY{p}{,}
                           \PY{n}{X\PYZus{}dev}\PY{o}{=}\PY{n}{topic\PYZus{}dev\PYZus{}ids}\PY{p}{,}
                           \PY{n}{Y\PYZus{}dev}\PY{o}{=}\PY{n}{topic\PYZus{}dev\PYZus{}labels}\PY{p}{,}
                           \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{0.07}\PY{p}{,}
                           \PY{n}{dropout}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,}
                           \PY{n}{freeze\PYZus{}emb}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
                           \PY{n}{early\PYZus{}stopping}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,}
                           \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Shape W0 (7610, 300)
Shape W1 (300, 1050)
Shape W2 (1050, 3)
Epoch: 0 | Training loss: 0.42479884818659164 | Validation loss:
0.367079861662041
Epoch: 1 | Training loss: 0.41608202469722905 | Validation loss:
0.34109356664542934
Epoch: 2 | Training loss: 0.4626220294584154 | Validation loss:
0.446505678173534
Epoch: 3 | Training loss: 0.3572658445781216 | Validation loss:
0.27464592135579863
Epoch: 4 | Training loss: 0.35519588083525444 | Validation loss:
0.2802350066339664
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{28}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{tr\PYZus{}loss}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{dev\PYZus{}loss}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training Monitoring (Pre\PYZhy{}trained + Hidden layers)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_53_0.png}
    \end{center}
    { \hspace*{\fill} \\}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{29}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{preds\PYZus{}te} \PY{o}{=} \PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n}{forward\PYZus{}pass}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{W}\PY{p}{,} \PY{n}{dropout\PYZus{}rate}\PY{o}{=}\PY{l+m+mf}{0.0}\PY{p}{)}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
            \PY{k}{for} \PY{n}{x}\PY{p}{,} \PY{n}{y} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{topic\PYZus{}test\PYZus{}ids}\PY{p}{,} \PY{n}{topic\PYZus{}test\PYZus{}labels}\PY{p}{)}\PY{p}{]}
\PY{n}{args} \PY{o}{=} \PY{n}{topic\PYZus{}test\PYZus{}labels}\PY{p}{,} \PY{n}{preds\PYZus{}te}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Accuracy:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{accuracy\PYZus{}score}\PY{p}{(}\PY{o}{*}\PY{n}{args}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Precision:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{precision\PYZus{}score}\PY{p}{(}\PY{o}{*}\PY{n}{args}\PY{p}{,} \PY{n}{average}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{macro}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Recall:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{recall\PYZus{}score}\PY{p}{(}\PY{o}{*}\PY{n}{args}\PY{p}{,} \PY{n}{average}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{macro}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{F1\PYZhy{}Score:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{f1\PYZus{}score}\PY{p}{(}\PY{o}{*}\PY{n}{args}\PY{p}{,} \PY{n}{average}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{macro}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Accuracy: 0.9033333333333333
Precision: 0.9031030443799583
Recall: 0.9033333333333333
F1-Score: 0.9020881592476028
    \end{Verbatim}

    \hypertarget{discuss-how-did-you-choose-model-hyperparameters}{%
\subsection{Discuss how did you choose model hyperparameters
?}\label{discuss-how-did-you-choose-model-hyperparameters}}

\hypertarget{searching-the-first-hidden-layer-dimension}{%
\subsubsection{Searching the First Hidden Layer
Dimension}\label{searching-the-first-hidden-layer-dimension}}

The first step is to find the hyperparameter combination that is able to
achieve the closest performance to \textbf{Pre-trained Embeddings
Model}.

\begin{longtable}[]{@{}cccccccc@{}}
\toprule
Trial & hidden\_dim & lr & dropout & Epochs & Tr. loss & Val. loss &
F1-score\tabularnewline
\midrule
\endhead
0 & 100 & 0.1 & 0.2 & 1 & 0.618 & 0.5411 & 0.8293\tabularnewline
1 & 200 & 0.1 & 0.2 & 1 & 0.5676 & 0.5865 & 0.8777\tabularnewline
2 & 300 & 0.1 & 0.2 & 2 & 0.4808 & 0.4006 & 0.8517\tabularnewline
3 & 200 & 0.05 & 0.2 & 4 & 0.5609 & 0.5045 & 0.8727\tabularnewline
4 & 200 & 0.05 & 0.1 & 5 & 0.4562 & 0.4156 & 0.8697\tabularnewline
5 & 1000 & 0.05 & 0.1 & 3 & 0.3257 & 0.2171 & 0.8913\tabularnewline
6 & 1000 & 0.05 & 0.2 & 3 & 0.3623 & 0.2515 & 0.8953\tabularnewline
\bottomrule
\end{longtable}

The hyperparameter combination
\texttt{hidden\_dim={[}1000{]},\ lr=0.05,\ dropout=0.2} has achieve a
F1-score of 89.53\%, which is only 0.14\% lower than \textbf{Pre-trained
Embeddings Model}.

\hypertarget{relationship-between-hyperparameters-and-loss}{%
\subsubsection{Relationship between Hyperparameters and
Loss}\label{relationship-between-hyperparameters-and-loss}}

According to the table in \textbf{Searching the First Hidden Layer
Dimension},

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Increasing hidden layer dimension will decrease training loss
\item
  Increasing dropout rate will increase training and validation loss
\end{enumerate}

The aim is to adjust each hyperparameter appropriately to reach about
30\% final training loss and 20\% final validation loss. The model
appear to be producing the best performance when both losses fall into
the respective range.

\hypertarget{further-trial-and-error}{%
\subsubsection{Further Trial and Error}\label{further-trial-and-error}}

\begin{longtable}[]{@{}cccccccc@{}}
\toprule
Trial & hidden\_dim & lr & dropout & Epochs & Tr. loss & Val. loss &
F1-score\tabularnewline
\midrule
\endhead
0 & 500 & 0.05 & 0.2 & 3 & 0.4220 & 0.3357 & 0.8683\tabularnewline
1 & 500 & 0.1 & 0.2 & 3 & 0.4073 & 0.3223 & 0.8702\tabularnewline
2 & 500 & 0.2 & 0.2 & 3 & 0.4212 & 0.3500 & 0.8689\tabularnewline
3 & 600 & 0.2 & 0.2 & 0 & 0.5173 & 0.3960 & 0.8327\tabularnewline
4 & 600 & 0.1 & 0.3 & 2 & 0.4654 & 0.3992 & 0.8704\tabularnewline
5 & 1000 & 0.01 & 0.3 & 10 & 0.4389 & 0.3525 & 0.8928\tabularnewline
6 & 1100 & 0.01 & 0.3 & 6 & 0.4626 & 0.4242 & 0.8938\tabularnewline
7 & 1200 & 0.01 & 0.3 & 4 & 0.4719 & 0.4409 & 0.8889\tabularnewline
8 & 1100 & 0.012 & 0.3 & 2 & 0.5108 & 0.4725 & 0.8936\tabularnewline
9 & 1100 & 0.12 & 0.25 & 2 & 0.4661 & 0.4189 & 0.8926\tabularnewline
\bottomrule
\end{longtable}

No better hyperparameter values have been found.

\hypertarget{disable-early-stopping}{%
\subsubsection{Disable Early Stopping}\label{disable-early-stopping}}

After further analysis, it is observed that the overall trend of
validation loss is decreasing. However, there would be irregular
``spikes'' that trigger early stopping and stop the training process
earlier then the optimal case.

To address this issue, an extra argument \texttt{early\_stopping} is
added to the \texttt{SGD()} function. As a result, the training process
is now controlled by the \texttt{epochs} parameter.

\begin{longtable}[]{@{}cccccccc@{}}
\toprule
Trial & hidden\_dim & lr & dropout & Epochs & Tr. loss & Val. loss &
F1-score\tabularnewline
\midrule
\endhead
0 & 1000 & 0.05 & 0.2 & 5 & 0.3482 & 0.2765 & 0.8953\tabularnewline
1 & 1050 & 0.05 & 0.2 & 5 & 0.3568 & 0.2853 & 0.9011\tabularnewline
2 & 1100 & 0.05 & 0.2 & 5 & 0.3710 & 0.2544 & 0.8935\tabularnewline
3 & 1050 & 0.055 & 0.15 & 5 & 0.3198 & 0.2445 & 0.8980\tabularnewline
4 & 1050 & 0.055 & 0.2 & 5 & 0.3559 & 0.2835 & 0.9011\tabularnewline
5 & 1050 & 0.06 & 0.2 & 5 & 0.3553 & 0.2821 & 0.9011\tabularnewline
6 & 1050 & 0.07 & 0.2 & 5 & 0.3551 & 0.2802 & 0.9020\tabularnewline
7 & 1050 & 0.08 & 0.2 & 5 & 0.3559 & 0.2791 & 0.9009\tabularnewline
8 & 1050 & 0.075 & 0.2 & 5 & 0.3554 & 0.2796 & 0.9020\tabularnewline
9 & 1050 & 0.07 & 0.2 & 10 & 0.3706 & 0.2861 & 0.8824\tabularnewline
\bottomrule
\end{longtable}

The hyperparameter values
\texttt{hidden\_dim={[}1050{]},\ lr=0.07,\ dropout=0.2,\ epochs=5} has
achieved a F1-score of 90.2\%.

\hypertarget{attempts-on-double-hidden-layer-dimensions}{%
\subsubsection{Attempts on Double Hidden Layer
Dimensions}\label{attempts-on-double-hidden-layer-dimensions}}

\begin{longtable}[]{@{}cccccccc@{}}
\toprule
Trial & hidden\_dim & lr & dropout & Epochs & Tr. loss & Val. loss &
F1-score\tabularnewline
\midrule
\endhead
0 & 525, 525 & 0.07 & 0.2 & 5 & 0.3783 & 0.2741 & 0.8973\tabularnewline
1 & 1000, 50 & 0.07 & 0.2 & 5 & 0.4345 & 0.2467 & 0.8920\tabularnewline
2 & 1000, 50 & 0.1 & 0.2 & 5 & 0.4678 & 0.2628 & 0.8954\tabularnewline
3 & 1000, 50 & 0.15 & 0.2 & 5 & 0.4882 & 0.3367 & 0.8927\tabularnewline
4 & 1000, 50 & 0.1 & 0.2 & 10 & 0.4033 & 0.2668 & 0.8862\tabularnewline
5 & 300, 300 & 0.1 & 0.2 & 10 & 0.4323 & 0.4404 & 0.8743\tabularnewline
\bottomrule
\end{longtable}

Unfortunately, the attemps on double hidden layer dimensions did not
manage to achieve a new high score.

\hypertarget{summary}{%
\subsubsection{Summary}\label{summary}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Adding a hidden layer will increase the training time due to the extra
  dimensions, but this does not ensure that a good performance will be
  produced.
\item
  According to the optimisation attemps, the lowest F1-score obtained is
  82.93\% in \textbf{Searching the First Hidden Layer Dimension}.
  Therefore, this model is still better than \textbf{Average Embedding
  Model} considering that it requires more optimisation effort and has
  higher implementation complexity.
\item
  As shown in the plot \textbf{Training Monitoring - (Pre-trained +
  Hidden layers)} above, the sudden ``spike'' increase of both training
  and validation does not always indicate a negative outcome. The model
  still managed to achieve a 90.2\% of F1-score in this case.
\item
  It is shown that the performance of \textbf{Pre-Trained Embeddings +
  Hidden Layer Model} can be better than \textbf{Pre-Trained Embeddings
  Model}. However, a significant amount of time is required to adjust
  each hyperparameters to achieve the optimal result.
\end{enumerate}

    \hypertarget{full-results}{%
\section{Full Results}\label{full-results}}

\begin{longtable}[]{@{}ccccc@{}}
\toprule
\begin{minipage}[b]{0.52\columnwidth}\centering
Model\strut
\end{minipage} & \begin{minipage}[b]{0.09\columnwidth}\centering
Accuracy\strut
\end{minipage} & \begin{minipage}[b]{0.10\columnwidth}\centering
Precision\strut
\end{minipage} & \begin{minipage}[b]{0.07\columnwidth}\centering
Recall\strut
\end{minipage} & \begin{minipage}[b]{0.09\columnwidth}\centering
F1-Score\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.52\columnwidth}\centering
Average Embedding\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering
0.8577\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering
0.8575\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
0.8577\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering
0.8572\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.52\columnwidth}\centering
Average Embedding (Pre-trained)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering
0.8977\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering
0.8979\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
0.8977\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering
0.8967\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.52\columnwidth}\centering
Average Embedding (Pre-trained) + X hidden layers (BONUS)\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering
0.9033\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\centering
0.9031\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\centering
0.9033\strut
\end{minipage} & \begin{minipage}[t]{0.09\columnwidth}\centering
0.9020\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}


    % Add a bibliography block to the postdoc



\end{document}
