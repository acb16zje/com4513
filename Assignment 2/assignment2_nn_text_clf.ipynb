{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [COM4513-6513] Assignment 2: Text Classification with a Feedforward Network\n",
    "\n",
    "### Instructor: Nikos Aletras\n",
    "\n",
    "The goal of this assignment is to develop a Feedforward network for text classification. \n",
    "\n",
    "For that purpose, you will implement:\n",
    "\n",
    "- Text processing methods for transforming raw text data into input vectors for your network  (**1 mark**)\n",
    "- A Feedforward network consisting of:\n",
    "    - **One-hot** input layer mapping words into an **Embedding weight matrix** (**1 mark**)\n",
    "    - **One hidden layer** computing the mean embedding vector of all words in input followed by a **ReLU activation function** (**1 mark**)\n",
    "    - **Output layer** with a **softmax** activation. (**1 mark**)\n",
    "- The Stochastic Gradient Descent (SGD) algorithm with **back-propagation** to learn the weights of your Neural network. Your algorithm should:\n",
    "    - Use (and minimise) the **Categorical Cross-entropy loss** function (**1 mark**)\n",
    "    - Perform a **Forward pass** to compute intermediate outputs (**4 marks**)\n",
    "    - Perform a **Backward pass** to compute gradients and update all sets of weights (**4 marks**)\n",
    "    - Implement and use **Dropout** after each hidden layer for regularisation (**2 marks**)\n",
    "- Discuss how did you choose hyperparameters? You can tune the learning rate (hint: choose small values), embedding size {e.g. 50, 300, 500}, the dropout rate {e.g. 0.2, 0.5} and the learning rate. Please use tables or graphs to show training and validation performance for each hyperparam combination  (**2 marks**). \n",
    "- After training the model, plot the learning process (i.e. training and validation loss in each epoch) using a line plot and report accuracy.\n",
    "- Re-train your network by using pre-trained embeddings ([GloVe](https://nlp.stanford.edu/projects/glove/)) trained on large corpora. Instead of randomly initialising the embedding weights matrix, you should initialise it with the pre-trained weights. During training, you should not update them (i.e. weight freezing) and backprop should stop before computing gradients for updating embedding weights. Report results by performing hyperparameter tuning and plotting the learning process. Do you get better performance? (**3 marks**).\n",
    "\n",
    "- **BONUS:** Extend you Feedforward network by adding more hidden layers (e.g. one more). How does it affect the performance? Note: You need to repeat hyperparameter tuning, but the number of combinations grows exponentially. Therefore, you need to choose a subset of all possible combinations (**+2 extra marks**)\n",
    "\n",
    "\n",
    "### Data \n",
    "\n",
    "The data you will use for Task 2 is a subset of the [AG News Corpus](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html) and you can find it in the `./data_topic` folder in CSV format:\n",
    "\n",
    "- `data_topic/train.csv`: contains 2,400 news articles, 800 for each class to be used for training.\n",
    "- `data_topic/dev.csv`: contains 150 news articles, 50 for each class to be used for hyperparameter selection and monitoring the training process.\n",
    "- `data_topic/test.csv`: contains 900 news articles, 300 for each class to be used for testing.\n",
    "\n",
    "\n",
    "### Pre-trained Embeddings\n",
    "\n",
    "You can download pre-trained GloVe embeddings trained on Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download) from [here](http://nlp.stanford.edu/data/glove.840B.300d.zip). No need to unzip, the file is large.\n",
    "\n",
    "\n",
    "### Save Memory\n",
    "\n",
    "To save RAM, when you finish each experiment you can delete the weights of your network using `del W` followed by Python's garbage collector `gc.collect()`\n",
    "\n",
    "\n",
    "### Submission Instructions\n",
    "\n",
    "You should submit a Jupyter Notebook file (assignment2.ipynb) and an exported PDF version (you can do it from Jupyter: `File->Download as->PDF via Latex`).\n",
    "\n",
    "You are advised to follow the code structure given in this notebook by completing all given funtions. You can also write any auxilliary/helper functions (and arguments for the functions) that you might need but note that you can provide a full solution without any such functions. Similarly, you can just use only the packages imported below but you are free to use any functionality from the [Python Standard Library](https://docs.python.org/2/library/index.html), NumPy, SciPy and Pandas. You are not allowed to use any third-party library such as Scikit-learn (apart from metric functions already provided), NLTK, Spacy, Keras etc.. You are allowed to re-use your code from Assignment 1.\n",
    "\n",
    "Please make sure to comment your code. You should also mention if you've used Windows to write and test your code. There is no single correct answer on what your accuracy should be, but correct implementations usually achieve F1 of ~75-80% and ~85% without and with using pre-trained embeddings respectively. \n",
    "\n",
    "This assignment will be marked out of 20. It is worth 20\\% of your final grade in the module. If you implement the bonus question you can get up to 2 extra points but your final grade will be capped at 20.\n",
    "\n",
    "The deadline for this assignment is **23:59 on Mon, 18 May 2020** and it needs to be submitted via Blackboard (MOLE). Standard departmental penalties for lateness will be applied. We use a range of strategies to detect [unfair means](https://www.sheffield.ac.uk/ssid/unfair-means/index), including Turnitin which helps detect plagiarism, so make sure you do not plagiarise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:00:18.625532Z",
     "start_time": "2020-04-02T15:00:17.377733Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from time import localtime, strftime\n",
    "from scipy.stats import spearmanr,pearsonr\n",
    "import zipfile\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Raw texts into training and development data\n",
    "\n",
    "First, you need to load the training, development and test sets from their corresponding CSV files (tip: you can use Pandas dataframes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:39.748484Z",
     "start_time": "2020-04-02T14:26:39.727404Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_dev = pd.read_csv('data_topic/dev.csv', names=['label', 'text'])\n",
    "topic_test = pd.read_csv('data_topic/test.csv', names=['label', 'text'])\n",
    "topic_train = pd.read_csv('data_topic/train.csv', names=['label', 'text'])\n",
    "\n",
    "topic_dev_texts = list(topic_dev['text'])\n",
    "topic_train_texts = list(topic_train['text'])\n",
    "topic_test_texts = list(topic_test['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create input representations\n",
    "\n",
    "To train your Feedforward network, you first need to obtain input representations given a vocabulary. One-hot encoding requires large memory capacity. Therefore, we will instead represent documents as lists of vocabulary indices (each word corresponds to a vocabulary index). \n",
    "\n",
    "\n",
    "## Text Pre-Processing Pipeline\n",
    "\n",
    "To obtain a vocabulary of words. You should: \n",
    "- tokenise all texts into a list of unigrams (tip: you can re-use the functions from Assignment 1) \n",
    "- remove stop words (using the one provided or one of your preference) \n",
    "- remove unigrams appearing in less than K documents\n",
    "- use the remaining to create a vocabulary of the top-N most frequent unigrams in the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:40.851926Z",
     "start_time": "2020-04-02T14:26:40.847500Z"
    }
   },
   "outputs": [],
   "source": [
    "default_stop_words = {\n",
    "    'a', 'ad', 'after', 'again', 'all', 'also', 'am', 'an', 'and', 'any',\n",
    "    'are', 'as', 'at', 'be', 'because', 'been', 'being', 'between', 'both',\n",
    "    'but', 'by', 'can', 'could', 'does', 'each', 'ed', 'eg', 'either', 'etc',\n",
    "    'even', 'ever', 'every', 'for', 'from', 'had', 'has', 'have', 'he', 'her',\n",
    "    'hers', 'herself', 'him', 'himself', 'his', 'i', 'ie', 'if', 'in', 'inc',\n",
    "    'into', 'is', 'it', 'its', 'itself', 'li', 'll', 'ltd', 'may', 'maybe',\n",
    "    'me', 'might', 'mine', 'minute', 'minutes', 'must', 'my', 'myself',\n",
    "    'neither', 'nor', 'now', 'of', 'on', 'only', 'or', 'other', 'our', 'ours',\n",
    "    'ourselves', 'own', 'same', 'seem', 'seemed', 'shall', 'she', 'some',\n",
    "    'somehow', 'something', 'sometimes', 'somewhat', 'somewhere', 'spoiler',\n",
    "    'spoilers', 'such', 'suppose', 'that', 'the', 'their', 'theirs', 'them',\n",
    "    'themselves', 'there', 'these', 'they', 'this', 'those', 'thus', 'to',\n",
    "    'today', 'tomorrow', 'us', 've', 'vs', 'was', 'we', 'were', 'what',\n",
    "    'whatever', 'when', 'whenever', 'where', 'whereby', 'which', 'who', 'whom',\n",
    "    'whose', 'will', 'with', 'yesterday', 'you', 'your', 'yours', 'yourself',\n",
    "    'yourselves'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram extraction from a document\n",
    "\n",
    "You first need to implement the `extract_ngrams` function. It takes as input:\n",
    "- `x_raw`: a string corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `vocab`: a given vocabulary. It should be used to extract specific features.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- a list of all extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:41.505459Z",
     "start_time": "2020-04-02T14:26:41.498388Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_ngrams(x_raw,\n",
    "                   ngram_range=(1, 3),\n",
    "                   token_pattern=r'\\b[A-Za-z]{2,}\\b',\n",
    "                   stop_words=default_stop_words,\n",
    "                   vocab=None):\n",
    "\n",
    "    tokens = [\n",
    "        word.lower() for word in re.findall(token_pattern, x_raw)\n",
    "        if word.lower() not in stop_words\n",
    "    ]\n",
    "\n",
    "    ngrams = []\n",
    "\n",
    "    for n in range(ngram_range[0], ngram_range[1] + 1):\n",
    "        if n == 1:\n",
    "            # Create unigram by concatenating list\n",
    "            ngrams += tokens\n",
    "        else:\n",
    "            # Create bigram / trigram by unzipping list\n",
    "            ngrams += zip(*(tokens[i:] for i in range(n)))\n",
    "\n",
    "    return [ngram for ngram in ngrams if ngram in vocab] if vocab else ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocabulary of n-grams\n",
    "\n",
    "Then the `get_vocab` function will be used to (1) create a vocabulary of ngrams; (2) count the document frequencies of ngrams; (3) their raw frequency. It takes as input:\n",
    "- `X_raw`: a list of strings each corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `min_df`: keep ngrams with a minimum document frequency.\n",
    "- `keep_topN`: keep top-N more frequent ngrams.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `vocab`: a set of the n-grams that will be used as features.\n",
    "- `df`: a Counter (or dict) that contains ngrams as keys and their corresponding document frequency as values.\n",
    "- `ngram_counts`: counts of each ngram in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:42.563876Z",
     "start_time": "2020-04-02T14:26:42.557967Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_vocab(X_raw,\n",
    "              ngram_range=(1, 3),\n",
    "              token_pattern=r'\\b[A-Za-z]{2,}\\b',\n",
    "              min_df=1,\n",
    "              keep_topN=None,\n",
    "              stop_words=default_stop_words):\n",
    "\n",
    "    df = Counter()\n",
    "    ngram_counts = Counter()\n",
    "\n",
    "    for text in X_raw:\n",
    "        # A list of ngrams for the given document `text`\n",
    "        ngram_list = extract_ngrams(text, ngram_range, token_pattern, stop_words)\n",
    "        \n",
    "        # Count document frequency\n",
    "        df.update(set(ngram_list))\n",
    "\n",
    "        # Count ngram frequency\n",
    "        ngram_counts.update(ngram for ngram in ngram_list if df[ngram] >= min_df)\n",
    "    \n",
    "    # Extract ngram into vocab set\n",
    "    vocab = {ngram for ngram, _ in ngram_counts.most_common(keep_topN)}\n",
    "\n",
    "    return vocab, df, ngram_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should use `get_vocab` to create your vocabulary and get document and raw frequencies of unigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:43.577997Z",
     "start_time": "2020-04-02T14:26:43.478950Z"
    }
   },
   "outputs": [],
   "source": [
    "dev_vocab, _, _ = get_vocab(topic_dev_texts, ngram_range=(1, 1), keep_topN=5000)\n",
    "train_vocab, _, _ = get_vocab(topic_train_texts, ngram_range=(1, 1), keep_topN=5000)\n",
    "test_vocab, _, _ = get_vocab(topic_test_texts, ngram_range=(1, 1), keep_topN=5000)\n",
    "\n",
    "# Set order is random for every new run. Use `sorted` for reproducibility\n",
    "vocab = sorted(dev_vocab.union(train_vocab).union(test_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you need to create vocabulary id -> word and word -> id dictionaries for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:44.069661Z",
     "start_time": "2020-04-02T14:26:44.065058Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_id_to_word = dict(enumerate(vocab))\n",
    "word_to_vocab_id = {v: k for k, v in vocab_id_to_word.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the list of unigrams  into a list of vocabulary indices\n",
    "\n",
    "Storing actual one-hot vectors into memory for all words in the entire data set is prohibitive. Instead, we will store word indices in the vocabulary and look-up the weight matrix. This is equivalent of doing a dot product between an one-hot vector and the weight matrix. \n",
    "\n",
    "First, represent documents in train, dev and test sets as lists of words in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dev_uni = [\n",
    "    extract_ngrams(doc, ngram_range=(1, 1), vocab=vocab)\n",
    "    for doc in topic_dev_texts\n",
    "]\n",
    "topic_train_uni = [\n",
    "    extract_ngrams(doc, ngram_range=(1, 1), vocab=vocab)\n",
    "    for doc in topic_train_texts\n",
    "]\n",
    "topic_test_uni = [\n",
    "    extract_ngrams(doc, ngram_range=(1, 1), vocab=vocab)\n",
    "    for doc in topic_test_texts\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then convert them into lists of indices in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:45.752658Z",
     "start_time": "2020-04-02T14:26:45.730409Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_dev_ids = [[word_to_vocab_id[uni] for uni in unigrams]\n",
    "                 for unigrams in topic_dev_uni]\n",
    "\n",
    "topic_train_ids = [[word_to_vocab_id[uni] for uni in unigrams]\n",
    "                   for unigrams in topic_train_uni]\n",
    "\n",
    "topic_test_ids = [[word_to_vocab_id[uni] for uni in unigrams]\n",
    "                  for unigrams in topic_test_uni]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the labels `Y` for train, dev and test sets into arrays: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:03:13.183996Z",
     "start_time": "2020-04-02T15:03:13.077575Z"
    }
   },
   "outputs": [],
   "source": [
    "# The labels start from 1, subtract 1 to make them start from 0\n",
    "topic_dev_labels = np.array(topic_dev['label']) - 1\n",
    "topic_train_labels = np.array(topic_train['label']) - 1\n",
    "topic_test_labels = np.array(topic_test['label']) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture\n",
    "\n",
    "Your network should pass each word index into its corresponding embedding by looking-up on the embedding matrix and then compute the first hidden layer $\\mathbf{h}_1$:\n",
    "\n",
    "$$\\mathbf{h}_1 = \\frac{1}{|x|}\\sum_i W^e_i, i \\in x$$\n",
    "\n",
    "where $|x|$ is the number of words in the document and $W^e$ is an embedding matrix $|V|\\times d$, $|V|$ is the size of the vocabulary and $d$ the embedding size.\n",
    "\n",
    "Then $\\mathbf{h}_1$ should be passed through a ReLU activation function:\n",
    "\n",
    "$$\\mathbf{a}_1 = relu(\\mathbf{h}_1)$$\n",
    "\n",
    "Finally the hidden layer is passed to the output layer:\n",
    "\n",
    "\n",
    "$$\\mathbf{y} = \\text{softmax}(\\mathbf{a}_1W^T) $$ \n",
    "where $W$ is a matrix $d \\times |{\\cal Y}|$, $|{\\cal Y}|$ is the number of classes.\n",
    "\n",
    "During training, $\\mathbf{a}_1$ should be multiplied with a dropout mask vector (elementwise) for regularisation before it is passed to the output layer.\n",
    "\n",
    "You can extend to a deeper architecture by passing a hidden layer to another one:\n",
    "\n",
    "$$\\mathbf{h_i} = \\mathbf{a}_{i-1}W_i^T $$\n",
    "\n",
    "$$\\mathbf{a_i} = relu(\\mathbf{h_i}) $$\n",
    "\n",
    "\n",
    "# Network Training\n",
    "\n",
    "First we need to define the parameters of our network by initiliasing the weight matrices. For that purpose, you should implement the `network_weights` function that takes as input:\n",
    "\n",
    "- `vocab_size`: the size of the vocabulary\n",
    "- `embedding_dim`: the size of the word embeddings\n",
    "- `hidden_dim`: a list of the sizes of any subsequent hidden layers (for the Bonus). Empty if there are no hidden layers between the average embedding and the output layer \n",
    "- `num_classes`: the number of the classes for the output layer\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `W`: a dictionary mapping from layer index (e.g. 0 for the embedding matrix) to the corresponding weight matrix initialised with small random numbers (hint: use numpy.random.uniform with from -0.1 to 0.1)\n",
    "\n",
    "See the examples below for expected outputs. Make sure that the dimensionality of each weight matrix is compatible with the previous and next weight matrix, otherwise you won't be able to perform forward and backward passes. Consider also using np.float32 precision to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:09:10.086665Z",
     "start_time": "2020-04-02T15:09:10.083429Z"
    }
   },
   "outputs": [],
   "source": [
    "def network_weights(vocab_size=1000, embedding_dim=300, hidden_dim=[], num_classes=3):\n",
    "    # fixing random seed for reproducibility\n",
    "    np.random.seed(123)\n",
    "\n",
    "    dimensions = [vocab_size, embedding_dim] + hidden_dim + [num_classes]\n",
    "\n",
    "    # Use \"He Weight Initialisation\"\n",
    "    W = [np.random.randn(*size).astype(np.float32) * np.sqrt(2 / (size[0]))\n",
    "         for size in zip(*(dimensions[i:] for i in range(2)))]\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T10:31:57.970152Z",
     "start_time": "2020-04-01T10:31:57.966123Z"
    }
   },
   "source": [
    "Then you need to develop a `softmax` function (same as in Assignment 1) to be used in the output layer. It takes as input:\n",
    "\n",
    "- `z`: array of real numbers \n",
    "\n",
    "and returns:\n",
    "\n",
    "- `smax`: the softmax of `z`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:50.504086Z",
     "start_time": "2020-04-02T14:26:50.500686Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Compute probability for each class\n",
    "    \"\"\"\n",
    "    e_z = np.exp(z)\n",
    "    return e_z / np.sum(e_z, axis=1 if e_z.ndim > 1 else None, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to implement the categorical cross entropy loss by slightly modifying the function from Assignment 1 to depend only on the true label `y` and the class probabilities vector `y_preds`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:51.360838Z",
     "start_time": "2020-04-02T14:26:51.356935Z"
    }
   },
   "outputs": [],
   "source": [
    "def categorical_loss(y, y_preds):    \n",
    "    return -np.log(y_preds[y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T15:02:56.149535Z",
     "start_time": "2020-03-31T15:02:56.145738Z"
    }
   },
   "source": [
    "Then, implement the `relu` function to introduce non-linearity after each hidden layer of your network (during the forward pass): \n",
    "\n",
    "$$relu(z_i)= max(z_i,0)$$\n",
    "\n",
    "and the `relu_derivative` function to compute its derivative (used in the backward pass):\n",
    "\n",
    "\\begin{equation}\n",
    "  \\text{relu_derivative}(z_i)=\\begin{cases}\n",
    "    0, & \\text{if $z_i<=0$}.\\\\\n",
    "    1, & \\text{otherwise}.\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Note that both functions take as input a vector $z$ \n",
    "\n",
    "Hint use .copy() to avoid in place changes in array z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:52.665236Z",
     "start_time": "2020-04-02T14:26:52.661519Z"
    }
   },
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.clip(z, a_min=0, a_max=None)\n",
    "\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return np.clip(z, a_min=0, a_max=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training you should also apply a dropout mask element-wise after the activation function (i.e. vector of ones with a random percentage set to zero). The `dropout_mask` function takes as input:\n",
    "\n",
    "- `size`: the size of the vector that we want to apply dropout\n",
    "- `dropout_rate`: the percentage of elements that will be randomly set to zeros\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `dropout_vec`: a vector with binary values (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:53.429192Z",
     "start_time": "2020-04-02T14:26:53.425301Z"
    }
   },
   "outputs": [],
   "source": [
    "def dropout_mask(size, dropout_rate):\n",
    "    return np.random.choice([0, 1], size, p=[dropout_rate, 1 - dropout_rate])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to implement the `forward_pass` function that passes the input x through the network up to the output layer for computing the probability for each class using the weight matrices in `W`. The ReLU activation function should be applied on each hidden layer. \n",
    "\n",
    "- `x`: a list of vocabulary indices each corresponding to a word in the document (input)\n",
    "- `W`: a list of weight matrices connecting each part of the network, e.g. for a network with a hidden and an output layer: W[0] is the weight matrix that connects the input to the first hidden layer, W[1] is the weight matrix that connects the hidden layer to the output layer.\n",
    "- `dropout_rate`: the dropout rate that is used to generate a random dropout mask vector applied after each hidden layer for regularisation.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `out_vals`: a dictionary of output values from each layer: h (the vector before the activation function), a (the resulting vector after passing h to the activation function), its dropout mask vector; and the prediction vector (probability for each class) from the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:54.761268Z",
     "start_time": "2020-04-02T14:26:54.753402Z"
    }
   },
   "outputs": [],
   "source": [
    "def forward_pass(x, W, dropout_rate=0.2):\n",
    "\n",
    "    h_vecs = []\n",
    "    a_vecs = []\n",
    "    dropout_vecs = []\n",
    "\n",
    "    # Embedding layer\n",
    "    h_1 = np.mean(W[0][x], axis=0)\n",
    "    h_vecs.append(h_1)\n",
    "\n",
    "    a_1 = relu(h_1)\n",
    "    a_vecs.append(a_1)\n",
    "\n",
    "    # Apply dropout mask to embedding layer\n",
    "    dropout_vecs.append(dropout_mask(W[0].shape[1], dropout_rate))\n",
    "    a_i = a_1 * dropout_vecs[-1]\n",
    "\n",
    "    # Loop through each hidden layer\n",
    "    for weights in W[1:-1]:\n",
    "        h_i = a_i.dot(weights)\n",
    "        h_vecs.append(h_i)\n",
    "\n",
    "        a_i = relu(h_i)\n",
    "        a_vecs.append(a_i)\n",
    "\n",
    "        dropout_vecs.append(dropout_mask(weights.shape[1], dropout_rate))\n",
    "        a_i *= dropout_vecs[-1]\n",
    "\n",
    "    # Output layer: make prediction\n",
    "    y = softmax(a_i.dot(W[-1]))\n",
    "\n",
    "    return {'h': h_vecs, 'a': a_vecs, 'dropout_vec': dropout_vecs, 'y': y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `backward_pass` function computes the gradients and update the weights for each matrix in the network from the output to the input. It takes as input \n",
    "\n",
    "- `x`: a list of vocabulary indices each corresponding to a word in the document (input)\n",
    "- `y`: the true label\n",
    "- `W`: a list of weight matrices connecting each part of the network, e.g. for a network with a hidden and an output layer: W[0] is the weight matrix that connects the input to the first hidden layer, W[1] is the weight matrix that connects the hidden layer to the output layer.\n",
    "- `out_vals`: a dictionary of output values from a forward pass.\n",
    "- `learning_rate`: the learning rate for updating the weights.\n",
    "- `freeze_emb`: boolean value indicating whether the embedding weights will be updated.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `W`: the updated weights of the network.\n",
    "\n",
    "Hint: the gradients on the output layer are similar to the multiclass logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:26:56.225630Z",
     "start_time": "2020-04-02T14:26:56.216508Z"
    }
   },
   "outputs": [],
   "source": [
    "def backward_pass(x, y, W, out_vals, lr=0.001, freeze_emb=False):\n",
    "    # Compute the gradient on the output layer\n",
    "    g = out_vals['y'] - (np.arange(len(out_vals['y'])) == y)\n",
    "\n",
    "    # Compute gradients on weights\n",
    "    out_layer_input = out_vals['a'][-1] * out_vals['dropout_vec'][-1]\n",
    "    g_on_w = np.outer(g, out_layer_input).T\n",
    "\n",
    "    # Propagate the gradients w.r.t. the last hidden layer\n",
    "    g = g.dot(W[-1].T)\n",
    "    \n",
    "    # Update output layer weight\n",
    "    W[-1] -= lr * g_on_w\n",
    "\n",
    "    # Update each hidden layer\n",
    "    for i in range(len(W) - 2, 1, -1):\n",
    "        # Convert the gradient on the layer's output\n",
    "        # into a gradient before the activation function\n",
    "        g *= relu_derivative(out_vals['h'][i])\n",
    "\n",
    "        # Compute gradients on weights\n",
    "        layer_input = out_vals['a'][i - 1] * out_vals['dropout_vec'][i - 1]\n",
    "        g_on_w = np.outer(g, layer_input).T\n",
    "\n",
    "        # Propagate the gradients w.r.t. the next hidden layer's activations\n",
    "        g = g.dot(W[i].T)\n",
    "\n",
    "        # Update weights\n",
    "        W[i] -= lr * g_on_w\n",
    "\n",
    "    # Update embedding layer weight\n",
    "    if not freeze_emb:\n",
    "        g *= relu_derivative(out_vals['h'][0])\n",
    "        W[0][x] -= lr * g\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:08:59.937442Z",
     "start_time": "2020-02-15T14:08:59.932221Z"
    }
   },
   "source": [
    "Finally you need to modify SGD to support back-propagation by using the `forward_pass` and `backward_pass` functions.\n",
    "\n",
    "The `SGD` function takes as input:\n",
    "\n",
    "- `X_tr`: array of training data (vectors)\n",
    "- `Y_tr`: labels of `X_tr`\n",
    "- `W`: the weights of the network (dictionary)\n",
    "- `X_dev`: array of development (i.e. validation) data (vectors)\n",
    "- `Y_dev`: labels of `X_dev`\n",
    "- `lr`: learning rate\n",
    "- `dropout`: regularisation strength\n",
    "- `epochs`: number of full passes over the training data\n",
    "- `tolerance`: stop training if the difference between the current and previous validation loss is smaller than a threshold\n",
    "- `freeze_emb`: boolean value indicating whether the embedding weights will be updated (to be used by the backward pass function).\n",
    "- `print_progress`: flag for printing the training progress (train/validation loss)\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `weights`: the weights learned\n",
    "- `training_loss_history`: an array with the average losses of the whole training set after each epoch\n",
    "- `validation_loss_history`: an array with the average losses of the whole development set after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:09:19.021428Z",
     "start_time": "2020-04-02T15:09:19.017835Z"
    }
   },
   "outputs": [],
   "source": [
    "def SGD(X_tr, Y_tr, W, X_dev, Y_dev, lr=0.001, dropout=0.2, epochs=5,\n",
    "        tolerance=0.001, freeze_emb=False, early_stopping=True, print_progress=True):\n",
    "    # fixing random seed for reproducibility\n",
    "    np.random.seed(123)\n",
    "    training_loss_history = []\n",
    "    validation_loss_history = []\n",
    "\n",
    "    # Create training tuples\n",
    "    train_docs = list(zip(X_tr, Y_tr))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Randomise order in train_docs\n",
    "        np.random.shuffle(train_docs)\n",
    "\n",
    "        for x_i, y_i in train_docs:\n",
    "            W = backward_pass(x_i, y_i, W, forward_pass(x_i, W, dropout), lr, freeze_emb)\n",
    "\n",
    "        # Monitor training loss\n",
    "        cur_loss_tr = np.mean([categorical_loss(y_i, forward_pass(x_i, W, dropout)['y'])\n",
    "                               for x_i, y_i in train_docs])\n",
    "\n",
    "        # Monitor validation loss\n",
    "        cur_loss_dev = np.mean([categorical_loss(y_i, forward_pass(x_i, W, dropout)['y'])\n",
    "                                for x_i, y_i in zip(X_dev, Y_dev)])\n",
    "\n",
    "        # Early stopping\n",
    "        if early_stopping and epoch > 0 and validation_loss_history[-1] - cur_loss_dev < tolerance:\n",
    "            break\n",
    "        else:\n",
    "            training_loss_history.append(cur_loss_tr)\n",
    "            validation_loss_history.append(cur_loss_dev)\n",
    "\n",
    "        if print_progress:\n",
    "            print(f'Epoch: {epoch} | Training loss: {cur_loss_tr} | Validation loss: {cur_loss_dev}')\n",
    "\n",
    "    return W, training_loss_history, validation_loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:10:15.772383Z",
     "start_time": "2020-02-15T14:10:15.767855Z"
    }
   },
   "source": [
    "Now you are ready to train and evaluate you neural net. First, you need to define your network using the `network_weights` function followed by SGD with backprop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:09:33.643515Z",
     "start_time": "2020-04-02T15:09:33.640943Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape W0 (7610, 300)\n",
      "Shape W1 (300, 3)\n",
      "Epoch: 0 | Training loss: 1.0941477671067623 | Validation loss: 1.0965148843157566\n",
      "Epoch: 1 | Training loss: 1.0683677614308555 | Validation loss: 1.0847936884412994\n",
      "Epoch: 2 | Training loss: 0.6963450312406894 | Validation loss: 0.7978179175058632\n",
      "Epoch: 3 | Training loss: 0.5057582228183621 | Validation loss: 0.5977412120295634\n",
      "Epoch: 4 | Training loss: 0.350007241604076 | Validation loss: 0.4881203616660654\n",
      "Epoch: 5 | Training loss: 0.2997647915233003 | Validation loss: 0.4866841181602369\n"
     ]
    }
   ],
   "source": [
    "W = network_weights(vocab_size=len(vocab),\n",
    "                    embedding_dim=300,\n",
    "                    num_classes=3)\n",
    "\n",
    "for i in range(len(W)):\n",
    "    print(f'Shape W{i} {W[i].shape}')\n",
    "\n",
    "W, tr_loss, dev_loss = SGD(X_tr=topic_train_ids,\n",
    "                           Y_tr=topic_train_labels,\n",
    "                           W=W,\n",
    "                           X_dev=topic_dev_ids,\n",
    "                           Y_dev=topic_dev_labels,\n",
    "                           lr=0.15,\n",
    "                           dropout=0.2,\n",
    "                           epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the learning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:27:15.716497Z",
     "start_time": "2020-04-02T14:27:15.612736Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(tr_loss, label='Training loss')\n",
    "plt.plot(dev_loss, label='Validation loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Monitoring (Average embedding)')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy, precision, recall and F1-Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:10:11.037495Z",
     "start_time": "2020-04-02T15:10:11.034999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8577777777777778\n",
      "Precision: 0.8575432340880728\n",
      "Recall: 0.8577777777777778\n",
      "F1-Score: 0.8572448556010199\n"
     ]
    }
   ],
   "source": [
    "preds_te = [np.argmax(forward_pass(x, W, dropout_rate=0.0)['y'])\n",
    "            for x, y in zip(topic_test_ids, topic_test_labels)]\n",
    "args = topic_test_labels, preds_te\n",
    "\n",
    "print('Accuracy:', accuracy_score(*args))\n",
    "print('Precision:', precision_score(*args, average='macro'))\n",
    "print('Recall:', recall_score(*args, average='macro'))\n",
    "print('F1-Score:', f1_score(*args, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discuss how did you choose model hyperparameters ?\n",
    "\n",
    "### Preliminary Analysis\n",
    "\n",
    "#### Fixed Set Ordering of Vocabulary\n",
    "\n",
    "The vocabulary is stored as a set object to prevent duplicates. However, Python Sets are unordered collection that do not record element position or order of insertion ([Python Set Types](https://docs.python.org/3.8/library/stdtypes.html#set-types-set-frozenset)). This imples that the order of each vocabulary in the set is not reproducible when the Python kernel is restarted.\n",
    "\n",
    "After several experiments, it is concluded that different orders of the vocabulary set produce different results. The highest and lowest F1-Score obtained are ~83% and ~81% respectively. To ensure that each set of hyperparameters combination produce the same result (i.e. not affected by ordering of vocabulary), it is decided to sort the vocabulary alphabetically, as shown in the code below.\n",
    "\n",
    "```python\n",
    "# Set order is random when kernel is restarted. Use `sorted` for reproducibility\n",
    "vocab = sorted(dev_vocab.union(train_vocab).union(test_vocab))\n",
    "```\n",
    "\n",
    "#### Weight Initialisation Method: He Weight Initialisation\n",
    "\n",
    "When using ReLU as the activation function and initialising the weights using a uniform distribution method between the same lower and upper boundary (e.g. `np.random.uniform(-0.5, 0.5)`), then by uniform distribution half of the layer will output zero when passing the ReLU activation function ([Glorot et al., 2011](http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf)).\n",
    "\n",
    "[He et al. (2015)](https://arxiv.org/abs/1502.01852) suggested a weight initialisation method that is suitable for the ReLU activation function, as shown below ([Ref 1](https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94), [Ref 2](https://towardsdatascience.com/weight-initialization-techniques-in-neural-networks-26c649eb3b78)):\n",
    "\n",
    "```python\n",
    "np.random.randn(layer_dim[i], layer_dim[i + 1]) * np.sqrt(2 / (layer_dim[i]))\n",
    "```\n",
    "\n",
    "Advantages of using He Weight Initialisation:\n",
    "1. The method is mathematically proven to be effective with ReLU activation function in a published paper (~7810 Google Scholar citations)\n",
    "\n",
    "2. The weights of each layer is not fixed between the same lower and upper boundary. Now the weights of each layer are multipled by `np.sqrt(2 / (layer_dim[i]))`.\n",
    "\n",
    "3. The process of hyperparameter optimisation is simpler as the initial boundary of weights (`init_val` in `network_weights`) is excluded.\n",
    "    1. Manually searching for the optimal initial weights is challenging. The optimal value could depends on the learning rate (`lr`), the embedding dimension (`embedding_dim`). With these dependency relationship between different hyperparameters, having fewer hyperparameters to optimise would greatly reduce the difficulty to find the optimal hyperparameter combination.\n",
    "\n",
    "\n",
    "#### Relationship Between Hyperparameters and Scores\n",
    "\n",
    "Before optimising the hyperparameters, it is required to know the relationship between each hyperparameter and the resulting scores.\n",
    "\n",
    "\n",
    "##### Embedding dimension\n",
    "\n",
    "The large the embedding dimension, the longer the training time.\n",
    "\n",
    "- If the embedding dimension is too large, then there might be an excessive amount of information that is not useful. This would have an adverse effect on performance if the model is overfitting.\n",
    "\n",
    "- If the embedding dimension is too small, then there might not be enough information to represent the word relation.\n",
    "\n",
    "According to [Yin and Shen (2018)](http://papers.nips.cc/paper/7368-on-the-dimensionality-of-word-embedd), the performance of a word embedding is largely affected by the chosen dimensionality. However, unlike learning rate, there does not seems to be a general rule for choosing embedding dimension.\n",
    "\n",
    "##### Learning Rate\n",
    "\n",
    "Choosing a good learning rate is challenging as every model differs from each other. Generally, it is required to perform some preliminary analysis on the model performance before the hyperparameters optimisation.\n",
    "\n",
    "Learning rate controls how much to change the model in response to the estimated error each time the model weights are updated.\n",
    "\n",
    "- If the learning rate is too large, the model may overshoot and lead to divergent behaviour (epochs required is low)\n",
    "\n",
    "- If the learning rate is too small, the model will require many updates to the weights  before the loss is converged (epochs required is high)\n",
    "    - However, if the tolerance is higher, the training is likely to stop in the first epoch due to early stopping\n",
    "\n",
    "\n",
    "##### Dropout Rate (Regularisation)\n",
    "\n",
    "Dropout rate is used to prevent the model from overfitting. If the dropout rate is too high, most of the layer outputs will be ignored, and thus the model would underfit.\n",
    "\n",
    "\n",
    "### Optimising Hyperparameters\n",
    "\n",
    "#### Limitations of Trial and Error Method\n",
    "\n",
    "Although the trial and error method is a straightforward strategy and requires no extra implementation, it is prone to the local optimum problem. It is impractical to try every possible combination of learning rate and regularisation strength to find the optimal result. Therefore, the best achievable performance improvement through fine-tuning is largely dependent on the initial set of values selected to explore the hyperparameters.\n",
    "\n",
    "Furthermore, another optimisation problem has arisen as the number of hyperparameters to search is more than one. Some hyperparameters could also have a dependency relationship with each other. This has lead to another assumption that the optimisation order of hyperparameters may have a certain impact on the final result.\n",
    "\n",
    "\n",
    "#### Finding the Baseline Performance\n",
    "\n",
    "According to the submission instructions, the model should achieve a F1-score of at least 75%. However, the default hyperparameters provided is only getting a F1-score of 37.2%. Therefore, the first step is to find the hyperparameter combination that is able to achieve the baseline performance.\n",
    "\n",
    "| Trial | embedding_dim |   lr  | dropout | tolerance | Epochs | Tr. loss | Val. loss | F1-score |\n",
    "|:-----:|:-------------:|:-----:|:-------:|:---------:|:------:|:--------:|:---------:|:--------:|\n",
    "|   0   |      300      | 0.001 |   0.2   |    0.01   |    0   |  1.0984  |   1.0986  |  0.3721  |\n",
    "|   1   |      300      |  0.01 |   0.2   |    0.01   |    0   |  1.0982  |   1.0985  |  0.4255  |\n",
    "|   2   |      300      |  0.1  |   0.2   |    0.01   |    0   |  1.0959  |   1.0973  |  0.6668  |\n",
    "|   3   |      300      |  0.2  |   0.2   |    0.01   |    4   |  0.2310  |   0.4400  |  0.8164  |\n",
    "\n",
    "As shown in the table above, the initial learning rate of 0.001 was too low. After increasing the learning rate to 0.2, the F1-score has improved to 79.5%. As a result, F1-score of 81.6% is set as the baseline peformance. Any subsequent optimisations should therefore be aiming to achieve performance higher than 81.6%.\n",
    "\n",
    "#### Optimising Learning Rate and Tolerance\n",
    "\n",
    "In **Finding the Baseline Performance**, the learning rate is increased in big step size, which resulted in small epochs. It would be better for the model to converge with slower speed to prevent the gradient from overshooting.\n",
    "\n",
    "Fixed parameters: `embedding_dim=300, dropout=0.2`\n",
    "\n",
    "| Trial |  lr  | tolerance | Epochs | Tr. loss | Val. loss | F1-score |\n",
    "|:-----:|:----:|:---------:|:------:|:--------:|:---------:|:--------:|\n",
    "|   0   |  0.2 |    0.01   |    4   |  0.2310  |   0.4400  |  0.8164  |\n",
    "|   1   |  0.2 |   0.001   |    4   |  0.2310  |   0.4400  |  0.8164  |\n",
    "|   2   | 0.15 |    0.01   |    4   |  0.3500  |   0.4881  |  0.8164  |\n",
    "|   3   | 0.15 |   0.001   |    5   |  0.2997  |   0.4866  |  0.8572  |\n",
    "|   4   |  0.1 |    0.01   |    0   |  1.0959  |   1.0973  |  0.6668  |\n",
    "|   5   |  0.1 |   0.001   |    7   |  0.3303  |   0.4705  |  0.8486  |\n",
    "|   6   | 0.05 |    0.01   |    0   |  1.0973  |   1.0980  |  0.5775  |\n",
    "|   7   | 0.05 |   0.001   |    0   |  1.0973  |   1.0980  |  0.5775  |\n",
    "\n",
    "With `lr=0.15, tolerance=0.001`, the model is now able to achieve a F1-score of 85.7%. \n",
    "\n",
    "It is observed that for smaller learning rate (< 0.2), both training and validation will decrease very slowly in the first 3 epochs. Therefore, higher tolerance such as 0.01 will stop the training very early, which is why a significant performance improvement is achieved when the tolerance is lowered to 0.001.\n",
    "\n",
    "#### Optimising Learning Rate\n",
    "\n",
    "With the tolerance fixed, the learning rate can now be fine-tuned in a small step size to search for possible improvements.\n",
    "\n",
    "Fixed parameters: `embedding_dim=300, dropout=0.2, tolerance=0.001`\n",
    "\n",
    "| Trial |   lr   | Epochs | Tr. loss | Val. loss | F1-score |\n",
    "|:-----:|:------:|:------:|:--------:|:---------:|:--------:|\n",
    "|   0   |  0.15  |    5   |  0.2997  |   0.4866  |  0.8572  |\n",
    "|   1   |  0.16  |    4   |  0.3134  |   0.4680  |  0.8181  |\n",
    "|   2   |  0.14  |    5   |  0.3293  |   0.5035  |  0.8550  |\n",
    "|   3   |  0.13  |    7   |  0.2500  |   0.4384  |  0.8470  |\n",
    "|   4   |  0.145 |    5   |  0.3137  |   0.4939  |  0.8561  |\n",
    "|   5   | 0.1475 |    5   |  0.3067  |   0.4901  |  0.8572  |\n",
    "|   6   |  0.151 |    4   |  0.3465  |   0.4859  |  0.8197  |\n",
    "|   7   | 0.1505 |    5   |  0.2983  |   0.4860  |  0.8560  |\n",
    "\n",
    "No better `lr` has been found. 0.15 seems to be the maximum limit of the learning rate to achieve the best F1-score with lowest training and validation loss.\n",
    "\n",
    "#### Optimising Dropout Rate\n",
    "\n",
    "Compared to other hyperparameters, the dropout rate is relatively easier to optimise as it can only be a value between 0 and 1.\n",
    "\n",
    "Fixed parameters: `embedding_dim=300, lr=0.15, tolerance=0.001`\n",
    "\n",
    "| Trial | dropout | Epochs | Tr. loss | Val. loss | F1-score |\n",
    "|:-----:|:-------:|:------:|:--------:|:---------:|:--------:|\n",
    "|   0   |   0.2   |    5   |  0.2997  |   0.4866  |  0.8572  |\n",
    "|   1   |   0.3   |    7   |  0.2490  |   0.3969  |  0.8053  |\n",
    "|   2   |   0.4   |    5   |  0.4433  |   0.4704  |  0.8545  |\n",
    "|   3   |   0.5   |    6   |  0.3479  |   0.4878  |  0.7705  |\n",
    "|   4   |   0.1   |    4   |  0.2929  |   0.4610  |  0.8055  |\n",
    "|   5   |   0.19  |    5   |  0.2609  |   0.4490  |  0.8539  |\n",
    "|   6   |   0.21  |    5   |  0.2963  |   0.4472  |  0.8537  |\n",
    "\n",
    "No better dropout rate has been found. The initial dropout rate of 0.2 is considered to be the optimal value.\n",
    "\n",
    "\n",
    "#### Optimising Embedding Dimension\n",
    "\n",
    "Fixed parameters: `lr=0.15, tolerance=0.001, dropout=0.2`\n",
    "\n",
    "| Trial | embedding_dim | Epochs | Tr. loss | Val. loss | F1-score |\n",
    "|:-----:|:-------------:|:------:|:--------:|:---------:|:--------:|\n",
    "|   0   |      300      |    5   |  0.2997  |   0.4866  |  0.8572  |\n",
    "|   1   |      400      |    5   |  0.2723  |   0.3679  |  0.8513  |\n",
    "|   2   |      500      |    7   |  0.1673  |   0.3869  |  0.8095  |\n",
    "|   3   |      1000     |    8   |  0.1523  |   0.3579  |  0.8482  |\n",
    "|   4   |      200      |    2   |  0.6866  |   0.7152  |  0.6770  |\n",
    "|   5   |      100      |    4   |  0.2890  |   0.4059  |  0.8551  |\n",
    "|   6   |       50      |    4   |  0.2989  |   0.4216  |  0.7969  |\n",
    "\n",
    "No better embedding dimension has been found. The initial dimension of 300 is considered to be the optimal value.\n",
    "\n",
    "According to the table above, there is no clear indiciation whether increasing or decreasing the embedding dimension would have positive or negative effect on the performance.\n",
    "\n",
    "When the embedding dimension is decreased to 200, the F1-score has to 67.7%. However, the F1-score has increased back to 85.5% after further decreasing the dimension down to 100. Conversely, the performance did not drop below 80% when the dimension is increased up to 1000.\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "Factors that affect the model performance, excluding hyperparameters:\n",
    "1. The order of the vocabulary\n",
    "\n",
    "2. The weight initialisation method\n",
    "\n",
    "The optimal value found for hyperparameters:\n",
    "1. Embedding dimension: 300\n",
    "2. Learning rate: 0.15\n",
    "3. Dropout rate: 0.2\n",
    "4. Tolerance: 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Pre-trained Embeddings\n",
    "\n",
    "Now re-train the network using GloVe pre-trained embeddings. You need to modify the `backward_pass` function above to stop computing gradients and updating weights of the embedding matrix.\n",
    "\n",
    "Use the function below to obtain the embedding martix for your vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:27:32.020697Z",
     "start_time": "2020-04-02T14:27:32.015733Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_glove_embeddings(f_zip, f_txt, word2id, emb_size=300):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        f_zip: The compressed zip file\n",
    "        f_txt: The text file containing the embeddings\n",
    "        word2id: The word to vocabulary ID dictionary\n",
    "        emb_size: The size of the word embeddings\n",
    "        \n",
    "    Returns:\n",
    "        Weights for the embedding layer\n",
    "    \"\"\"\n",
    "    w_emb = np.zeros((len(word2id), emb_size))\n",
    "\n",
    "    with zipfile.ZipFile(f_zip) as z:\n",
    "        with z.open(f_txt) as f:\n",
    "            for line in f:\n",
    "                line = line.decode('utf-8')\n",
    "                word = line.split()[0]\n",
    "\n",
    "                if word in vocab:\n",
    "                    emb = np.array(line.strip('\\n').split()[1:]).astype(np.float32)\n",
    "                    w_emb[word2id[word]] += emb\n",
    "    return w_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:28:54.548613Z",
     "start_time": "2020-04-02T14:27:32.780248Z"
    }
   },
   "outputs": [],
   "source": [
    "w_glove = get_glove_embeddings(\"glove.840B.300d.zip\", \"glove.840B.300d.txt\",\n",
    "                               word_to_vocab_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, initialise the weights of your network using the `network_weights` function. Second, replace the weigths of the embedding matrix with `w_glove`. Finally, train the network by freezing the embedding weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:30:11.121198Z",
     "start_time": "2020-04-02T14:29:24.946124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape W0 (7610, 300)\n",
      "Shape W1 (300, 3)\n",
      "Epoch: 0 | Training loss: 0.39196310346400937 | Validation loss: 0.31723636739356187\n",
      "Epoch: 1 | Training loss: 0.37113303463091457 | Validation loss: 0.2614393910627511\n",
      "Epoch: 2 | Training loss: 0.32417943335986604 | Validation loss: 0.225559466206273\n",
      "Epoch: 3 | Training loss: 0.3072251838800586 | Validation loss: 0.22377844162323404\n",
      "Epoch: 4 | Training loss: 0.30196525400380764 | Validation loss: 0.21991298689775662\n",
      "Epoch: 5 | Training loss: 0.2921520713493237 | Validation loss: 0.2116361607768931\n"
     ]
    }
   ],
   "source": [
    "W = network_weights(vocab_size=len(vocab),\n",
    "                    embedding_dim=300,\n",
    "                    num_classes=3)\n",
    "W[0] = w_glove\n",
    "\n",
    "\n",
    "for i in range(len(W)):\n",
    "    print(f'Shape W{i} {W[i].shape}')\n",
    "\n",
    "W, tr_loss, dev_loss = SGD(X_tr=topic_train_ids,\n",
    "                           Y_tr=topic_train_labels,\n",
    "                           W=W,\n",
    "                           X_dev=topic_dev_ids,\n",
    "                           Y_dev=topic_dev_labels,\n",
    "                           lr=0.1,\n",
    "                           dropout=0.2,\n",
    "                           freeze_emb=True,\n",
    "                           epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(tr_loss, label='Training loss')\n",
    "plt.plot(dev_loss, label='Validation loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Monitoring (Pre-trained embedding)')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8977777777777778\n",
      "Precision: 0.897868613832692\n",
      "Recall: 0.8977777777777778\n",
      "F1-Score: 0.8967871274658675\n"
     ]
    }
   ],
   "source": [
    "preds_te = [np.argmax(forward_pass(x, W, dropout_rate=0.0)['y'])\n",
    "            for x, y in zip(topic_test_ids, topic_test_labels)]\n",
    "args = topic_test_labels, preds_te\n",
    "\n",
    "print('Accuracy:', accuracy_score(*args))\n",
    "print('Precision:', precision_score(*args, average='macro'))\n",
    "print('Recall:', recall_score(*args, average='macro'))\n",
    "print('F1-Score:', f1_score(*args, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discuss how did you choose model hyperparameters ?\n",
    "\n",
    "### Initial Results\n",
    "\n",
    "Using the optimal hyperparameters obtained in the previous model (average embedding):\n",
    "\n",
    "Parameters: `embedding_dim=300, lr=0.15, dropout=0.2, tolerance=0.001`\n",
    "\n",
    "| Epochs | Training loss | Validation loss | Accuracy | Precision | Recall | F1-score |\n",
    "|:------:|:-------------:|:---------------:|:--------:|:---------:|:------:|:--------:|\n",
    "|    5   |     0.2798    |      0.2010     |  0.8966  |   0.8965  | 0.8966 |  0.8957  |\n",
    "\n",
    "\n",
    "### Optimising Hyperparameters\n",
    "\n",
    "Since the dimension has to be 300 with the GloVe embeddings, there are only two hyperparameters need to optimise: learning rate and dropout rate.\n",
    "\n",
    "#### Optimising Learning Rate\n",
    "\n",
    "Fixed parameters: `dropout=0.2, tolerance=0.001`\n",
    "\n",
    "| Trial |  lr  | Epochs | Tr. loss | Val. loss | F1-score |\n",
    "|:-----:|:----:|:------:|:--------:|:---------:|:--------:|\n",
    "|   0   | 0.15 |    5   |  0.2798  |   0.2010  |  0.8957  |\n",
    "|   1   |  0.2 |    5   |  0.2754  |   0.1960  |  0.8924  |\n",
    "|   2   |  0.3 |    5   |  0.2807  |   0.1962  |  0.8944  |\n",
    "|   3   |  0.4 |    3   |  0.2777  |   0.1968  |  0.8800  |\n",
    "|   4   |  0.1 |    5   |  0.2921  |   0.2116  |  0.8967  |\n",
    "|   5   | 0.05 |    3   |  0.3343  |   0.2523  |  0.8776  |\n",
    "|   6   | 0.01 |   11   |  0.3594  |   0.2760  |  0.8837  |\n",
    "\n",
    "`lr=0.1` has the highest F1-score of 89.67%, therefore it is selected as the optimal learning rate. Although it has about 1% higher training and validation loss than `lr=0.15`, but the aim of hyperparameters optimisation is to achieve the highest possible scores.\n",
    "\n",
    "According to the results, changing the learning rate only produce a ± 2% performance differences. It is also observed that the validation loss is always lower than the training loss in this model.\n",
    "\n",
    "\n",
    "#### Optimising Dropout Rate\n",
    "\n",
    "Fixed parameters: `lr=0.1, tolerance=0.001`\n",
    "\n",
    "| Trial | dropout | Epochs | Tr. loss | Val. loss | F1-score |\n",
    "|:-----:|:-------:|:------:|:--------:|:---------:|:--------:|\n",
    "|   0   |   0.2   |    5   |  0.2921  |   0.2116  |  0.8967  |\n",
    "|   1   |   0.1   |    2   |  0.3206  |   0.2033  |  0.8861  |\n",
    "|   2   |   0.3   |    3   |  0.3317  |   0.2379  |  0.8868  |\n",
    "|   3   |   0.5   |    3   |  0.3923  |   0.3119  |  0.8757  |\n",
    "|   4   |   0.7   |    7   |  0.4672  |   0.3993  |  0.8879  |\n",
    "|   5   |   0.9   |   11   |  0.7462  |   0.7007  |  0.8809  |\n",
    "\n",
    "No better dropout rate has been found. `dropout=0.2` has the best F1-score with lowest training and validation loss.\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "According to the results in the tables above, the performance of **Pre-Trained Embeddings Model** is not affected significantly by the value of learning rate and dropout rate. In comparison with **Average Embeddings Model**, it is safe to conclude that the performance this feedforward network is largely dependent on the initial weights.\n",
    "\n",
    "The optimal value found for hyperparameters:\n",
    "1. Learning rate: 0.1\n",
    "2. Dropout rate: 0.2\n",
    "\n",
    "Using pre-trained embeddings improved the total performance by ~4% in comparison to randomly initialising the embedding weights matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend to support deeper architectures (Bonus)\n",
    "\n",
    "Extend the network to support back-propagation for more hidden layers. You need to modify the `backward_pass` function above to compute gradients and update the weights between intermediate hidden layers. Finally, train and evaluate a network with a deeper architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T14:58:51.764619Z",
     "start_time": "2020-04-02T14:58:47.483690Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape W0 (7610, 300)\n",
      "Shape W1 (300, 1050)\n",
      "Shape W2 (1050, 3)\n",
      "Epoch: 0 | Training loss: 0.42479884818659164 | Validation loss: 0.367079861662041\n",
      "Epoch: 1 | Training loss: 0.41608202469722905 | Validation loss: 0.34109356664542934\n",
      "Epoch: 2 | Training loss: 0.4626220294584154 | Validation loss: 0.446505678173534\n",
      "Epoch: 3 | Training loss: 0.3572658445781216 | Validation loss: 0.27464592135579863\n",
      "Epoch: 4 | Training loss: 0.35519588083525444 | Validation loss: 0.2802350066339664\n"
     ]
    }
   ],
   "source": [
    "W = network_weights(vocab_size=len(vocab),\n",
    "                    embedding_dim=300,\n",
    "                    hidden_dim=[1050],\n",
    "                    num_classes=3)\n",
    "W[0] = w_glove\n",
    "\n",
    "for i in range(len(W)):\n",
    "    print(f'Shape W{i} {W[i].shape}')\n",
    "\n",
    "W, tr_loss, dev_loss = SGD(X_tr=topic_train_ids,\n",
    "                           Y_tr=topic_train_labels,\n",
    "                           W=W,\n",
    "                           X_dev=topic_dev_ids,\n",
    "                           Y_dev=topic_dev_labels,\n",
    "                           lr=0.07,\n",
    "                           dropout=0.2,\n",
    "                           freeze_emb=True,\n",
    "                           early_stopping=False,\n",
    "                           epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(tr_loss, label='Training loss')\n",
    "plt.plot(dev_loss, label='Validation loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Monitoring (Pre-trained + Hidden layers)')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T15:11:51.994986Z",
     "start_time": "2020-04-02T15:11:51.992563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9033333333333333\n",
      "Precision: 0.9031030443799583\n",
      "Recall: 0.9033333333333333\n",
      "F1-Score: 0.9020881592476028\n"
     ]
    }
   ],
   "source": [
    "preds_te = [np.argmax(forward_pass(x, W, dropout_rate=0.0)['y'])\n",
    "            for x, y in zip(topic_test_ids, topic_test_labels)]\n",
    "args = topic_test_labels, preds_te\n",
    "\n",
    "print('Accuracy:', accuracy_score(*args))\n",
    "print('Precision:', precision_score(*args, average='macro'))\n",
    "print('Recall:', recall_score(*args, average='macro'))\n",
    "print('F1-Score:', f1_score(*args, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discuss how did you choose model hyperparameters ?\n",
    "\n",
    "### Searching the First Hidden Layer Dimension\n",
    "\n",
    "The first step is to find the hyperparameter combination that is able to achieve the closest performance to **Pre-trained Embeddings Model**.\n",
    "\n",
    "| Trial | hidden_dim |  lr  | dropout | Epochs | Tr. loss | Val. loss | F1-score |\n",
    "|:-----:|:----------:|:----:|:-------:|:------:|:--------:|:---------:|:--------:|\n",
    "|   0   |     100    |  0.1 |   0.2   |    1   |   0.618  |   0.5411  |  0.8293  |\n",
    "|   1   |     200    |  0.1 |   0.2   |    1   |  0.5676  |   0.5865  |  0.8777  |\n",
    "|   2   |     300    |  0.1 |   0.2   |    2   |  0.4808  |   0.4006  |  0.8517  |\n",
    "|   3   |     200    | 0.05 |   0.2   |    4   |  0.5609  |   0.5045  |  0.8727  |\n",
    "|   4   |     200    | 0.05 |   0.1   |    5   |  0.4562  |   0.4156  |  0.8697  |\n",
    "|   5   |    1000    | 0.05 |   0.1   |    3   |  0.3257  |   0.2171  |  0.8913  |\n",
    "|   6   |    1000    | 0.05 |   0.2   |    3   |  0.3623  |   0.2515  |  0.8953  |\n",
    "\n",
    "The hyperparameter combination `hidden_dim=[1000], lr=0.05, dropout=0.2` has achieve a F1-score of 89.53%, which is only 0.14% lower than **Pre-trained Embeddings Model**.\n",
    "\n",
    "### Relationship between Hyperparameters and Loss\n",
    "\n",
    "According to the table in **Searching the First Hidden Layer Dimension**,\n",
    "\n",
    "1. Increasing hidden layer dimension will decrease training loss\n",
    "\n",
    "2. Increasing dropout rate will increase training and validation loss\n",
    "\n",
    "The aim is to adjust each hyperparameter appropriately to reach about 30% final training loss and 20% final validation loss. The model appear to be producing the best performance when both losses fall into the respective range.\n",
    "\n",
    "### Further Trial and Error\n",
    "\n",
    "| Trial | hidden_dim |   lr  | dropout | Epochs | Tr. loss | Val. loss | F1-score |\n",
    "|:-----:|:----------:|:-----:|:-------:|:------:|:--------:|:---------:|:--------:|\n",
    "|   0   |     500    |  0.05 |   0.2   |    3   |  0.4220  |   0.3357  |  0.8683  |\n",
    "|   1   |     500    |  0.1  |   0.2   |    3   |  0.4073  |   0.3223  |  0.8702  |\n",
    "|   2   |     500    |  0.2  |   0.2   |    3   |  0.4212  |   0.3500  |  0.8689  |\n",
    "|   3   |     600    |  0.2  |   0.2   |    0   |  0.5173  |   0.3960  |  0.8327  |\n",
    "|   4   |     600    |  0.1  |   0.3   |    2   |  0.4654  |   0.3992  |  0.8704  |\n",
    "|   5   |    1000    |  0.01 |   0.3   |   10   |  0.4389  |   0.3525  |  0.8928  |\n",
    "|   6   |    1100    |  0.01 |   0.3   |    6   |  0.4626  |   0.4242  |  0.8938  |\n",
    "|   7   |    1200    |  0.01 |   0.3   |    4   |  0.4719  |   0.4409  |  0.8889  |\n",
    "|   8   |    1100    | 0.012 |   0.3   |    2   |  0.5108  |   0.4725  |  0.8936  |\n",
    "|   9   |    1100    |  0.12 |   0.25  |    2   |  0.4661  |   0.4189  |  0.8926  |\n",
    "\n",
    "No better hyperparameter values have been found. \n",
    "\n",
    "\n",
    "### Disable Early Stopping\n",
    "\n",
    "After further analysis, it is observed that the overall trend of validation loss is decreasing. However, there would be irregular \"spikes\" that trigger early stopping and stop the training process earlier then the optimal case.\n",
    "\n",
    "To address this issue, an extra argument `early_stopping` is added to the `SGD()` function. As a result, the training process is now controlled by the `epochs` parameter.\n",
    "\n",
    "| Trial | hidden_dim |   lr  | dropout | Epochs | Tr. loss | Val. loss | F1-score |\n",
    "|:-----:|:----------:|:-----:|:-------:|:------:|:--------:|:---------:|:--------:|\n",
    "|   0   | 1000       | 0.05  | 0.2     | 5      | 0.3482   | 0.2765    | 0.8953   |\n",
    "|   1   | 1050       | 0.05  | 0.2     | 5      | 0.3568   | 0.2853    | 0.9011   |\n",
    "|   2   | 1100       | 0.05  | 0.2     | 5      | 0.3710   | 0.2544    | 0.8935   |\n",
    "|   3   | 1050       | 0.055 | 0.15    | 5      | 0.3198   | 0.2445    | 0.8980   |\n",
    "|   4   | 1050       | 0.055 | 0.2     | 5      | 0.3559   | 0.2835    | 0.9011   |\n",
    "|   5   | 1050       | 0.06  | 0.2     | 5      | 0.3553   | 0.2821    | 0.9011   |\n",
    "|   6   | 1050       | 0.07  | 0.2     | 5      | 0.3551   | 0.2802    | 0.9020   |\n",
    "|   7   | 1050       | 0.08  | 0.2     | 5      | 0.3559   | 0.2791    | 0.9009   |\n",
    "|   8   | 1050       | 0.075 | 0.2     | 5      | 0.3554   | 0.2796    | 0.9020   |\n",
    "|   9   | 1050       | 0.07  | 0.2     | 10     | 0.3706   | 0.2861    | 0.8824   |\n",
    "\n",
    "The hyperparameter values `hidden_dim=[1050], lr=0.07, dropout=0.2, epochs=5` has achieved a F1-score of 90.2%.\n",
    "\n",
    "\n",
    "### Attempts on Double Hidden Layer Dimensions\n",
    "\n",
    "| Trial | hidden_dim |  lr  | dropout | Epochs | Tr. loss | Val. loss | F1-score |\n",
    "|:-----:|:----------:|:----:|:-------:|:------:|:--------:|:---------:|:--------:|\n",
    "|   0   | 525, 525   | 0.07 | 0.2     | 5      | 0.3783   | 0.2741    | 0.8973   |\n",
    "|   1   | 1000, 50   | 0.07 | 0.2     | 5      | 0.4345   | 0.2467    | 0.8920   |\n",
    "|   2   | 1000, 50   | 0.1  | 0.2     | 5      | 0.4678   | 0.2628    | 0.8954   |\n",
    "|   3   | 1000, 50   | 0.15 | 0.2     | 5      | 0.4882   | 0.3367    | 0.8927   |\n",
    "|   4   | 1000, 50   | 0.1  | 0.2     | 10     | 0.4033   | 0.2668    | 0.8862   |\n",
    "|   5   | 300, 300   | 0.1  | 0.2     | 10     | 0.4323   | 0.4404    | 0.8743   |\n",
    "\n",
    "Unfortunately, the attemps on double hidden layer dimensions did not manage to achieve a new high score.\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "1. Adding a hidden layer will increase the training time due to the extra dimensions, but this does not ensure that a good performance will be produced.\n",
    "\n",
    "2. According to the optimisation attemps, the lowest F1-score obtained is 82.93% in **Searching the First Hidden Layer Dimension**. Therefore, this model is still better than **Average Embedding Model** considering that it requires more optimisation effort and has higher implementation complexity.\n",
    "\n",
    "3. As shown in the plot **Training Monitoring - (Pre-trained + Hidden layers)** above, the sudden \"spike\" increase of both training and validation does not always indicate a negative outcome. The model still managed to achieve a 90.2% of F1-score in this case.\n",
    "\n",
    "4. It is shown that the performance of **Pre-Trained Embeddings + Hidden Layer Model** can be better than **Pre-Trained Embeddings Model**. However, a significant amount of time is required to adjust each hyperparameters to achieve the optimal result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Results\n",
    "\n",
    "|                           Model                           | Accuracy | Precision | Recall | F1-Score |\n",
    "|:---------------------------------------------------------:|:--------:|:---------:|:------:|:--------:|\n",
    "|                     Average Embedding                     |  0.8577  |   0.8575  | 0.8577 |  0.8572  |\n",
    "|              Average Embedding (Pre-trained)              |  0.8977  |   0.8979  | 0.8977 |  0.8967  |\n",
    "| Average Embedding (Pre-trained) + X hidden layers (BONUS) |  0.9033  |   0.9031  | 0.9033 |  0.9020  |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
