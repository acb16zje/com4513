{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [COM4513-6513] Assignment 1: Text Classification with Logistic Regression\n",
    "\n",
    "### Instructor: Nikos Aletras\n",
    "\n",
    "The goal of this assignment is to develop and test two text classification systems:\n",
    "\n",
    "- **Task 1:** sentiment analysis, in particular, to predict the sentiment of movie reviews, i.e. positive or negative (binary classification).\n",
    "- **Task 2:** topic classification, to predict whether a news article is about International issues, Sports or Business (multiclass classification).\n",
    "\n",
    "For that purpose, you will implement:\n",
    "\n",
    "- Text processing methods for extracting Bag-Of-Word features, using (1) unigrams, bigrams and trigrams to obtain vector representations of documents. Two vector weighting schemes should be tested: (1) raw frequencies (**3 marks; 1 for each ngram type**); (2) tf.idf (**1 marks**).\n",
    "- Binary Logistic Regression classifiers that will be able to accurately classify movie reviews trained with (1) BOW-count (raw frequencies); and (2) BOW-tfidf (tf.idf weighted) for Task 1.\n",
    "- Multiclass Logistic Regression classifiers that will be able to accurately classify news articles trained with (1) BOW-count (raw frequencies); and (2) BOW-tfidf (tf.idf weighted) for Task 2.\n",
    "- The Stochastic Gradient Descent (SGD) algorithm to estimate the parameters of your Logistic Regression models. Your SGD algorithm should:\n",
    "    - Minimise the Binary Cross-entropy loss function for Task 1 (**3 marks**)\n",
    "    - Minimise the Categorical Cross-entropy loss function for Task 2 (**3 marks**)\n",
    "    - Use L2 regularisation (both tasks) (**1 mark**)\n",
    "    - Perform multiple passes (epochs) over the training data (**1 mark**)\n",
    "    - Randomise the order of training data after each pass (**1 mark**)\n",
    "    - Stop training if the difference between the current and previous validation loss is smaller than a threshold (**1 mark**)\n",
    "    - After each epoch print the training and development loss (**1 mark**)\n",
    "- Discuss how did you choose hyperparameters (e.g. learning rate and regularisation strength)?  (**2 marks; 0.5 for each model in each task**).\n",
    "- After training the LR models, plot the learning process (i.e. training and validation loss in each epoch) using a line plot (**1 mark; 0.5 for both BOW-count and BOW-tfidf LR models in each task**) and discuss if your model overfits/underfits/is about right.\n",
    "- Model interpretability by showing the most important features for each class (i.e. most positive/negative weights). Give the top 10 for each class and comment on whether they make sense (if they don't you might have a bug!).  If we were to apply the classifier we've learned into a different domain such as laptop reviews or restaurant reviews, do you think these features would generalise well? Can you propose what features the classifier could pick up as important in the new domain? (**2 marks; 0.5 for BOW-count and BOW-tfidf LR models respectively in each task**)\n",
    "\n",
    "### Data - Task 1\n",
    "\n",
    "The data you will use for Task 1 is taken from here: [http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/) and you can find it in the `./data_sentiment` folder in CSV format:\n",
    "\n",
    "- `data_sentiment/train.csv`: contains 1,400 reviews, 700 positive (label: 1) and 700 negative (label: 0) to be used for training.\n",
    "- `data_sentiment/dev.csv`: contains 200 reviews, 100 positive and 100 negative to be used for hyperparameter selection and monitoring the training process.\n",
    "- `data_sentiment/test.csv`: contains 400 reviews, 200 positive and 200 negative to be used for testing.\n",
    "\n",
    "### Data - Task 2\n",
    "\n",
    "The data you will use for Task 2 is a subset of the [AG News Corpus](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html) and you can find it in the `./data_topic` folder in CSV format:\n",
    "\n",
    "- `data_topic/train.csv`: contains 2,400 news articles, 800 for each class to be used for training.\n",
    "- `data_topic/dev.csv`: contains 150 news articles, 50 for each class to be used for hyperparameter selection and monitoring the training process.\n",
    "- `data_topic/test.csv`: contains 900 news articles, 300 for each class to be used for testing.\n",
    "\n",
    "### Submission Instructions\n",
    "\n",
    "You should submit a Jupyter Notebook file (assignment1.ipynb) and an exported PDF version (you can do it from Jupyter: `File->Download as->PDF via Latex`).\n",
    "\n",
    "You are advised to follow the code structure given in this notebook by completing all given functions. You can also write any auxiliary/helper functions (and arguments for the functions) that you might need but note that you can provide a full solution without any such functions. Similarly, you can just use only the packages imported below but you are free to use any functionality from the [Python Standard Library](https://docs.python.org/2/library/index.html), NumPy, SciPy, and Pandas. You are not allowed to use any third-party library such as Scikit-learn (apart from metric functions already provided), NLTK, Spacy, Keras, etc.\n",
    "\n",
    "Please make sure to comment your code. You should also mention if you've used Windows (not recommended) to write and test your code. There is no single correct answer on what your accuracy should be, but correct implementations usually achieve F1-scores around 80\\% or higher. The quality of the analysis of the results is as important as the accuracy itself.\n",
    "\n",
    "This assignment will be marked out of 20. It is worth 20\\% of your final grade in the module.\n",
    "\n",
    "The deadline for this assignment is **23:59 on Fri, 20 Mar 2020** and it needs to be submitted via MOLE. Standard departmental penalties for lateness will be applied. We use a range of strategies to detect [unfair means](https://www.sheffield.ac.uk/ssid/unfair-means/index), including Turnitin which helps detect plagiarism, so make sure you do not plagiarise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# fixing random seed for reproducibility\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Raw texts and labels into arrays\n",
    "\n",
    "First, you need to load the training, development and test sets from their corresponding CSV files (tip: you can use Pandas dataframes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_dev = pd.read_csv('data_sentiment/dev.csv', names=['text', 'label'])\n",
    "sentiment_test = pd.read_csv('data_sentiment/test.csv', names=['text', 'label'])\n",
    "sentiment_train = pd.read_csv('data_sentiment/train.csv', names=['text', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use Pandas you can see a sample of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to put the raw texts into Python lists and their corresponding labels into NumPy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_dev_texts = list(sentiment_dev['text'])\n",
    "sentiment_dev_labels = np.array(sentiment_dev['label'])\n",
    "\n",
    "sentiment_test_texts = list(sentiment_test['text'])\n",
    "sentiment_test_labels = np.array(sentiment_test['label'])\n",
    "\n",
    "sentiment_train_texts = list(sentiment_train['text'])\n",
    "sentiment_train_labels = np.array(sentiment_train['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-Words Representation\n",
    "\n",
    "To train and test Logisitc Regression models, you first need to obtain vector representations for all documents given a vocabulary of features (unigrams, bigrams, trigrams).\n",
    "\n",
    "## Text Pre-Processing Pipeline\n",
    "\n",
    "To obtain a vocabulary of features, you should:\n",
    "- tokenise all texts into a list of unigrams (tip: using a regular expression)\n",
    "- remove stop words (using the one provided or one of your preference)\n",
    "- compute bigrams, trigrams given the remaining unigrams\n",
    "- remove ngrams appearing in less than K documents\n",
    "- use the remaining to create a vocabulary of unigrams, bigrams and trigrams (you can keep top N if you encounter memory issues)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_stop_words = {\n",
    "    'a', 'ad', 'after', 'again', 'all', 'also', 'am', 'an', 'and', 'any',\n",
    "    'are', 'as', 'at', 'be', 'because', 'been', 'being', 'between', 'both',\n",
    "    'but', 'by', 'can', 'could', 'does', 'each', 'ed', 'eg', 'either', 'etc',\n",
    "    'even', 'ever', 'every', 'for', 'from', 'had', 'has', 'have', 'he', 'her',\n",
    "    'hers', 'herself', 'him', 'himself', 'his', 'i', 'ie', 'if', 'in', 'inc',\n",
    "    'into', 'is', 'it', 'its', 'itself', 'li', 'll', 'ltd', 'may', 'maybe',\n",
    "    'me', 'might', 'mine', 'minute', 'minutes', 'must', 'my', 'myself',\n",
    "    'neither', 'nor', 'now', 'of', 'on', 'only', 'or', 'other', 'our', 'ours',\n",
    "    'ourselves', 'own', 'same', 'seem', 'seemed', 'shall', 'she', 'some',\n",
    "    'somehow', 'something', 'sometimes', 'somewhat', 'somewhere', 'spoiler',\n",
    "    'spoilers', 'such', 'suppose', 'that', 'the', 'their', 'theirs', 'them',\n",
    "    'themselves', 'there', 'these', 'they', 'this', 'those', 'thus', 'to',\n",
    "    'today', 'tomorrow', 'us', 've', 'vs', 'was', 'we', 'were', 'what',\n",
    "    'whatever', 'when', 'where', 'which', 'who', 'whom', 'whose', 'will',\n",
    "    'with', 'yesterday', 'you', 'your', 'yours', 'yourself', 'yourselves'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram extraction from a document\n",
    "\n",
    "You first need to implement the `extract_ngrams` function. It takes as input:\n",
    "- `x_raw`: a string corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `vocab`: a given vocabulary. It should be used to extract specific features.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- a list of all extracted features.\n",
    "\n",
    "See the examples below to see how this function should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(x_raw,\n",
    "                   ngram_range=(1, 3),\n",
    "                   token_pattern=r'\\b[A-Za-z]{2,}\\b',\n",
    "                   stop_words=default_stop_words,\n",
    "                   vocab=None):\n",
    "\n",
    "    tokens = [\n",
    "        word.lower() for word in re.findall(token_pattern, x_raw)\n",
    "        if word.lower() not in stop_words\n",
    "    ]\n",
    "\n",
    "    ngrams = []\n",
    "\n",
    "    for n in range(ngram_range[0], ngram_range[1] + 1):\n",
    "        if n == 1:\n",
    "            # Create unigram by concatenating list\n",
    "            ngrams += tokens\n",
    "        else:\n",
    "            # Create bigram / trigram by unzipping list\n",
    "            ngrams += zip(*(tokens[i:] for i in range(n)))\n",
    "\n",
    "    return [ngram for ngram in ngrams if ngram in vocab] if vocab else ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_ngrams('this is a great movie to watch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_ngrams('this is a great movie to watch',\n",
    "               ngram_range=(1, 2),\n",
    "               vocab={'great', ('great', 'movie')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it is OK to represent n-grams using lists instead of tuples: e.g. `['great', ['great', 'movie']]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocabulary of n-grams\n",
    "\n",
    "Then the `get_vocab` function will be used to (1) create a vocabulary of ngrams; (2) count the document frequencies of ngrams; (3) their raw frequency. It takes as input:\n",
    "- `X_raw`: a list of strings each corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `min_df`: keep ngrams with a minimum document frequency.\n",
    "- `keep_topN`: keep top-N more frequent ngrams.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `vocab`: a set of the n-grams that will be used as features.\n",
    "- `df`: a Counter (or dict) that contains ngrams as keys and their corresponding document frequency as values.\n",
    "- `ngram_counts`: counts of each ngram in vocab\n",
    "\n",
    "Hint: it should make use of the `extract_ngrams` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(X_raw,\n",
    "              ngram_range=(1, 3),\n",
    "              token_pattern=r'\\b[A-Za-z]{2,}\\b',\n",
    "              min_df=1,\n",
    "              keep_topN=None,\n",
    "              stop_words=default_stop_words):\n",
    "\n",
    "    df = Counter()\n",
    "    ngram_counts = Counter()\n",
    "\n",
    "    for text in X_raw:\n",
    "        # A list of ngrams for the given document `text`\n",
    "        ngram_list = extract_ngrams(text, ngram_range, token_pattern, stop_words)\n",
    "        \n",
    "        # Count document frequency\n",
    "        df.update(set(ngram_list))\n",
    "\n",
    "        # Count ngram frequency\n",
    "        ngram_counts.update(ngram for ngram in ngram_list if df[ngram] >= min_df)\n",
    "    \n",
    "    # Extract ngram into vocab set\n",
    "    vocab = {ngram for ngram, _ in ngram_counts.most_common(keep_topN)}\n",
    "\n",
    "    return vocab, df, ngram_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should use `get_vocab` to create your vocabulary and get document and raw frequencies of n-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab, df, _ = get_vocab(sentiment_train_texts, keep_topN=5000)\n",
    "print(len(vocab))\n",
    "print()\n",
    "print(list(vocab)[:100])\n",
    "print()\n",
    "print(df.most_common()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you need to create vocabulary id -> word and word -> id dictionaries for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_id_to_word = dict(enumerate(vocab))\n",
    "\n",
    "word_to_vocab_id = {v: k for k, v in vocab_id_to_word.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should be able to extract n-grams for each text in the training, development and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_train_texts_ngrams = (extract_ngrams(text, vocab=vocab)\n",
    "                                for text in sentiment_train_texts)\n",
    "\n",
    "sentiment_dev_texts_ngrams = (extract_ngrams(text, vocab=vocab)\n",
    "                              for text in sentiment_dev_texts)\n",
    "\n",
    "sentiment_test_texts_ngrams = (extract_ngrams(text, vocab=vocab)\n",
    "                               for text in sentiment_test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorise documents\n",
    "\n",
    "Next, write a function `vectoriser` to obtain Bag-of-ngram representations for a list of documents. The function should take as input:\n",
    "- `X_ngram`: a list of texts (documents), where each text is represented as list of n-grams in the `vocab`\n",
    "- `vocab`: a set of n-grams to be used for representing the documents\n",
    "\n",
    "and return:\n",
    "- `X_vec`: an array with dimensionality Nx|vocab| where N is the number of documents and |vocab| is the size of the vocabulary. Each element of the array should represent the frequency of a given n-gram in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorise(X_ngram, vocab):\n",
    "    X_vec = []\n",
    "\n",
    "    for ngram_list in X_ngram:\n",
    "        counter = Counter(ngram_list)\n",
    "        X_vec.append([counter[v] for v in vocab])\n",
    "\n",
    "    return np.array(X_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use `vectorise` to obtain document vectors for each document in the train, development and test set. You should extract both count and tf.idf vectors respectively:\n",
    "\n",
    "### Count vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_train_count = vectorise(sentiment_train_texts_ngrams, vocab)\n",
    "\n",
    "sentiment_dev_count = vectorise(sentiment_dev_texts_ngrams, vocab)\n",
    "\n",
    "sentiment_test_count = vectorise(sentiment_test_texts_ngrams, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_train_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_train_count[:2,:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF.IDF vectors\n",
    "\n",
    "First compute `idfs` an array containing inverted document frequencies (Note: its elements should correspond to your `vocab`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sentiment_train_docs = len(sentiment_train_texts)\n",
    "total_sentiment_dev_docs = len(sentiment_dev_texts)\n",
    "total_sentiment_test_docs = len(sentiment_test_texts)\n",
    "\n",
    "_, sentiment_dev_df, _ = get_vocab(sentiment_dev_texts, keep_topN=5000)\n",
    "\n",
    "_, sentiment_test_df, _ = get_vocab(sentiment_test_texts, keep_topN=5000)\n",
    "\n",
    "sentiment_train_idf = np.array([\n",
    "    np.log10(total_sentiment_train_docs / df[v]) for v in vocab]\n",
    ")\n",
    "\n",
    "sentiment_dev_idf = np.array([\n",
    "    np.log10(total_sentiment_dev_docs / sentiment_dev_df[v])\n",
    "    if sentiment_dev_df[v] else 0 for v in vocab\n",
    "])\n",
    "\n",
    "sentiment_test_idf = np.array([\n",
    "    np.log10(total_sentiment_test_docs / sentiment_test_df[v])\n",
    "    if sentiment_test_df[v] else 0 for v in vocab\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then transform your count vectors to TF.IDF vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the \"log normalisation\" variant to scale TF for better results\n",
    "sentiment_train_tfidf = np.log10(1 + sentiment_train_count) * sentiment_train_idf\n",
    "\n",
    "sentiment_dev_tfidf = np.log10(1 + sentiment_dev_count) * sentiment_dev_idf\n",
    "\n",
    "sentiment_test_tfidf = np.log10(1 + sentiment_test_count) * sentiment_test_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_train_tfidf[1, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Logistic Regression\n",
    "\n",
    "After obtaining vector representations of the data, now you are ready to implement Binary Logistic Regression for classifying sentiment.\n",
    "\n",
    "First, you need to implement the `sigmoid` function. It takes as input:\n",
    "\n",
    "- `z`: a real number or an array of real numbers\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `sig`: the sigmoid of `z`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sigmoid(0))\n",
    "print(sigmoid(np.array([-5., 1.2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, implement the `predict_proba` function to obtain prediction probabilities. It takes as input:\n",
    "\n",
    "- `X`: an array of inputs, i.e. documents represented by bag-of-ngram vectors ($N \\times |vocab|$)\n",
    "- `weights`: a 1-D array of the model's weights $(1, |vocab|)$\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `preds_proba`: the prediction probabilities of X given the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(X, weights):\n",
    "    z = X.dot(weights)\n",
    "\n",
    "    return sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, implement the `predict_class` function to obtain the most probable class for each vector in an array of input vectors. It takes as input:\n",
    "\n",
    "- `X`: an array of documents represented by bag-of-ngram vectors ($N \\times |vocab|$)\n",
    "- `weights`: a 1-D array of the model's weights $(1, |vocab|)$\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `preds_class`: the predicted class for each x in X given the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(X, weights):\n",
    "    return [0 if prob < 0.5 else 1 for prob in predict_proba(X, weights)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn the weights from data, we need to minimise the binary cross-entropy loss. Implement `binary_loss` that takes as input:\n",
    "\n",
    "- `X`: input vectors\n",
    "- `Y`: labels\n",
    "- `weights`: model weights\n",
    "- `alpha`: regularisation strength\n",
    "\n",
    "and return:\n",
    "\n",
    "- `l`: the loss score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_loss(X, Y, weights, alpha=0.00001):\n",
    "    predicted_probabilities = predict_proba(X, weights)\n",
    "\n",
    "    l = -Y * np.log(predicted_probabilities) - (1 - Y) * np.log(1 - predicted_probabilities)\n",
    "\n",
    "    # L2 Regularisation\n",
    "    l += alpha * weights.dot(weights)\n",
    "\n",
    "    # Return the average loss\n",
    "    return np.mean(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can implement Stochastic Gradient Descent to learn the weights of your sentiment classifier. The `SGD` function takes as input:\n",
    "\n",
    "- `X_tr`: array of training data (vectors)\n",
    "- `Y_tr`: labels of `X_tr`\n",
    "- `X_dev`: array of development (i.e. validation) data (vectors)\n",
    "- `Y_dev`: labels of `X_dev`\n",
    "- `lr`: learning rate\n",
    "- `alpha`: regularisation strength\n",
    "- `epochs`: number of full passes over the training data\n",
    "- `tolerance`: stop training if the difference between the current and previous validation loss is smaller than a threshold\n",
    "- `print_progress`: flag for printing the training progress (train/validation loss)\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `weights`: the weights learned\n",
    "- `training_loss_history`: an array with the average losses of the whole training set after each epoch\n",
    "- `validation_loss_history`: an array with the average losses of the whole development set after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(X_tr, Y_tr, X_dev, Y_dev, lr=0.1, alpha=0.00001, epochs=5, tolerance=0.0001, print_progress=True):\n",
    "    # fixing random seed for reproducibility\n",
    "    np.random.seed(123)\n",
    "    training_loss_history = []\n",
    "    validation_loss_history = []\n",
    "    \n",
    "    # Initialise weight to zero\n",
    "    weights = np.zeros(X_tr.shape[1])\n",
    "\n",
    "    # Create training tuples\n",
    "    train_docs = list(zip(X_tr, Y_tr))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Randomise order in train_docs\n",
    "        np.random.shuffle(train_docs)\n",
    "\n",
    "        for x_i, y_i in train_docs:\n",
    "            weights -= lr * (x_i * (predict_proba(x_i, weights) - y_i) + 2 * alpha * weights)\n",
    "\n",
    "        # Monitor training and validation loss\n",
    "        cur_loss_tr = binary_loss(X_tr, Y_tr, weights, alpha)\n",
    "        cur_loss_dev = binary_loss(X_dev, Y_dev, weights, alpha)\n",
    "\n",
    "        # Early stopping\n",
    "        if epoch > 0 and validation_loss_history[-1] - cur_loss_dev < tolerance:\n",
    "            break\n",
    "        else:\n",
    "            training_loss_history.append(cur_loss_tr)\n",
    "            validation_loss_history.append(cur_loss_dev)\n",
    "\n",
    "        if print_progress:\n",
    "            print(f'Epoch: {epoch} | Training loss: {cur_loss_tr} | Validation loss: {cur_loss_dev}')\n",
    "            \n",
    "    return weights, training_loss_history, validation_loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Binary Logistic Regression with Count Vectors\n",
    "\n",
    "First train the model using SGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w_count, tr_loss_count, dev_loss_count = SGD(X_tr=sentiment_train_count,\n",
    "                                             Y_tr=sentiment_train_labels,\n",
    "                                             X_dev=sentiment_dev_count,\n",
    "                                             Y_dev=sentiment_dev_labels,\n",
    "                                             lr=0.00010125,\n",
    "                                             alpha=0.00001,\n",
    "                                             epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the training and validation history per epoch. Does your model underfit, overfit or is it about right? Explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(tr_loss_count, label='Training loss')\n",
    "plt.plot(dev_loss_count, label='Validation loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.title('Training Monitoring (Binary - Count)')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the plot **Training Monitoring (Binary - Count)**, \n",
    "\n",
    "1. The training loss decreases as epoch increases and eventually reaches a point of stability\n",
    "2. The validation loss decreases as epoch increases and eventually reaches a point of stability\n",
    "3. There exists a \"generalisation gap\" between validation and training loss\n",
    "\n",
    "The following techniques are implemented in the Stochastic Gradient Descent algorithm to avoid overfitting of the training data:\n",
    "\n",
    "1. Early stopping\n",
    "2. L2 regularisation\n",
    "\n",
    "Hence, the model is **about right**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy, precision, recall and F1-scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args = sentiment_test_labels, predict_class(sentiment_test_count, w_count)\n",
    "\n",
    "print('Accuracy:', accuracy_score(*args))\n",
    "print('Precision:', precision_score(*args))\n",
    "print('Recall:', recall_score(*args))\n",
    "print('F1-Score:', f1_score(*args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, print the top-10 words for the negative and positive class respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top10_positive_ids = (-w_count).argsort()[:10]\n",
    "top10_negative_ids = w_count.argsort()[:10]\n",
    "\n",
    "print(\n",
    "    f'Top 10 positive: {[vocab_id_to_word[id] for id in top10_positive_ids]} \\n'\n",
    ")\n",
    "print(\n",
    "    f'Top 10 negative: {[vocab_id_to_word[id] for id in top10_negative_ids]}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Evaluation\n",
    "\n",
    "The top 10 features obtained for each class using **count vectors** are reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Binary Logistic Regression with TF.IDF Vectors\n",
    "\n",
    "Follow the same steps as above (i.e. evaluating count n-gram representations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w_tfidf, tr_loss_tfidf, dev_loss_tfidf = SGD(X_tr=sentiment_train_tfidf,\n",
    "                                             Y_tr=sentiment_train_labels,\n",
    "                                             X_dev=sentiment_dev_tfidf,\n",
    "                                             Y_dev=sentiment_dev_labels,\n",
    "                                             lr=0.00322,\n",
    "                                             alpha=0.0005,\n",
    "                                             epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the training and validation history per epoch. Does your model underfit, overfit or is it about right? Explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(tr_loss_tfidf, label='Training loss')\n",
    "plt.plot(dev_loss_tfidf, label='Validation loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.title('Training Monitoring (Binary - TFIDF)')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the plot **Training Monitoring (Binary - TFIDF)**, \n",
    "\n",
    "1. The training loss decreases as epoch increases and eventually reaches a point of stability\n",
    "2. The validation loss decreases as epoch increases and eventually reaches a point of stability\n",
    "3. There exists a \"generalisation gap\" between validation and training loss\n",
    "\n",
    "The following techniques are implemented in the Stochastic Gradient Descent algorithm to avoid overfitting of the training data:\n",
    "\n",
    "1. Early stopping\n",
    "2. L2 regularisation\n",
    "\n",
    "Hence, the model is **about right**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy, precision, recall and F1-scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args = sentiment_test_labels, predict_class(sentiment_test_tfidf, w_tfidf)\n",
    "\n",
    "print('Accuracy:', accuracy_score(*args))\n",
    "print('Precision:', precision_score(*args))\n",
    "print('Recall:', recall_score(*args))\n",
    "print('F1-Score:', f1_score(*args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print top-10 most positive and negative words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_positive_ids = (-w_tfidf).argsort()[:10]\n",
    "top10_negative_ids = w_tfidf.argsort()[:10]\n",
    "\n",
    "print(\n",
    "    f'Top 10 positive: {[vocab_id_to_word[id] for id in top10_positive_ids]} \\n'\n",
    ")\n",
    "print(\n",
    "    f'Top 10 negative: {[vocab_id_to_word[id] for id in top10_negative_ids]}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Evaluation\n",
    "\n",
    "The top 10 features obtained for each class using **TF.IDF vectors** are reasonable. They are more relevant than the features obtained using **count vectors**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If we were to apply the classifier we've learned into a different domain such as laptop reviews or restaurant reviews, do you think these features would generalise well? Can you propose what features the classifier could pick up as important in the new domain?\n",
    "\n",
    "### Count Vectors\n",
    "\n",
    "The following top 10 words: 'great', 'well', 'bad', 'worst', 'unfortunately' are common words in reviews. If the classifier is to apply into a different domain, it is expected that the classier will be able to correctly classify some of the reviews, assuming that the reviews satisfy the following conditions:\n",
    "\n",
    "1. The positive reviews must contain words that have been learnt by the model as positive (e.g. great, well, etc.)\n",
    "2. The negative reviews must contain words that have been learnt by the model as negative (e.g. bad, worst, etc.)\n",
    "\n",
    "However, this assumption is unlikely to be true for most of the laptop or restaurant reviews in real-life scenarios. A user may give a positive rating despite writing many negative words in the review. It is also possible that a review contains only neutral unemotional words but expresses a different sentiment.\n",
    "\n",
    "Most of the top features are irrelevant to laptop or restaurant reviews, such as 'fun', 'movies', 'script', 'boring, 'plot', etc. This implies that the classifier is likely to be underfitting in the new domains and perform worse than the movie domain. Therefore, these features **would not generalise well** in a new domain.\n",
    "\n",
    "### TF.IDF Vectors\n",
    "\n",
    "The TF.IDF vectors model has better performance than the count vectors model on the movie review domain due to the top features being identified more accurately. Conversely, this has implied that the TF.IDF vectors model is less generalised than the count vectors model. Hence, the features **would not generalise well** in a new domain too.\n",
    "\n",
    "### Features in the New Domain\n",
    "\n",
    "Apart from the common sentiment lexicon (e.g. good, bad), the classifier could pick up features that is specific to the new domain. Below is an estimation of possible top features in the respective new domains:\n",
    "\n",
    "Laptop reviews: The features learned are likely to be the terms that are related to the attribute of an electrical device, as shown in the following list. Possible n-grams may include `(durable, battery)`, `(hd, screen)`, etc.\n",
    "\n",
    "- long ; short (battery life)\n",
    "- light ; heavy (weight)\n",
    "- thin ; bulky (physical size)\n",
    "- cheap ; expensive (price)\n",
    "- fast ; slow (performance\n",
    "\n",
    "Restaurant reviews: The features learned are likely to be the terms that are related to food quality and the customer experience in the restaurant, as shown in the following list. Possible n-grams may include `(polite, staff)`, `(tasty, food)`, etc.\n",
    "\n",
    "- long ; short (waiting time)\n",
    "- delicious ; disgusting, tasteless (food quality) \n",
    "- polite ; rude (staff attitude)\n",
    "- cosy ; dull (environment)\n",
    "- clean ; dirty, messy (hygiene)\n",
    "- cheap ; expensive (price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discuss how did you choose model hyperparameters (e.g. learning rate and regularisation strength)? What is the relation between training epochs and learning rate? How does the regularisation strength affect performance?\n",
    "\n",
    "### Objective\n",
    "\n",
    "The primary objective is to find the best configuration of hyperparameters that will give the best scores on the development and test set. It can be achieved by improving the generalisation of the learned model through the following processes:\n",
    "\n",
    "1. Lower the generalisation gap between validation and training loss. This is based on the assumption that both the validation and training loss has reached a point of stability, and the model is in good fit.\n",
    "\n",
    "2. Improve, or at least maintain the precision, recall, and F1-score while lowering the validation loss.\n",
    "\n",
    "The processes are carried out using *trial and error* strategy. Small performance improvements (≤ 2%) are expected after the optimisation. The quality of the training dataset is still the major factor in model performance.\n",
    "\n",
    "While many existing hyperparameters optimisation algorithms are better than the chosen strategy, almost all of them require a searching algorithm to work. After considering the scope of this assignment and the feasibility of self-implementing a searching algorithm, it is decided to choose a simpler approach for experimentation and learning purpose.\n",
    "\n",
    "### Limitations of Trial and Error Method\n",
    "\n",
    "Although the trial and error method is a straightforward strategy and requires no extra implementation, it is prone to the local optimum problem. It is impractical to try every possible combination of learning rate and regularisation strength to find the optimal result. Therefore, the best achievable performance improvement through fine-tuning is largely dependent on the initial set of values selected to explore the hyperparameters. \n",
    "\n",
    "Furthermore, another optimisation problem has arisen as the number of hyperparameters to search is more than one. This has lead to another assumption that the optimisation order of hyperparameters may have a certain impact on the final result.\n",
    "\n",
    "### Hyperparameters Optimisation\n",
    "\n",
    "Before performing the search on hyperparameters, the lower and upper bounds for the values of the hyperparameters need to be defined. The lower bound value is the baseline for the model performance, and the upper bound is the value where the performance would start dropping. Both lower bound and upper bound would converge after each trial. \n",
    "\n",
    "For consistency, the initial lower and upper bound of learning rate is set to `0.0001` and `0.1` respectively. The initial lower and upper bound of regularisation strength is set to `0.00001` and `0.01` respectively.\n",
    "\n",
    "#### Optimisation procedure for learning rate\n",
    "\n",
    "1. Set `lr=0.0001` and `alpha=0.00001`. Train and record the lower bound scores.\n",
    "\n",
    "2. If the gradient descent is taking too many epochs (> 100) to converge, increase the value of `lr`. Repeat this step until the number of epochs is below 100. If the final scores are higher than the current lower bound scores, then update the lower bound, else update the upper bound\n",
    "\n",
    "3. If the current `lr` is the lower bound value, increase `lr` until the new scores are lower than the current lower bound scores, then record this value as the new upper bound.\n",
    "\n",
    "4. If the current `lr` is the upper bound value, decrease `lr` until the new scores are higher than the current upper bound scores. Record this value as the new lower bound.\n",
    "\n",
    "5. Repeat Step 3 and 4 until the upper bound scores ≥ lower bound scores. The `lr` value is now considered to be the optimal value.\n",
    "\n",
    "\n",
    "#### Optimisation procedure for regularisation strength\n",
    "\n",
    "1. Set `lr` to the optimal value obtained and `alpha=0.00001`. Train and record the lower bound validation loss.\n",
    "\n",
    "2. If the current `alpha` is the lower bound value, increase `alpha` until the validation loss has increased **OR** the scores have decreased. Record this value as the new upper bound\n",
    "\n",
    "3. If the current `alpha` is the upper bound value, decrease `alpha` until the new validation loss is lower than the current upper bound validation loss. Record this value as the new lower bound.\n",
    "\n",
    "4. Repeat Step 2 and 3 until a lower validation loss has been found **OR** the scores have improved.\n",
    "\n",
    "**Note:** The optimisation procedure is a general guideline for the manual hyperparameters search. There may be cases where several changes have to be adapted. \n",
    "\n",
    "1. There is no fixed value of how much should `lr` and `alpha` increase or decrease during the optimisation. `lr` and `alpha` are adjusted according to the amount of changes in scores and validation loss in the previous trial.\n",
    "\n",
    "2. When optimising `alpha`, it is possible that the validation loss and scores are not improving after several trials. In this case, the initial `alpha` value is considered to be the optimum.\n",
    "\n",
    "\n",
    "### Count Vectors\n",
    "\n",
    "#### Optimising Learning Rate\n",
    "\n",
    "Initial parameters: `lr=0.0001, alpha=0.00001`\n",
    "\n",
    "| Trial | Learning rate | Epochs | Tr. loss | Val. loss | Precision | Recall | F1-Score |\n",
    "|-------|---------------|--------|----------|-----------|-----------|--------|----------|\n",
    "| 0     | 0.0001        | 70     | 0.2037   | 0.4100    | 0.8536    | 0.875  | 0.8641   |\n",
    "| 1     | 0.00011       | 70     | 0.1932   | 0.4072    | 0.8522    | 0.865  | 0.8585   |\n",
    "| 2     | 0.000105      | 70     | 0.1983   | 0.4085    | 0.8487    | 0.87   | 0.8592   |\n",
    "| 3     | 0.0001025     | 70     | 0.2009   | 0.4092    | 0.8487    | 0.87   | 0.8592   |\n",
    "| 4     | 0.00010125    | 70     | 0.2023   | 0.4096    | 0.8536    | 0.875  | 0.8641   |\n",
    "\n",
    "The optimal `lr` is found to be `0.00010125`.\n",
    "\n",
    "#### Optimising Regularisation Strength\n",
    "\n",
    "Initial parameters: `lr=0.00010125, alpha=0.00001`\n",
    "\n",
    "| Trial | Alpha      | Epochs | Tr. loss | Val. loss | Precision | Recall | F1-Score |\n",
    "|-------|------------|--------|----------|-----------|-----------|--------|----------|\n",
    "| 0     | 0.00001    | 70     | 0.2023   | 0.40962   | 0.8536    | 0.875  | 0.8641   |\n",
    "| 1     | 0.00002    | 70     | 0.2024   | 0.40966   | 0.8536    | 0.875  | 0.8641   |\n",
    "| 2     | 0.000015   | 70     | 0.2023   | 0.40964   | 0.8536    | 0.875  | 0.8641   |\n",
    "| 3     | 0.0000125  | 70     | 0.2023   | 0.40963   | 0.8536    | 0.875  | 0.8641   |\n",
    "| 4     | 0.00001125 | 70     | 0.2023   | 0.40963   | 0.8536    | 0.875  | 0.8641   |\n",
    "| 5     | 0.0001     | 70     | 0.2028   | 0.4099    | 0.8536    | 0.875  | 0.8641   |\n",
    "| 6     | 0.001      | 70     | 0.2074   | 0.4134    | 0.8536    | 0.875  | 0.8641   |\n",
    "| 7     | 0.01       | 47     | 0.2788   | 0.4487    | 0.8514    | 0.86   | 0.8557   |\n",
    "\n",
    "No better `alpha` has been found. The initial `alpha` is considered to be the optimal value.\n",
    "\n",
    "**Conclusion**: Based on the trials above, it can be concluded that the optimal values are `lr=0.00010125` and `alpha=0.00001`\n",
    "\n",
    "\n",
    "### TF.IDF Vectors\n",
    "\n",
    "#### Optimising Learning Rate\n",
    "\n",
    "Initial parameters: `lr=0.0001, alpha=0.00001`\n",
    "\n",
    "| Trial | Learning rate | Epochs | Tr. loss | Val. loss | Precision | Recall | F1-Score |\n",
    "|-------|---------------|--------|----------|-----------|-----------|--------|----------|\n",
    "| 0     | 0.0001        | 99     | 0.5025   | 0.5866    | 0.8436    | 0.89   | 0.8661   |\n",
    "| 1     | 0.0002        | 99     | 0.4053   | 0.5329    | 0.8634    | 0.885  | 0.8740   |\n",
    "| 2     | 0.0003        | 99     | 0.3439   | 0.4994    | 0.8676    | 0.885  | 0.8762   |\n",
    "| 3     | 0.001         | 99     | 0.1760   | 0.4125    | 0.8923    | 0.87   | 0.8810   |\n",
    "| 4     | 0.002         | 99     | 0.1059   | 0.3819    | 0.8883    | 0.875  | 0.8816   |\n",
    "| 5     | 0.003         | 99     | 0.0762   | 0.3724    | 0.8826    | 0.865  | 0.8737   |\n",
    "| 6     | 0.0029        | 99     | 0.0784   | 0.3730    | 0.8826    | 0.865  | 0.8737   |\n",
    "| 7     | 0.0028        | 99     | 0.0807   | 0.3736    | 0.8871    | 0.865  | 0.8759   |\n",
    "| 8     | 0.0027        | 99     | 0.0831   | 0.3743    | 0.8871    | 0.865  | 0.8759   |\n",
    "| 9     | 0.0026        | 99     | 0.0858   | 0.3751    | 0.8871    | 0.865  | 0.8759   |\n",
    "| 10    | 0.0025        | 99     | 0.0893   | 0.3762    | 0.8883    | 0.875  | 0.8816   |\n",
    "\n",
    "The \"temporary\" optimal `lr` is found to be `0.0025` because the model is not showing a converging trenddespite the learning rate has increased significantly.\n",
    "\n",
    "#### Optimising Regularisation Strength\n",
    "\n",
    "Initial parameters: `lr=0.0025, alpha=0.00001`\n",
    "\n",
    "| Trial | Alpha   | Epochs | Tr. loss | Val. loss | Precision | Recall | F1-Score |\n",
    "|-------|---------|--------|----------|-----------|-----------|--------|----------|\n",
    "| 0     | 0.00001 | 99     | 0.0893   | 0.3762    | 0.8883    | 0.875  | 0.8816   |\n",
    "| 1     | 0.00002 | 99     | 0.0904   | 0.3775    | 0.8883    | 0.875  | 0.8816   |\n",
    "| 2     | 0.00004 | 99     | 0.0940   | 0.3805    | 0.8838    | 0.875  | 0.8793   |\n",
    "| 3     | 0.0001  | 99     | 0.1044   | 0.3893    | 0.8838    | 0.875  | 0.8793   |\n",
    "| 4     | 0.0002  | 83     | 0.1302   | 0.4043    | 0.8883    | 0.875  | 0.8816   |\n",
    "| 5     | 0.0004  | 70     | 0.1645   | 0.4265    | 0.8934    | 0.88   | 0.8866   |\n",
    "| 6     | 0.0006  | 58     | 0.1943   | 0.4439    | 0.9025    | 0.88   | 0.8911   |\n",
    "| 7     | 0.0008  | 48     | 0.2206   | 0.4583    | 0.8974    | 0.875  | 0.8860   |\n",
    "\n",
    "A converging pattern has been observed. However, since the `lr` used is the \"temporary\" optimal value, a further optimisation is required validate it.\n",
    "\n",
    "#### Optimising Learning Rate and Regularisation Strength\n",
    "\n",
    "Based on the trials above, it could be concluded that the optimal values are `lr=0.0025` and `alpha=0.0006`. However, it has been observed that increasing the learning rate only is not decreasing the epochs required. The epochs required only start decreasing when `alpha` is increased to `0.0002`. Therefore, further optimisation is required to validate the optimal values.\n",
    "\n",
    "Initial parameters: `lr=0.0025, alpha=0.0006`\n",
    "\n",
    "| Trial | Learning rate | Alpha   | Epochs | Tr. loss | Val. loss | Precision | Recall | F1-Score |\n",
    "|-------|---------------|---------|--------|----------|-----------|-----------|--------|----------|\n",
    "| 0     | 0.0025        | 0.0006  | 58     | 0.1943   | 0.4439    | 0.9025    | 0.88   | 0.8911   |\n",
    "| 1     | 0.003         | 0.0006  | 47     | 0.1955   | 0.4440    | 0.9025    | 0.88   | 0.8911   |\n",
    "| 2     | 0.004         | 0.0006  | 37     | 0.1924   | 0.4437    | 0.9025    | 0.88   | 0.8911   |\n",
    "| 3     | 0.005         | 0.0006  | 29     | 0.1929   | 0.4437    | 0.9025    | 0.88   | 0.8911   |\n",
    "| 4     | 0.006         | 0.0006  | 24     | 0.1927   | 0.4436    | 0.9025    | 0.88   | 0.8911   |\n",
    "| 5     | 0.003         | 0.00055 | 51     | 0.1866   | 0.4399    | 0.9025    | 0.88   | 0.8911   |\n",
    "| 6     | 0.0032        | 0.0005  | 48     | 0.1815   | 0.4359    | 0.9025    | 0.88   | 0.8911   |\n",
    "\n",
    "After further optimisation, it can be concluded that the optimal values are `lr=0.0032` and `alpha=0.0005`. These two values have the lowest training and validation loss while maintaining the same scores.\n",
    "\n",
    "### Relationship Between Epochs and Learning Rate\n",
    "\n",
    "Before conducting any of the optimisation trials, the relationship between epochs and learning can already be deduced from the equation in `SGD` function. By observing the line `weights -= lr * (...)` in `SGD` function, it can be deduced that the larger the learning rate, the bigger the weight update after each epoch. Therefore, it is assumed that the higher the learning rate, the lower the epochs required to converge. \n",
    "\n",
    "As shown in the table in section **TF.IDF: Optimising Learning Rate and Regularisation Strength** above, it has been proved that epochs are inversely proportional to the learning rate. Another test has been conducted to further validate the assumption,\n",
    "\n",
    "- Test vector: `count`\n",
    "- Parameters: `alpha=0.00001, epochs=300`\n",
    "\n",
    "| Trial | Learning rate | Epochs required |\n",
    "|-------|---------------|-----------------|\n",
    "| 0     | 0.00001       | 299             |\n",
    "| 1     | 0.0001        | 70              |\n",
    "| 2     | 0.001         | 14              |\n",
    "| 3     | 0.01          | 0               |\n",
    "\n",
    "\n",
    "#### Discussion about Epochs, Learning Rate, and Model Performance\n",
    "\n",
    "Choosing a good learning rate is challenging as every model differs from each other. Generally, it is required to perform some preliminary analysis on the model performance before the hyperparameters optimisation. \n",
    "\n",
    "- If the learning rate is too large, the model may overshoot and lead to divergent behaviour (epochs required is low)\n",
    "- If the learning rate is too small, the model will require many updates to the weights before the loss is converged (epochs required is high)\n",
    "\n",
    "### Relationship Between Regularisation Strength and Model Performance\n",
    "\n",
    "From the table in **Count Vectors: Optimising Regularisation Strength** and **TF.IDF Vectors: Optimising Regularisation Strength**, it is observed that the regularisation strength can affect the epochs required to converge, and thus can affect the model performance indirectly. Moreover, a small change in regularisation strength would only have minimal impact on the overall metrics.\n",
    "\n",
    "By observing the line `weights -= lr * (... + 2 * alpha * weights)` in `SGD` function, it can be deduced that the higher the regularisation strength, the lower the epochs required to converge. However, as shown in **TF.IDF Vectors: Optimising Regularisation Strength**, increasing the regularisation strength will also increase both training and validation loss. This does not always produce an adverse effect on the model performance, as performance gained has been observed when the regularisation strength has been increased to a certain value.\n",
    "\n",
    "#### Discussion about Loss and Model Performance\n",
    "\n",
    "The increase in training and validation loss which lead to the improvement in model performance can be explained as follows:\n",
    "\n",
    "1. Before the optimisation, the model is slightly overfitted on the training dataset\n",
    "2. After the optimisation, the increase in regularisation strength has improved the generalisation of the model, thus observing an  increase in training and validation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Results\n",
    "\n",
    "|     LR    |      Precision     | Recall |      F1-Score      |\n",
    "|:---------:|:------------------:|:------:|:------------------:|\n",
    "| BOW-count | 0.8536585365853658 |  0.875 | 0.8641975308641976 |\n",
    "| BOW-tfidf | 0.9025641025641026 |  0.88  | 0.8911392405063291 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class Logistic Regression\n",
    "\n",
    "Now you need to train a Multiclass Logistic Regression (MLR) Classifier by extending the Binary model you developed above. You will use the MLR model to perform topic classification on the AG news dataset consisting of three classes:\n",
    "\n",
    "- Class 1: World\n",
    "- Class 2: Sports\n",
    "- Class 3: Business\n",
    "\n",
    "You need to follow the same process as in Task 1 for data processing and feature extraction by reusing the functions you wrote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dev = pd.read_csv('data_topic/dev.csv', names=['label', 'text'])\n",
    "topic_test = pd.read_csv('data_topic/test.csv', names=['label', 'text'])\n",
    "topic_train = pd.read_csv('data_topic/train.csv', names=['label', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dev_texts = list(topic_dev['text'])\n",
    "topic_dev_labels = np.array(topic_dev['label'])\n",
    "\n",
    "topic_test_texts = list(topic_test['text'])\n",
    "topic_test_labels = np.array(topic_test['label'])\n",
    "\n",
    "topic_train_texts = list(topic_train['text'])\n",
    "topic_train_labels = np.array(topic_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, df, _ = get_vocab(topic_train_texts, keep_topN=5000)\n",
    "print(len(vocab))\n",
    "print()\n",
    "print(list(vocab)[:100])\n",
    "print()\n",
    "print(df.most_common()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_id_to_word = dict(enumerate(vocab))\n",
    "\n",
    "word_to_vocab_id = {v: k for k, v in vocab_id_to_word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_train_texts_ngrams = (extract_ngrams(text, vocab=vocab)\n",
    "                            for text in topic_train_texts)\n",
    "\n",
    "topic_dev_texts_ngrams = (extract_ngrams(text, vocab=vocab)\n",
    "                          for text in topic_dev_texts)\n",
    "\n",
    "topic_test_texts_ngrams = (extract_ngrams(text, vocab=vocab)\n",
    "                           for text in topic_test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_train_count = vectorise(topic_train_texts_ngrams, vocab)\n",
    "\n",
    "topic_dev_count = vectorise(topic_dev_texts_ngrams, vocab)\n",
    "\n",
    "topic_test_count = vectorise(topic_test_texts_ngrams, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF.IDF vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_topic_train_docs = len(topic_train_texts)\n",
    "total_topic_dev_docs = len(topic_dev_texts)\n",
    "total_topic_test_docs = len(topic_test_texts)\n",
    "\n",
    "_, topic_dev_df, _ = get_vocab(topic_dev_texts, keep_topN=5000)\n",
    "\n",
    "_, topic_test_df, _ = get_vocab(topic_test_texts, keep_topN=5000)\n",
    "\n",
    "topic_train_idf = np.array([\n",
    "    np.log10(total_topic_train_docs / df[v]) for v in vocab]\n",
    ")\n",
    "\n",
    "topic_dev_idf = np.array([\n",
    "    np.log10(total_topic_dev_docs / topic_dev_df[v])\n",
    "    if topic_dev_df[v] else 0 for v in vocab\n",
    "])\n",
    "\n",
    "topic_test_idf = np.array([\n",
    "    np.log10(total_topic_test_docs / topic_test_df[v])\n",
    "    if topic_test_df[v] else 0 for v in vocab\n",
    "])\n",
    "\n",
    "# Use the \"log normalisation\" variant to scale TF for better results\n",
    "topic_train_tfidf = np.log10(1 + topic_train_count) * topic_train_idf\n",
    "\n",
    "topic_dev_tfidf = np.log10(1 + topic_dev_count) * topic_dev_idf\n",
    "\n",
    "topic_test_tfidf = np.log10(1 + topic_test_count) * topic_test_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to change `SGD` to support multiclass datasets. First, you need to develop a `softmax` function. It takes as input:\n",
    "\n",
    "- `z`: an array of real numbers\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `smax`: the softmax of `z`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Compute probability for each class\n",
    "    \"\"\"\n",
    "    e_z = np.exp(z)\n",
    "    return e_z / np.sum(e_z, axis=1 if e_z.ndim > 1 else None, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then modify `predict_proba` and `predict_class` functions for the multiclass case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(X, weights):\n",
    "    \"\"\"\n",
    "    :param weights: (3, |vocab|) shape, one weight vector for each class\n",
    "    \"\"\"\n",
    "    z = X.dot(weights.T)\n",
    "    return softmax(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(X, weights):\n",
    "    \"\"\"\n",
    "    Each document will have one probability for each class, \n",
    "    use argmax to find the highest probability class\n",
    "    \"\"\"\n",
    "    # Add 1 after argmax as the topic class starts from 1\n",
    "    return np.argmax(predict_proba(X, weights), axis=1) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test example and expected functionality of the functions above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0.1, 0.2], [0.2, 0.1], [0.1, -0.2]])\n",
    "w = np.array([[2, -5], [-5, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_proba(X, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_class(X, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to compute the categorical cross-entropy loss (extending the binary loss to support multiple classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_loss(X, Y, weights, num_classes=5, alpha=0.00001):\n",
    "    # Compute the negative log-likelihood and L2 regularisation for true class only\n",
    "    l = np.array([\n",
    "        -np.log(probs[Y[idx] - 1]) + alpha * np.sum(weights[Y[idx] - 1]**2)\n",
    "        for idx, probs in enumerate(predict_proba(X, weights))\n",
    "    ])\n",
    "\n",
    "    # Return average loss\n",
    "    return np.mean(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally you need to modify SGD to support the categorical cross entropy loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(X_tr, Y_tr, X_dev, Y_dev, num_classes=5, lr=0.01, alpha=0.00001, epochs=5, tolerance=0.001, print_progress=True):\n",
    "    # fixing random seed for reproducibility\n",
    "    np.random.seed(123)\n",
    "    training_loss_history = []\n",
    "    validation_loss_history = []\n",
    "\n",
    "    # Initialise weight to zero\n",
    "    weights = np.zeros((num_classes, X_tr.shape[1]))\n",
    "\n",
    "    # Create training tuples\n",
    "    train_docs = list(zip(X_tr, Y_tr))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Randomise order in train_docs\n",
    "        np.random.shuffle(train_docs)\n",
    "\n",
    "        for x_i, y_i in train_docs:\n",
    "            # Compute gradient and update weight for correct class only\n",
    "            gradient = x_i * (np.max(predict_proba(x_i, weights)) - 1)\n",
    "            weights[y_i - 1] -= lr * (gradient + 2 * alpha * weights[y_i - 1])\n",
    "\n",
    "        # Monitor training and validation loss\n",
    "        cur_loss_tr = categorical_loss(X_tr, Y_tr, weights, alpha)\n",
    "        cur_loss_dev = categorical_loss(X_dev, Y_dev, weights, alpha)\n",
    "\n",
    "        # Early stopping\n",
    "        if epoch > 0 and validation_loss_history[-1] - cur_loss_dev < tolerance:\n",
    "            break\n",
    "        else:\n",
    "            training_loss_history.append(cur_loss_tr)\n",
    "            validation_loss_history.append(cur_loss_dev)\n",
    "\n",
    "        if print_progress:\n",
    "            print(f'Epoch: {epoch} | Training loss: {cur_loss_tr} | Validation loss: {cur_loss_dev}')\n",
    "            \n",
    "    return weights, training_loss_history, validation_loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Multi-class Logistic Regresstion with Count Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w_count, tr_loss_count, dev_loss_count = SGD(X_tr=topic_train_count,\n",
    "                                             Y_tr=topic_train_labels,\n",
    "                                             X_dev=topic_dev_count,\n",
    "                                             Y_dev=topic_dev_labels,\n",
    "                                             num_classes=3,\n",
    "                                             lr=0.00425,\n",
    "                                             alpha=0.00001,\n",
    "                                             epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training and validation process and explain if your model overfit, underfit or is about right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tr_loss_count, label='Training loss')\n",
    "plt.plot(dev_loss_count, label='Validation loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.title('Training Monitoring (Multi-class - Count)')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the plot **Training Monitoring (Multi-class - Count)**, \n",
    "\n",
    "1. The training loss decreases as epoch increases and eventually reaches a point of stability\n",
    "2. The validation loss decreases as epoch increases and eventually reaches a point of stability\n",
    "3. THe validation loss is slightly higher than the training loss, i.e. the \"generalisation gap\" is small\n",
    "\n",
    "The following techniques are implemented in the Stochastic Gradient Descent algorithm to avoid overfitting of the training data:\n",
    "\n",
    "1. Early stopping\n",
    "2. L2 regularisation\n",
    "\n",
    "Hence, the model is **about right**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy, precision, recall and F1-scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = topic_test_labels, predict_class(topic_test_count, w_count)\n",
    "\n",
    "print('Accuracy:', accuracy_score(*args))\n",
    "print('Precision:', precision_score(*args, average='macro'))\n",
    "print('Recall:', recall_score(*args, average='macro'))\n",
    "print('F1-Score:', f1_score(*args, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the top-10 words for each class respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top10_ids = (-w_count).argsort()[:, :10]\n",
    "\n",
    "print(\n",
    "    f'Top 10 Class 1 (World): {[vocab_id_to_word[id] for id in top10_ids[0]]} \\n'\n",
    ")\n",
    "print(\n",
    "    f'Top 10 Class 2 (Sports): {[vocab_id_to_word[id] for id in top10_ids[1]]} \\n'\n",
    ")\n",
    "print(\n",
    "    f'Top 10 Class 3 (Business): {[vocab_id_to_word[id] for id in top10_ids[2]]} \\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Evaluation\n",
    "\n",
    "The top 10 features obtained for each class using **count vectors** are reasonable. Most distinguishable features are:\n",
    "\n",
    "1. World: president\n",
    "2. Sports: athens, team, olympic\n",
    "3. Business: company, oil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Multi-class Logistic Regresstion with TF.IDF Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w_tfidf, tr_loss_tfidf, dev_loss_tfidf = SGD(X_tr=topic_train_tfidf,\n",
    "                                             Y_tr=topic_train_labels,\n",
    "                                             X_dev=topic_dev_tfidf,\n",
    "                                             Y_dev=topic_dev_labels,\n",
    "                                             num_classes=3,\n",
    "                                             lr=0.01525,\n",
    "                                             alpha=0.00001,\n",
    "                                             epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training and validation process and explain if your model overfit, underfit or is about right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tr_loss_tfidf, label='Training loss')\n",
    "plt.plot(dev_loss_tfidf, label='Validation loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.title('Training Monitoring (Multi-class - TFIDF)')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the plot **Training Monitoring (Multi-class - TFIDF)**, \n",
    "\n",
    "1. The training loss decreases as epoch increases and eventually reaches a point of stability\n",
    "2. The validation loss decreases as epoch increases and eventually reaches a point of stability\n",
    "3. THe validation loss is slightly higher than the training loss, i.e. the \"generalisation gap\" is small \n",
    "\n",
    "The following techniques are implemented in the Stochastic Gradient Descent algorithm to avoid overfitting of the training data:\n",
    "\n",
    "1. Early stopping\n",
    "2. L2 regularisation\n",
    "\n",
    "Hence, the model is **about right**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy, precision, recall and F1-scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = topic_test_labels, predict_class(topic_test_tfidf, w_tfidf)\n",
    "\n",
    "print('Accuracy:', accuracy_score(*args))\n",
    "print('Precision:', precision_score(*args, average='macro'))\n",
    "print('Recall:', recall_score(*args, average='macro'))\n",
    "print('F1-Score:', f1_score(*args, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the top-10 words for each class respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top10_ids = (-w_tfidf).argsort()[:, :10]\n",
    "\n",
    "print(\n",
    "    f'Top 10 Class 1 (World): {[vocab_id_to_word[id] for id in top10_ids[0]]} \\n'\n",
    ")\n",
    "print(\n",
    "    f'Top 10 Class 2 (Sports): {[vocab_id_to_word[id] for id in top10_ids[1]]} \\n'\n",
    ")\n",
    "print(\n",
    "    f'Top 10 Class 3 (Business): {[vocab_id_to_word[id] for id in top10_ids[2]]}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Evaluation\n",
    "\n",
    "The top 10 features obtained for each class using **TF.IDF vectors** are reasonable. They are more relevant than the features obtained using **count vectors**. Most distinguishable features are:\n",
    "\n",
    "1. World: president, state, government\n",
    "2. Sports: athens, olympic, team, olympics, season\n",
    "3. Business: company, oil. business, million, prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If we were to apply the classifier we've learned into a different domain such as laptop reviews or restaurant reviews, do you think these features would generalise well?\n",
    "\n",
    "### Count Vectors and TF.IDF Vectors\n",
    "\n",
    "Both count vectors model and TF.IDF vectors **would not generalise well**. The overall explanation is similar to the one in Binary models, with a few differences regarding the top features obtained.\n",
    "\n",
    "1. The top features obtained by the Multi-Class models do not include any sentiment lexicons (e.g. good, bad, etc.).\n",
    "2. Most of the top features apart of general-purpose words (e.g. said, about, etc.) are not applicable to laptop or movie reviews.\n",
    "\n",
    "Therefore, Multi-Class models are less generalised and would have worse performance than the Binary models in the new domains. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discuss how did you choose model hyperparameters (e.g. learning rate and regularisation strength)? What is the relation between training epochs and learning rate? How does the regularisation strength affect performance?\n",
    "\n",
    "**Note:** Many similar explanations are already written in the hyperparameters discussion for **Binary Logistic Regression**, hence they are not repeated here.\n",
    "\n",
    "Similarly, the initial lower and upper bound of learning rate is set to `0.0001` and `0.1` respectively. The initial lower and upper bound of regularisation strength is set to `0.00001` and `0.01` respectively.\n",
    "\n",
    "\n",
    "### Count Vectors\n",
    "\n",
    "#### Optimising Learning Rate\n",
    "\n",
    "Initial parameters: `lr=0.0001, alpha=0.00001`\n",
    "\n",
    "| Trial | Learning rate | Epochs | Tr. loss | Val. loss | Precision | Recall | F1-Score |\n",
    "|-------|---------------|--------|----------|-----------|-----------|--------|----------|\n",
    "| 0     | 0.0001        | 99     | 0.5631   | 0.6983    | 0.8343    | 0.8333 | 0.8325   |\n",
    "| 1     | 0.0005        | 99     | 0.3693   | 0.4527    | 0.8600    | 0.8588 | 0.8580   |\n",
    "| 2     | 0.001         | 82     | 0.3399   | 0.4035    | 0.8619    | 0.8611 | 0.8603   |\n",
    "| 3     | 0.002         | 61     | 0.3247   | 0.3742    | 0.8650    | 0.8644 | 0.8637   |\n",
    "| 4     | 0.003         | 50     | 0.3192   | 0.3623    | 0.8662    | 0.8655 | 0.8648   |\n",
    "| 5     | 0.004         | 42     | 0.3167   | 0.3566    | 0.8698    | 0.8688 | 0.8682   |\n",
    "| 6     | 0.005         | 38     | 0.3147   | 0.3514    | 0.8688    | 0.8677 | 0.8672   |\n",
    "| 7     | 0.0045        | 40     | 0.3155   | 0.3536    | 0.8687    | 0.8677 | 0.8672   |\n",
    "| 8     | 0.00425       | 41     | 0.3161   | 0.3550    | 0.8698    | 0.8688 | 0.8682   |\n",
    "\n",
    "The optimal `lr` is found to be `0.00425`\n",
    "\n",
    "#### Optimising Regularisation Strength\n",
    "\n",
    "Initial parameters: `lr=0.00425, alpha=0.00001`\n",
    "\n",
    "| Trial | Alpha    | Epochs | Tr. loss | Val. loss | Precision | Recall | F1-Score |\n",
    "|-------|----------|--------|----------|-----------|-----------|--------|----------|\n",
    "| 0     | 0.00001  | 41     | 0.3161   | 0.3550    | 0.8698    | 0.8688 | 0.8682   |\n",
    "| 1     | 0.00002  | 41     | 0.3160   | 0.3552    | 0.8698    | 0.8688 | 0.8682   |\n",
    "| 2     | 0.000015 | 41     | 0.3160   | 0.3551    | 0.8698    | 0.8688 | 0.8682   |\n",
    "| 3     | 0.0001   | 41     | 0.3154   | 0.3565    | 0.8698    | 0.8688 | 0.8682   |\n",
    "\n",
    "No better `alpha` values have been found. The initial `alpha` is considered to be the optimal value.\n",
    "\n",
    "**Conclusion:** Based on the trials above, it can be concluded that the optimal values are `lr=0.00425` and `alpha=0.00001`\n",
    "\n",
    "\n",
    "### TF.IDF Vectors\n",
    "\n",
    "#### Optimising Learning rate\n",
    "\n",
    "Initial parameters: `lr=0.0001, alpha=0.00001`\n",
    "\n",
    "| Trial | Learning rate | Epochs | Tr. loss | Val. loss | Precision | Recall | F1-Score |\n",
    "|-------|---------------|--------|----------|-----------|-----------|--------|----------|\n",
    "| 0     | 0.0001        | 99     | 0.8116   | 0.9398    | 0.8765    | 0.8766 | 0.8758   |\n",
    "| 1     | 0.001         | 99     | 0.3748   | 0.5592    | 0.8882    | 0.8877 | 0.8869   |\n",
    "| 2     | 0.003         | 99     | 0.2849   | 0.4173    | 0.8914    | 0.8911 | 0.8904   |\n",
    "| 3     | 0.005         | 82     | 0.2716   | 0.3877    | 0.8937    | 0.8933 | 0.8927   |\n",
    "| 4     | 0.007         | 72     | 0.2653   | 0.3715    | 0.8946    | 0.8944 | 0.8937   |\n",
    "| 5     | 0.01          | 61     | 0.2609   | 0.3584    | 0.8945    | 0.8944 | 0.8937   |\n",
    "| 6     | 0.011         | 59     | 0.2597   | 0.3546    | 0.8947    | 0.8944 | 0.8938   |\n",
    "| 7     | 0.013         | 54     | 0.2587   | 0.3499    | 0.8958    | 0.8955 | 0.8949   |\n",
    "| 8     | 0.015         | 51     | 0.2579   | 0.3453    | 0.8970    | 0.8966 | 0.8960   |\n",
    "| 9     | 0.017         | 47     | 0.2578   | 0.3430    | 0.8938    | 0.8933 | 0.8927   |\n",
    "| 10    | 0.016         | 49     | 0.2578   | 0.3440    | 0.8959    | 0.8955 | 0.8950   |\n",
    "| 11    | 0.0155        | 50     | 0.2579   | 0.3446    | 0.8959    | 0.8955 | 0.8950   |\n",
    "| 12    | 0.01525       | 50     | 0.2580   | 0.3454    | 0.8970    | 0.8966 | 0.8960   |\n",
    "\n",
    "The optimal `lr` is found to be `0.01525`\n",
    "\n",
    "#### Optimising Regularisation Strength\n",
    "\n",
    "Initial parameters: `lr=0.0152, alpha=0.00001`\n",
    "\n",
    "| Trial | Learning rate | Epochs | Tr. loss | Val. loss | Precision | Recall | F1-Score |\n",
    "|-------|---------------|--------|----------|-----------|-----------|--------|----------|\n",
    "| 0     | 0.00001       | 50     | 0.2580   | 0.3454    | 0.8970    | 0.8966 | 0.8960   |\n",
    "| 1     | 0.00002       | 50     | 0.2578   | 0.3466    | 0.8970    | 0.8966 | 0.8960   |\n",
    "| 2     | 0.000015      | 50     | 0.2579   | 0.3461    | 0.8970    | 0.8966 | 0.8960   |\n",
    "| 3     | 0.0001        | 48     | 0.2572   | 0.3565    | 0.8970    | 0.8966 | 0.8960   |\n",
    "\n",
    "No better `alpha` has been found. The initial `alpha` is considered to be the optimal value.\n",
    "\n",
    "**Conclusion:** Based on the trials above, it can be concluded that the optimal values are `lr=0.01525` and `alpha=0.00001`\n",
    "\n",
    "### Relationship Between Epochs and Learning Rate\n",
    "\n",
    "According to the tables above, it has been shown that the higher the learning rate, the lower the epochs required to converge. This relationship is the same as the one in **Binary Logistic Regression**, hence the type of model does not affect the relationship between epochs and learning rate.\n",
    "\n",
    "### Relationship Between Regularisation Strength and Model Performance\n",
    "\n",
    "According to the tables above, it has been shown that the higher the regularisation strength, the lower the epochs required to converge. Moreover, the training and validation loss increase as the regularisation strength increases. This relationship is the same as the one in **Binary Logistic Regression**. \n",
    "\n",
    "However, no performance improvement is observed in both **Count Vectors** and **TF.IDF Vectors** of this multi-class model. \n",
    "\n",
    "Therefore, there is not enough evidence to state that the relationship between regularisation strength and model performance is consistent across different types of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Results\n",
    "\n",
    "|     LR    |      Precision     |       Recall       |      F1-Score      |\n",
    "|:---------:|:------------------:|:------------------:|:------------------:|\n",
    "| BOW-count | 0.8698296951326174 | 0.8688888888888888 |  0.868323408492766 |\n",
    "| BOW-tfidf | 0.8970314657551683 | 0.8966666666666666 | 0.8960678070376614 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Justifications for Implementation Choices\n",
    "\n",
    "## Lower Case for N-gram\n",
    "\n",
    "Since the actual sentence structure and word order is not taken into account by the classifier, reducing all n-grams to lower case is a good strategy. It will allow instances of \"*Best*\" at the beginning of a sentence to be considered as \"*best*\". This will also help the model to identify features more accurately.\n",
    "\n",
    "### Performance Improvement\n",
    "\n",
    "Performance improvement is achieved in **Multi-class TFIDF Vectors**, as shown in the following table:\n",
    "\n",
    "|     MLR TFIDF     |      Precision     |       Recall       |      F1-Score      |\n",
    "|:-----------------:|:------------------:|:------------------:|:------------------:|\n",
    "| Without lowercase | 0.8859205236286744 | 0.8855555555555555 | 0.8848282934618362 |\n",
    "|   With lowercase  | 0.8970314657551683 | 0.8966666666666666 | 0.8960678070376614 |\n",
    "\n",
    "### Top 10 Features Weight\n",
    "\n",
    "Without lower case preprocessing:\n",
    "\n",
    "- World: 'The', 'said', 'AFP', 'AP', 'Tuesday', 'Monday', 'President', 'Reuters', 'new', 'people'\n",
    "- Sports: 'AP', 'The', 'Olympic', 'ATHENS', 'first', 'Olympics', 'team', 'two', 'Tuesday', 'season'\n",
    "- Business: 'The', 'company', 'said', 'oil', 'Reuters', 'new', 'more', 'US', 'prices', 'business'\n",
    "\n",
    "With lower case preprocessing:\n",
    "\n",
    "- World: 'said', 'afp', 'ap', 'president', 'tuesday', 'monday', 'new', 'state', 'reuters', 'government'\n",
    "- Sports: 'ap', 'athens', 'olympic', 'team', 'first', 'olympics', 'no', 'two', 'season', 'tuesday'\n",
    "- Business: 'company', 'said', 'oil', 'new', 'reuters', 'more', 'business', 'million', 'prices', 'about'\n",
    "\n",
    "The most noticeable difference is the increases in weight for more important terms in the following classes:\n",
    "\n",
    "- World: 'president', 'state', 'government'\n",
    "- Business: 'business', 'million'\n",
    "\n",
    "\n",
    "## Log Normalisation Scheme for Term Frequency\n",
    "\n",
    "Raw term frequency might not be ideal because: \n",
    "\n",
    "- It is known that a document with `tf = 10` occurrences for a term is more relevant than a document with `tf = 1` occurrence for that term\n",
    "- However, this does not indicate that the document with `tf = 10` is 10 times more relevant than `tf = 1`\n",
    "\n",
    "Hence, relevance does not increase proportionally with term frequency. Using a sublinear function to calculate term frequency will help to reduce the importance of the term that has a high frequency.\n",
    "\n",
    "### Performance Improvement\n",
    "\n",
    "| BLR TF Scheme |      Precision     | Recall |      F1-Score      |\n",
    "|:-------------:|:------------------:|:------:|:------------------:|\n",
    "| Raw frequency | 0.8507462686567164 |  0.855 | 0.8528678304239402 |\n",
    "|   Log scaled  | 0.9025641025641026 |  0.88  | 0.8911392405063291 |\n",
    "\n",
    "| MLR TF Scheme |      Precision     |       Recall       |      F1-Score      |\n",
    "|:-------------:|:------------------:|:------------------:|:------------------:|\n",
    "| Raw frequency | 0.8822875333701315 | 0.8822222222222221 | 0.8816177693062457 |\n",
    "|   Log scaled  | 0.8970314657551683 | 0.8966666666666666 | 0.8960678070376614 |\n",
    "\n",
    "### Top 10 Features Weight\n",
    "\n",
    "The model is able to identify more relevant features using the log normalisation TF weighting scheme. \n",
    "\n",
    "#### Binary Logistic Regression TF.IDF\n",
    "\n",
    "Raw frequency TF weighting scheme:\n",
    "\n",
    "- Positive: 'great', 'fun', 'hilarious', 'terrific', 'overall', 'definitely', 'memorable', 'truman', 'pulp', 'perfectly'\n",
    "\n",
    "- Negative: 'nbsp', 'bad', 'worst', 'boring', 'supposed', 'unfortunately', 'nothing', 'why', 'waste', 'script'\n",
    "\n",
    "Log normalisation TF weighting scheme:\n",
    "\n",
    "- Positive: 'hilarious', 'perfectly', 'terrific', 'great', 'memorable', 'overall', 'definitely', 'perfect', 'excellent', 'fun'\n",
    "\n",
    "- Negative: 'bad', 'worst', 'boring', 'supposed', 'unfortunately', 'ridiculous', 'waste', 'script', 'awful', 'nothing'\n",
    "\n",
    "#### Multi-Class Logistic Regression TF.IDF\n",
    "\n",
    "Raw frequency TF weighting scheme:\n",
    "\n",
    "- World: 'said', 'afp', 'ap', 'president', 'tuesday', 'new', 'monday', 'state', 'reuters', 'government' \n",
    "\n",
    "- Sports: 'ap', 'athens', 'olympic', 'team', 'quot', 'no', 'first', 'olympics', 'tuesday', 'one' \n",
    "\n",
    "- Business: 'company', 'oil', 'said', 'new', 'more', 'reuters', 'business', 'over', 'about', 'up'\n",
    "\n",
    "Log normalisation TF weighting scheme:\n",
    "\n",
    "- World: 'said', 'afp', 'ap', 'president', 'tuesday', 'monday', 'new', 'state', 'reuters', 'government'\n",
    "\n",
    "- Sports: 'ap', 'athens', 'olympic', 'team', 'first', 'olympics', 'no', 'two', 'season', 'tuesday'\n",
    "\n",
    "- Business: 'company', 'said', 'oil', 'new', 'reuters', 'more', 'business', 'million', 'prices', 'about'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
